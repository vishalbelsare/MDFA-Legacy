% Parallelized computation chapters customization and replication
% simanz<-100 chapters customization and replication
% Load all chapters

\chapter{Introduction}\label{intro_sec}

\section{Classic Model-Based Paradigm}

Marc's perspective:
\begin{itemize}
\item Maximum Likelihood, main purpose: determine DGP. If DGP is known then optimality can be invoked, in principle. 
\item Problem: model misspecification. Pseudo maximum likelihood: one-step ahead mean-square criterion. 
\item Emphasizes short-term performances, only (contrast with real-time trend extraction: long-term component). 
\item Rigid criterion: can account neither for relevant problem-structure (signal extraction=one and multi-step ahead forecasts) nor for various user-priorities (ATS-trilemma).
\end{itemize}

Tucker/Chris' perspectives:...


\section{A Shift of Perspective: Tackling the (Relevant) Problem-Structure and Addressing User-Priorities}

Refer to DFA and Trilemma papers with Tucker. Refer to chapters \ref{mse_sec} (problem structure) and \ref{ats_sec} (user-priorities). 

\section{Univariate DFA}

Refer to DFA-paper with Tucker and \href{http://blog.zhaw.ch/sef/files/2014/10/DFA.pdf}{DFA} (Sweave environment: replication). 


\section{This Book's Contribution: Multivariate (M-) DFA}

Problem structure (chapter \ref{mse_sec}); forecasting/nowcasting/backcasting and filter revisions (chapter \ref{fil_sec});filter constraints (chapter \ref{con_sec}); ATS-trilemma, customization and user-priorities (chapter \ref{ats_sec}); replicating and enhancing classical model-based approaches and HP/CF-filters (chapter \ref{rep_sec}); addressing more sophisticated gain/loss structures (chapter \ref{exo_sec}); developing inferential aspects (chapter \ref{inf_sec}); Regularization Troika and tackling overfitting (chapter \ref{reg_sec}); data-revisions (chapter \ref{rev_sec}); solving the mixed-frequency problem (chapter \ref{mix_sec}); extensions to non-stationary integrated (chapter \ref{int_sec}) and cointegrated processes (chapter \ref{coint_sec}); adaptive filtering (chapter \ref{ada_sec}).




\section{R-Code}

We here briefly review the main R-code files and provide support for installation.

\subsection{Getting Started: Setting the Paths}

Start from a clean-sheet when replicating results in this book:
<<label=init,echo=TRUE>>=
rm(list=ls())
@
The R-code in the various chapters of this book requires installation of the following R-packages:
<<label=init,echo=TRUE>>=
# Load packages: time series and xts
library(tseries)
library(xts)
# State-space models (will be replicated by MDFA) 
library(dlm)
# Numerical package 
library(numDeriv)
# Graphical package for recession-shading (empirical examples based on US-GDP)
library(tis) 
#install.packages("devtools")
library(devtools)
# Load MDFA package from github
devtools::install_github("wiaidp/MDFA")
# MDFA package
library(MDFA) 
@
<<label=init,echo=FALSE>>=
# Quandl
# Load required libraries
library(RCurl)    # For getURL() and curl handler / cookie / google login
library(stringr)  # For str_trim() to trip whitespace from strings
library(Quandl)
require (Quandl)
Quandl.api_key("ivVdJGV57TXA1RX5jgvp")
@
US-GDP data for the empirical examples can be retrieved either directly from Quandl (this would require preliminary user-registration, though) or from a local data-folder which is the default-setting:
<<label=init,echo=TRUE>>=
# Load fresh data from quandl: T/F
#   Default-setting is False: the data will be loaded from local data folder
load_from_quandl<-F
@
Paths to MDFA-code as well as to the (US-GDP) data must be provided. It is assumed that the MDFA-package is saved to a main folder containing DFA-, MDFA-, model-based- and data-subfolders. The R-code in the book generates pdf-graphs which are saved in a separate folder whose path is specified by path.out.
<<label=init,echo=TRUE>>=
# Specify disk
disk_id<-"C"
# Set main path
path.main<-paste(disk_id,
      ":\\wia_desktop\\Projekte\\2016\\MDFA-Legacy\\Sweave\\",sep="")
# Set paths to subfolders
  # Path to Latex-folder: all pdfs generated by the R-code are filed there
path.out<-paste(path.main,"Latex\\",sep="")
  # Path to data (US-GDP)
path.dat<-paste(path.main,"Data\\",sep="")
  # Path to code that is part of MDFA-Legacy project but not part of MDFA-package 
path.outside<-paste(path.main,"R\\Code_outside_MDFA_package\\",sep="")
@
The (univariate) DFA-code is the same as in \href{http://blog.zhaw.ch/sef/files/2014/10/DFA.pdf}{DFA}: all  empirical examples are and will be fully compatible. 

\subsection{DFA}\label{dfa_intro}
We here briefly review the relevant \href{http://blog.zhaw.ch/sef/files/2014/10/DFA.pdf}{DFA}\footnote{Left-click to activate the hyperlink.}-pieces (anchoring). 

\subsubsection{DFT and Periodogram}

The Discrete Fourier Transform (DFT) and the periodogram are defined in \href{http://blog.zhaw.ch/sef/files/2014/10/DFA.pdf}{DFA}, sections 2.2 and 2.3. The following per(iodogram) function in the MDFA-package replicates these formulae\footnote{Note that frequency $\pi$ is treated differently, whether the sample size is odd or even, and that frequency zero is scaled by $1/\sqrt{2}$ which will be explained later on.}.  
<<dft,echo=TRUE>>=
head(per,100)
@
This function will be generalized in the new multivariate setting.

\subsubsection{DFA: Mean-Square Perspective}

A simple MSE (Mean-Square Error) version of the DFA, as proposed in \href{http://blog.zhaw.ch/sef/files/2014/10/DFA.pdf}{DFA}, section 4.1, is included in the MDFA-package:  

<<dfa_ms,echo=TRUE>>=
# This function computes mean-square DFA-solutions
# L is the length of the MA filter,
# periodogram is the frequency weighting function in the DFA
# Gamma is the transferfunction of the symmetric filter (target) and
# Lag is the lag-parameter: Lag=0 implies real-time filtering, Lag=L/2
#     implies symmetric filter
# The function returns optimal coefficients as well as the transfer 
#     function of the optimized real-time filter
head(dfa_ms,100)
@
This function is nested in the multivariate MDFA in the sense that the latter can replicate the former perfectly when suitably parametrized, see section \ref{ex_rep_dfa}. 



\subsubsection{DFA: ATS-Trilemma and Customization}

A more general DFA-function, called \emph{dfa\textunderscore analytic}, is proposed in \href{http://blog.zhaw.ch/sef/files/2014/10/DFA.pdf}{DFA}, section 4.3.5. Customization and the generic ATS-trilemma are presented in \href{http://blog.zhaw.ch/sef/files/2014/10/DFA.pdf}{DFA}, sections 4.3 and 5. This function is included in the MDFA-package: 
<<dfa_ms,echo=TRUE>>=
head(dfa_analytic)
@
The additional control parameters $lambda,eta$ allow for customization of the filter, see chapter \ref{ats_sec}, and the Boolean $i1,i2$ can enforce useful filter constraints, see chapter \ref{con_sec}. Once again, this function is nested in the multivariate MDFA. 



\subsection{MDFA}\label{mdfa_intro}

The R-code for MDFA is more sophisticated and correspondingly more complex and lengthy. As for the DFA-package, the MDFA-code can be sourced. We here briefly review the corresponding pieces.


\subsubsection{Data-Matrix}

All time series are collected in a data-\emph{matrix}, say \emph{X}, which is organized as follows: 
\begin{itemize}
\item the first column $X[,1]$ of $X$ always corresponds to the target series: the target series $X[,1]$ is the time series to be forecasted, nowcasted or backcasted.
\item Columns 2,3,... of $X$ are allocated to the explanatory variables (more than one in a multivariate setting). If the target series is part of the set of explanatory variables (it does not have to), then it must be assigned a specific column - by convention always the second one - in $X$ i.e. in this case the target series is contained twice: in the first column (target) and in the second column (explanatory data).     
\end{itemize}
Example:
<<dfa_ms,echo=TRUE>>=
set.seed(1)
len<-100
target<-arima.sim(list(ar=0.9),n=len)
explanatory_2<-target+rnorm(len)
explanatory<-cbind(target,explanatory_2)
x<-cbind(target,explanatory)
dimnames(x)[[2]]<-c("target","explanatory 1","explanatory 2")
head(x)
@
In this case we assume a two-dimensional Signal Extraction (SE-) problem whereby the target series (first column) is part of the set of explanatory variables. For a one-step ahead forecast problem we might consider lagging of the explanatory variables
<<dfa_ms,echo=TRUE>>=
x<-cbind(x[,1],lag(x[,2:3],-1))
dimnames(x)[[2]]<-c("target","lagged explanatory 1","lagged explanatory 2")
head(x)
@
However, our frequency-domain approach in such a case will be more general and it will avoid introduction of undesirable NA's.


\subsubsection{DFT}

In contrast to the univariate case, which can rely on the periodogram only, the multivariate case requires the (complex) DFT in order to account for cross-sectional dependencies\footnote{The relative phase-shift(s) of the explanatory time series import. In the univariate case the relative phase-shift vanishes since the target and the explanatory series are identical.}). Also, we here extend the scope of the method in order to cover the mixed-frequency case, see chapter \ref{mix_sec}. Finally, we allow for the possibility of integrated processes, see chapter \ref{int_sec}. In order to illustrate some of the new features we briefly look at the main DFT-function called $spec\textunderscore comp$:
<<dfa_ms,echo=TRUE>>=
spec_comp
@
The inner loop runs across the columns of the data-matrix \emph{X} (see above for definition and conventions) and the DFTs are stored in a matrix called \emph{weight\textunderscore func} which is returned by the function. The matrix \emph{weight\textunderscore func} collects all DFTs: the target series is always in the first column whereas the DFTs of the explanatory series are in columns 2,3,... The function \emph{periodogram\textunderscore bp}, called in the above loop, is slightly more general than the DFA-function \emph{periodogram}, proposed in the previous section. In particular it can handle various integration orders as well as seasonal peculiarities.  \\

\subsubsection{MDFA}


\subsection{Feeding and Controlling MDFA}\label{control_dfa}

\subsubsection{Generic Approach: Rich User-Interface}

MDFA is a generic forecast and signal extraction paradigm. Besides replicating classical time series approaches it allows for some unique features like customization and  general filter-regularization (Regularization Troika and general H0-shrinkage). Also, it allows for tackling data revisions, mixed-frequency problems and non-stationarity. Accordingly, the user-interface is more sophisticated than the precedent DFA-package. In order to illustrate the topic we here briefly look at the head of the main estimation routine:    

<<dfa_ms,echo=TRUE>>=
head(mdfa_analytic)
@
Besides straightforward entries, like the DFT (\emph{weight\textunderscore func}, see previous section), the filter-length ($L$), or the target specification \emph{Gamma} there are numerous additional control parameters: the relevance and the modus operandi of most of them will be discussed in this book. 


\subsubsection{Default-Settings}

For convenience, we store a so-called \emph{default-setting} of the parameters in a file called \emph{control\textunderscore default}. In order to do so we first need to define the data (initialize the DFT-matrix) and specify the filter-length:
<<dfa_ms,echo=TRUE>>=
weight_func<-matrix(rep(1:6,2),ncol=2)
L<-2
@
Given these two entries (data and filter-length), the default-settings are as follows:
<<dfa_ms,echo=TRUE>>=
K<-nrow(weight_func)-1
lambda<-0
Lag<-0
eta<-0
i1<-F
i2<-F
weight_constraint<-rep(1/(ncol(weight_func)-1),ncol(weight_func)-1)
lambda_cross<-lambda_decay<-lambda_smooth<-0
lin_eta<-F
shift_constraint<-rep(0,ncol(weight_func)-1)
grand_mean<-F
b0_H0<-NULL
c_eta<-F
weights_only<-F
weight_structure<-c(0,0)
white_noise<-F
synchronicity<-F
lag_mat<-matrix(rep(0:(L-1),ncol(weight_func)-1),nrow=L)
troikaner<-F
@
This particular configuration will be used extensively in chapter \ref{mse_sec}: mean-square criterion (no customization), no regularization, unconstrained design, no a priori knowledge, nowcasting, common identical sampling frequency, no data-revisions. The default-settings can be obtained by sourcing the corresponding R-file

<<dfa_ms,echo=TRUE>>=
source(file=paste(path.outside,"control_default.r",sep=""))
@
For later usage we here source a convenient plot-function
<<dfa_ms,echo=TRUE>>=
source(file=paste(path.outside,"mplot_func.r",sep=""))
@


