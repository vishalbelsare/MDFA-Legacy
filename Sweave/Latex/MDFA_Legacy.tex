% -*- mode: noweb; noweb-default-code-mode: R-mode; -*-
\documentclass[a4paper]{book}
%\documentclass[envcountsame,envcountchap]{svmono}


\usepackage{graphicx,url}
\usepackage{amssymb}

\def\pf{{\bf Proof. }}
\def\logimplies{\Rightarrow}
\def\convinlaw{\stackrel{{\cal L}}{\Longrightarrow }}
\def\convinp{\stackrel{P}{\longrightarrow }}
\def\convas{\stackrel{a.s.}{\longrightarrow }}
\def\convv{\stackrel{v}{\longrightarrow}}
\def\asymp{\stackrel{{\mathbb P}}{\sim}}
\def\RR{\mathbb R}
\def\ZZ{\mathbb Z}
\def\QQ{\mathbb Q}
\def\NN{\mathbb N}
\def\MM{\mathbb M}
\def\LL{\mathbb L}
\def\EE{\mathbb E}
\def\PP{\mathbb P}
\def\DD{\mathbb D}
\def\WW{\mathbb W}
\def\FF{\mathbb F}
\def\II{\mathbb I}
\def\FF{\mathbb F}
\def\XX{\mathbb X}
\def\sige{\sigma_{\epsilon}}

\def\eqinlaw{\stackrel{{\cal L}}{=}}
\def\tends{\rightarrow}
\def\tendsinf{\rightarrow\infty}
\def\isodynamo{\Leftrightarrow}

\newtheorem{Theorem}{Theorem}
\newtheorem{Lemma}{Lemma}
\newtheorem{Corollary}{Corollary}
\newtheorem{Proposition}{Proposition}
\newtheorem{Definition}{Definition}
\newtheorem{Remark}{Remark}
\newtheorem{Example}{Example}
\newtheorem{Exercise}{Exercise}
\newtheorem{Illustration}{Illustration}
\newcommand{\mbf}[1]{\mbox{\boldmath $#1$}}

\setlength{\textwidth}{6.5in} \setlength{\textheight}{9in}
\setlength{\evensidemargin}{12pt} \setlength{\oddsidemargin}{0in}
\setlength{\topmargin}{1in}
\renewcommand{\baselinestretch}{1.3}
\setlength{\headheight}{0.2in} 
\setlength{\headsep}{0.2in}

%- Makes the section title start with Appendix in the appendix environment
\newcommand{\Appendix}
{%\appendix
%\def\thesection{Appendix~\Alph{section}}
\def\thesection{Appendix~\Alph{chapter}}
%\def\thesubsection{\Alph{section}.\arabic{subsection}}
%\def\thesubsection{A.\arabic{subsection}}
\def\thesubsection{A.\arabic{section}}
}


%\pagestyle{empty}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{latexsym}
\usepackage{epsfig}
%\usepackage{html}
\usepackage{verbatim}
\usepackage{hyperref}
\usepackage{float}
\usepackage[utf8]{inputenc}
\usepackage{a4wide}

\title{Multivariate Real-Time Signal Extraction}
\author{Marc Wildi and Tucker McElroy}




\usepackage{Sweave}
\begin{document}

\maketitle

\date{}

%\SweaveOpts{prefix.string=c:/wia/tmp/bar}

\frontmatter%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\include{dedic}
%\newpage
%\phantom{rete}
%\newpage
%\include{preface}




\tableofcontents


\mainmatter%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%-----------------------------------------------

% Parallelized computation chapters customization and replication
% simanz<-100 chapters customization and replication
% Load all chapters

\chapter{Introduction}\label{intro_sec}

\section{Overview}

\subsection{Signals and Extraction}

In the applications of time series analysis to macroeconomics, finance, and quality control
 it is essential to extract useful information about trends, turning points, and anomalies
 in real time.  The practitioner does not have the luxury of sifting past data for 
 structural breaks, indicators of regime change, or changes to volatility.  Informative elections are
 contingent upon understanding the dynamics of various time series at time present.  Because
 long-term movements, as well as aberrations, are defined in terms of the long-run behavior of a 
 time series over past, present, and future, any analysis of the present state necessarily involves
 a degree of forecasting.  This  broad topic is referred to as real-time signal extraction.

A signal is any component of a time series that is deemed useful for a particular application.  
If long-term movements are of interest, the signal is a trend.  If short-term fluctuations about
 a longer-term mean are of interest, the signal is a cycle.  If shocks, due to rare
 terrorist events or natural disasters, are of interest, the signal consists of the extreme values.
 If regular patterns of an annual period, linked to cultural or meteorological patterns, are of interest,
 the signal is a seasonal component.

However, these signals are not directly observable at time present, because in each case their
 definition involves all the past and future values of a time series -- but the future is unknown,
 and only part of the past is available to us.  The statistical processes by which a signal is
 estimated from available data is referred to as extraction, and the residual from the 
 signal extraction is referred to as the noise.  Whereas signals can be estimated from historical, or past,
 sections of a time series, when effort is focused upon time present we refer to the analysis as
 real-time signal extraction.

Real-time signal extraction is considerably more challenging, and useful, than historical signal extraction.
 The difficulty lies in the uncertainty about the future, which is transmitted unto the signal 
 extraction estimates themselves.  One way to conceive of this difficulty is through the warring
 principles of timeliness and accuracy: should we procrastinate in providing our analysis of the present,
 we can increase the accuracy of signal extraction, but our answers become less relevant, even as the present
 time rapidly drifts into the past.  Conversely,   extremely timely extractions suffer from 
 greater future uncertainty, and are likely to exhibit inaccuracy.

There is a considerable body of literature addressing signal extraction, but this book focuses upon
 a particular methodology called Direct Filter Analysis (DFA).  As the original development of DFA
 was univariate, the methodology's power was limited to the information content
 within a single time series.  But because batches of time series can be closely linked, exhibiting 
 correlated trends, common dynamics, or even predictive relationships, it is natural to expect that
 a multivariate extension of DFA to vector time series will more greatly facilitate informed decision 
 making.  The topic of this book is Multivariate Direct Filter Analysis (MDFA).

 

\subsection{The Classic Model-Based Paradigm}

Many signals can be formulated as weighted linear combinations of a time series, in which case the real-time
 signal extraction problem can be approached as a Linear Prediction Problem (LPP).  In order to pose
 an LPP, a solution criterion is needed, and Mean Squared Error (MSE) is often used: one seeks a real-time 
 signal extraction that has minimal MSE discrepancy with the actual target signal.  Although an LPP
 can then be solved, the solution depends on knowing something about the dynamics in the time series process.
 The most venerable approach to understanding these dynamics is to posit a time series model, and fit
 this model to the observed data.  This approach, which goes back to the work of Yule in the 1930s, is called
 the classic paradigm, being based upon a Model-Based Analysis (MBA).

An attractive feature of MBA is that analytical formulas for the LPP solutions can often be obtained, thereby
 facilitating computation.  The philosophy underpinning the classic paradigm is that a Data Generation Process (DGP)
 exists -- as a theoretical, or Platonic construct -- to which the observed data closely adheres.  Formally,
 the DGP is some stochastic process defined upon a probability space, and the observed data is a realization, or sample path, of
 the DGP.  Statistical inference is involved with the science of identifying a model class for the DGP, narrowing down
 the class to a particular model (by eliminating contenders), and fitting that model via fixing values of the parameters.
 Successive applications of model diagnostics allow for refinements, and a process by which we can verify the validity
 of a postulated model.  Of course, all of this is done on the basis of the single realization of the DGP.

While recognizing that any such model need not be correct, i.e., exactly match the DGP itself, such models can yet
 be useful to the extent to which they reflect important features in the data.  Yet it is difficult to keep a model
 simple -- which is necessay to its utility -- and at the same time be sufficiently versatile to explain all the 
data's features.  Moreover, the appellation of importance is subjective: a feature deemed important to one user may
 be irrelevant to another.  This begs the question of customization: each user, with a distinct set of criteria and
 desired applications, could potentially stress the importance of a subset of features at the cost of de-emphasizing others.
 The classic paradigm ignores, or at least passes over, the issue of customization, and proposes a single all-purpose
 concept of utility: the minimization of one-step ahead forecast error MSE.

 Another term for this classic conception of model utility is the Wold decomposition, which breaks a wide class of
 stochastic processes down in terms of a component that is completely predictable from its own infinite past, and 
 a second component fully describable in terms of one-step ahead forecast errors.  Classical models can then
 be viewed as attempts to approximate the linear machinery in the Wold decomposition.    However, were attention to
 focus upon an alternative utility, e.g., 10-step ahead forecasting, a different class of models would be suggested,
 with different apparatus for model selection, fitting, and evaluation.

However, customizing the modeling apparatus to allow for specific applications offers only a partial solution, because
 model mis-specification is the larger challenge.  The full set of LPP solutions for a given time series is greatly
 constrained once a model is introduced, as only a particular subset of solutions can be obtained.  If the model is
 badly mis-specified, the resulting LPP solution will be inadequate, even if the criteria for model selection are customized.
 This empirical disfunctionality motivated the genesis of DFA, which essentially provides access to a much wider
 pool of LPP solutions.  Moreover, the basic DFA can be easily modified to allow for direct customization of 
real-time problems, according to whether users are concerned with timeliness, accuracy, or fidelity to the original signal (called
 smoothness).  
 

%Marc's perspective:
%\begin{itemize}
%\item Maximum Likelihood, main purpose: determine DGP. If DGP is known then optimality can be invoked, in principle. 
%\item Problem: model misspecification. Pseudo maximum likelihood: one-step ahead mean-square criterion. 
%\item Emphasizes short-term performances, only (contrast with real-time trend extraction: long-term component). 
%\item Rigid criterion: can account neither for relevant problem-structure (signal extraction=one and multi-step ahead forecasts) nor for various user-priorities (ATS-trilemma).
%\end{itemize}

\subsection{The Scope of MDFA}

Our critique of the classic paradigm has several facets.  First, there is typically model mis-specification present.  Second, the problem
 has typically not been structured properly, in the sense that the criteria used do not correspond to the relevant LPP, but rather to
   one-step ahead forecasting.  Third, there is no specific customization of the model, in order to account for timeliness and accuracy.
 These weaknesses are actually linked together.  

Model mis-specification is always present; the issue is whether it has a significant impact upon the objectives of analysis.  For instance,
 a given model's mis-specification may have grave repercussions for certain problem structures, while being adequate for other LPPs.
 The given LPP of interest determines the gravity and impact of model mis-specification.  Moreover, in the classic paradigm the one-step
 ahead forecasting LPP is solved, and it is merely hoped that timeliness and accuracy will be adequate for all users.  Model parameters
 can be tweaked, or tuned, in order to indirectly modify timeliness and accuracy -- but the relationships are indirect and often poorly
 understood.  By building the timeliness-accuracy tradeoff directly into the DFA criterion, the optimality of an LPP solution for a
 customized application is assured.

These topics have been treated in Wildi and McElroy (2016, JTSE) in the case of univariate time series, which discusses at length
 the basic \href{http://blog.zhaw.ch/sef/files/2014/10/DFA.pdf}{DFA} (Sweave environment: replication).  This book presents
 the generalized treatment of the multivariate LPP in   Chapter \ref{mse_sec}. But before discussing customization in Chapter \ref{ats_sec},
 we discuss the applications of forecasting and nowcasting, as well as the impact of data vintage, in  Chapter \ref{fil_sec}).
 Then the basic LPP treatment is extended to nonstationary processes in Chapter \ref{int_sec}, followed by a discussion of filter constraints (Chapter \ref{con_sec}).
  This treatment is extended to the case of co-integration in Chapter \ref{coint_sec}.   Applications to replicating and enhancing classical model-based approaches and HP/CF-filters 
 are given in Chapter \ref{rep_sec}, while  more sophisticated gain/loss structures  are discussed in Chapter \ref{exo_sec}.
Additional topics include inference (Chapter \ref{inf_sec}), regularization (Chapter \ref{reg_sec}), data revisions (Chapter \ref{rev_sec}),
 mixed-frequency data (Chapter \ref{mix_sec}), and   adaptive filtering (Chapter \ref{ada_sec}).

\section{The Style of the Book}

This book was generated using Sweave, in accordance with the philosophy of 
 scientific replicability.  Throughout the text are portions of R code that
 can be pasted into an R script and directly run, given that the user
 has certain packages already installed.  This installation is described below.
 
\subsection{Setting the Paths}

Begin by clearing the workspace: 
\begin{Schunk}
\begin{Sinput}
> #rm(list=ls())
\end{Sinput}
\end{Schunk}
The R code in   various chapters of this book requires installation of the following R packages:
\begin{Schunk}
\begin{Sinput}
> # Load packages: time series and xts
> #library(tseries)
> library(xts)
> # State-space models (will be replicated by MDFA) 
> library(dlm)
> # Classic filter designs (be replicated by MDFA)
> library(mFilter)
> # Numerical package 
> library(numDeriv)
> # Graphical package for recession-shading (empirical examples based on US-GDP)
> library(tis)
> # Library for tables
> library(Hmisc)
> require(xtable)
> #install.packages("devtools")
> library(devtools)
> # Load MDFA package from github
> devtools::install_github("wiaidp/MDFA")
> # MDFA package
> library(MDFA)
\end{Sinput}
\end{Schunk}
US-GDP data for the empirical examples can be retrieved either directly from 
 Quandl (requiring a preliminary user registration) or from a local data folder,
  which is the default-setting:
\begin{Schunk}
\begin{Sinput}
> # Load fresh data from quandl: T/F
> #   Default-setting is False: the data will be loaded from local data folder
> load_from_quandl <- F
\end{Sinput}
\end{Schunk}
Paths to MDFA code, as well as to the US-GDP data, must be provided. 
 It is assumed that the MDFA package is saved to a main folder containing
 subfolders labeled as DFA, MDFA, model-based, and data. 
The R code in the book generates pdf graphs that are saved in a separate folder, 
whose path is specified by {\em path.out}.
\begin{Schunk}
\begin{Sinput}
> # Set main path
> path.main <- paste(getwd(),"/Sweave/",sep="")
> #path.main <- "C:\\Users\\Tucker\\Documents\\MDFAbook\\"
> # Set paths to subfolders
>   # Path to Latex-folder: all pdfs generated by the R code are filed there
> path.out <- paste(path.main,"Latex/",sep="")
>   # Path to data (US-GDP)
> path.dat <- paste(path.main,"Data/",sep="")
>   # Path to code that is part of MDFA-Legacy project but not part of MDFA package 
> path.pgm <- paste(path.main,"R/",sep="")
\end{Sinput}
\end{Schunk}
The univariate DFA code is the same as in \href{http://blog.zhaw.ch/sef/files/2014/10/DFA.pdf}{DFA}; all 
 empirical examples are and will be fully compatible. 

\subsection{DFA}\label{dfa_intro}
We here briefly review the relevant facets of \href{http://blog.zhaw.ch/sef/files/2014/10/DFA.pdf}{DFA},
 thereby providing an anchor for the MDFA discussion. 

\subsubsection{DFT and Periodogram}

The Discrete Fourier Transform (DFT) and the periodogram are defined in Sections 2.2 and 2.3 of
\href{http://blog.zhaw.ch/sef/files/2014/10/DFA.pdf}{DFA}.  
The following periodogram function -- referred to as {\em per} below --
  in the MDFA package replicates these formulae.  Note that frequency $\pi$ is treated differently, depending on
 whether the  sample size is odd or even; also, the value at frequency zero is scaled by $1/\sqrt{2}$,
  which  is explained in later text.  
\begin{Schunk}
\begin{Sinput}
> head(per,100)
\end{Sinput}
\begin{Soutput}
1  function (x, plot_T)                                                              
2  {                                                                                 
3      len <- length(x)                                                              
4      per <- 0:(len/2)                                                              
5      DFT <- per                                                                    
6      for (k in 0:(len/2)) {                                                        
7          cexp <- exp((0+1i) * (1:len) * 2 * pi * k/len)                            
8          DFT[k + 1] <- sum(cexp * x * sqrt(1/(2 * pi * len)))                      
9      }                                                                             
10     if (abs(as.integer(len/2) - len/2) < 0.1)                                     
11         DFT[k + 1] <- DFT[k + 1]/sqrt(2)                                          
12     per <- abs(DFT)^2                                                             
13     if (plot_T) {                                                                 
14         par(mfrow = c(2, 1))                                                      
15         plot(per, type = "l", axes = F, xlab = "Frequency", ylab = "Periodogram", 
16             main = "Periodogram")                                                 
17         axis(1, at = 1 + 0:6 * len/12, labels = c("0", "pi/6",                    
18             "2pi/6", "3pi/6", "4pi/6", "5pi/6", "pi"))                            
19         axis(2)                                                                   
20         box()                                                                     
21         plot(log(per), type = "l", axes = F, xlab = "Frequency",                  
22             ylab = "Log-periodogram", main = "Log-periodogram")                   
23         axis(1, at = 1 + 0:6 * len/12, labels = c("0", "pi/6",                    
24             "2pi/6", "3pi/6", "4pi/6", "5pi/6", "pi"))                            
25         axis(2)                                                                   
26         box()                                                                     
27     }                                                                             
28     return(list(DFT = DFT, per = per))                                            
29 }                                                                                 
\end{Soutput}
\end{Schunk}
This function will be generalized in the new multivariate setting.

\subsubsection{Basic DFA}

A simple   version of the DFA  based on the MSE criterion alone -- 
 as proposed in Section 4.1 of \href{http://blog.zhaw.ch/sef/files/2014/10/DFA.pdf}{DFA} --
 is included in the MDFA package:  

\begin{Schunk}
\begin{Sinput}
> # This function computes MSE DFA solutions 
> # L is the length of the MA filter,
> # periodogram is the frequency weighting function in the DFA
> # Gamma is the transfer function of the symmetric filter (target) and
> # Lag is the lag-parameter: Lag=0 implies real-time filtering, Lag=L/2
> #     implies symmetric filter
> # The function returns optimal coefficients as well as the transfer 
> #     function of the optimized real-time filter
> head(dfa_ms,100)
\end{Sinput}
\begin{Soutput}
1  function (L, periodogram, Lag, Gamma)                                    
2  {                                                                        
3      periodogram[1] <- periodogram[1]/2                                   
4      K <- length(periodogram) - 1                                         
5      X <- exp(-(0+1i) * Lag * pi * (0:(K))/(K)) * rep(1, K + 1) *         
6          sqrt(periodogram)                                                
7      X_y <- exp(-(0+1i) * Lag * pi * (0:(K))/(K)) * rep(1, K +            
8          1)                                                               
9      for (l in 2:L) {                                                     
10         X <- cbind(X, (cos((l - 1 - Lag) * pi * (0:(K))/(K)) +           
11             (0+1i) * sin((l - 1 - Lag) * pi * (0:(K))/(K))) *            
12             sqrt(periodogram))                                           
13         X_y <- cbind(X_y, (cos((l - 1 - Lag) * pi * (0:(K))/(K)) +       
14             (0+1i) * sin((l - 1 - Lag) * pi * (0:(K))/(K))))             
15     }                                                                    
16     xtx <- t(Re(X)) %*% Re(X) + t(Im(X)) %*% Im(X)                       
17     b <- as.vector(solve(xtx) %*% (t(Re(X_y)) %*% (Gamma * periodogram)))
18     trffkt <- 1:(K + 1)                                                  
19     trffkt[1] <- sum(b)                                                  
20     for (k in 1:(K)) {                                                   
21         trffkt[k + 1] <- (b %*% exp((0+1i) * k * (0:(length(b) -         
22             1)) * pi/(K)))                                               
23     }                                                                    
24     return(list(b = b, trffkt = trffkt))                                 
25 }                                                                        
\end{Soutput}
\end{Schunk}
This function is nested in the multivariate MDFA,
  in the sense that the latter can replicate the former perfectly when suitably parametrized;
 see Section \ref{ex_rep_dfa} below.



\subsubsection{Customized DFA}

A more general DFA function, called \emph{dfa\textunderscore analytic}, is proposed in Section 4.3.5 of
\href{http://blog.zhaw.ch/sef/files/2014/10/DFA.pdf}{DFA}. Customization and the generic 
 Accuracy-Timeliness-Smoothness (ATS) trilemma are presented in Sections 4.3 and 5 of
 \href{http://blog.zhaw.ch/sef/files/2014/10/DFA.pdf}{DFA}. This function is included in the MDFA package: 
\begin{Schunk}
\begin{Sinput}
> head(dfa_analytic)
\end{Sinput}
\begin{Soutput}
1 function (L, lambda, periodogram, Lag, Gamma, eta, cutoff, i1, 
2     i2)                                                        
3 {                                                              
4     periodogram[1] <- periodogram[1]/2                         
5     lambda <- abs(lambda)                                      
6     eta <- abs(eta)                                            
\end{Soutput}
\end{Schunk}
The additional control parameters {\em lambda}, {\em eta} allow for customization of the filter,  as discussed below
 in Chapter \ref{ats_sec}.  The Boolean {\em i1} and {\em i2}
  can enforce useful filter constraints; see Chapter \ref{con_sec}. This function is also encompassed by the   MDFA. 



\subsection{MDFA}\label{mdfa_intro}

The R code for MDFA is more sophisticated than that of the DFA, and is correspondingly more complex and lengthy. 
 As for the DFA package, the MDFA code can be sourced. We here briefly review the corresponding pieces.


\subsubsection{Data Matrix}

All time series are collected in a data-\emph{matrix}, say $X$, which is organized as follows: 
\begin{itemize}
\item the first column $X[,1]$ of $X$ always corresponds to the target series: the target series $X[,1]$ is the time series
 to be forecasted, nowcasted or backcasted.
\item Columns $2$, $3$, $\ldots$ of $X$ are allocated to the explanatory variables (more than one in a multivariate setting). 
If the target series is part of the set of explanatory variables (it does not have to be), then it must be assigned a specific column 
-- by convention always the second one -- in $X$, i.e., in this case the target series is entered twice, in the first column (target) and
  in the second column (explanatory data).     
\end{itemize}

\noindent {\bf Example}.  Suppose we study a  two-dimensional signal extraction problem, whereby the target series (first column) 
is part of the set of explanatory variables:
\begin{Schunk}
\begin{Sinput}
> set.seed(1)
> len <- 100
> target <- arima.sim(list(ar=0.9),n=len)
> explanatory_2 <- target+rnorm(len)
> explanatory <- cbind(target,explanatory_2)
> x <- cbind(target,explanatory)
> dimnames(x)[[2]] <- c("target","explanatory 1","explanatory 2")
> head(x)
\end{Sinput}
\begin{Soutput}
       target explanatory 1 explanatory 2
[1,] 1.703613      1.703613     0.3191863
[2,] 1.398197      1.398197     3.2674879
[3,] 3.659995      3.659995     4.0850957
[4,] 3.254756      3.254756     3.0161086
[5,] 3.619020      3.619020     4.6775026
[6,] 3.285120      3.285120     4.1715424
\end{Soutput}
\end{Schunk}
For a one-step ahead forecast LPP, we might consider lagging both the explanatory variables:
\begin{Schunk}
\begin{Sinput}
> x<-cbind(x[,1],lag(x[,2:3],-1))
> dimnames(x)[[2]]<-c("target","lagged explanatory 1","lagged explanatory 2")
> head(x)
\end{Sinput}
\begin{Soutput}
       target lagged explanatory 1 lagged explanatory 2
[1,] 1.703613                   NA                   NA
[2,] 1.398197             1.703613            0.3191863
[3,] 3.659995             1.398197            3.2674879
[4,] 3.254756             3.659995            4.0850957
[5,] 3.619020             3.254756            3.0161086
[6,] 3.285120             3.619020            4.6775026
\end{Soutput}
\end{Schunk}
 By adopting the frequency-domain methods of this book, we can generalize this construction and
    avoid the introduction of missing values (denoted by NA in R).  $\quad \Box$


\subsubsection{DFT}

In contrast to the univariate DFA, where the LPP can be expressed in terms of  the periodogram, the multivariate case 
 requires the   DFT of each time series in order to account for cross-sectional dependencies.  These DFTs are complex-valued
 quantities, and the angular portion of the cross-spectrum provides information about the relative phase-shift of each explanatory time series. 
  In the univariate case the relative phase-shift is irrelevant, because the target series and the explanatory series are identical.
 The scope of the method is extended in order to cover the mixed-frequency case, which is discussed in Chapter \ref{mix_sec}. 
 Another facet, is that we allow for the possibility of integrated processes; see Chapter \ref{int_sec}. 
 In order to illustrate some of the new features we briefly look at the main DFT function called {\em spec\textunderscore comp}:
\begin{Schunk}
\begin{Sinput}
> spec_comp
\end{Sinput}
\begin{Soutput}
function (insamp, x, d) 
{
    if (d == 1) {
        weight_func <- periodogram_bp(diff(x[1:insamp, 1]), 1, 
            insamp - 1)$fourtrans
        if (length(weight_func) > 1) {
            for (j in 2:ncol(x)) {
                per <- periodogram_bp(diff(x[1:insamp, j]), 1, 
                  insamp - 1)$fourtrans
                weight_func <- cbind(weight_func, per)
            }
        }
    }
    else {
        weight_func <- periodogram_bp(x[1:insamp, 1], 0, insamp)$fourtrans
        if (length(weight_func) > 1) {
            for (j in 2:ncol(x)) {
                per <- periodogram_bp(x[1:insamp, j], 0, insamp)$fourtrans
                weight_func <- cbind(weight_func, per)
            }
        }
    }
    colnames(weight_func) <- colnames(x)
    return(list(weight_func = weight_func))
}
<bytecode: 0x000000f41a8d4948>
<environment: namespace:MDFA>
\end{Soutput}
\end{Schunk}
The inner loop   tracks the columns of the data matrix $X$ and the DFTs are stored in a matrix called \emph{weight\textunderscore func},
  which is returned by the function. The matrix \emph{weight\textunderscore func} collects all DFTs;
  the target series is always in the first column, whereas the DFTs of the explanatory series are in columns $2$, $3$, $\ldots$
 The function \emph{periodogram\textunderscore bp}, called in the above loop, is slightly more general than the DFA 
function \emph{per} proposed in the previous section. In particular, it can handle various integration orders as well as
 seasonal peculiarities. 

\subsection{Using MDFA}\label{control_dfa}

\subsubsection{A Versatile User Interface}

MDFA is a generic forecast and signal extraction paradigm. Besides its capacity to  replicate classical time series approaches, 
  MDFA possesses unique features such as customization and regularization (Chapter \ref{reg_sec}); it can
  treat data revisions (Chapter \ref{rev_sec}), mixed-frequency problems (Chapter \ref{mix_sec}),
 and non-stationarity (Chapters \ref{int_sec} and \ref{coint_sec}. Accordingly, the user interface 
is more sophisticated than the precediing DFA package.
Consider the head of the main estimation routine:    

\begin{Schunk}
\begin{Sinput}
> head(mdfa_analytic)
\end{Sinput}
\begin{Soutput}
1 function (L, lambda, weight_func, Lag, Gamma, eta, cutoff, i1,             
2     i2, weight_constraint, lambda_cross, lambda_decay, lambda_smooth,      
3     lin_eta, shift_constraint, grand_mean, b0_H0, c_eta, weight_structure, 
4     white_noise, synchronicity, lag_mat, troikaner)                        
5 {                                                                          
6     lambda <- abs(lambda)                                                  
\end{Soutput}
\end{Schunk}
 Arguments such as  \emph{weight\textunderscore func} (discussed above), the filter length ($L$), and the target specification \emph{Gamma}
 are straightforward.    But there are numerous additional control parameters: the relevance and the modus operandi of these
 will be discussed in this book. 


\subsubsection{Default Settings}

For convenience, we store a so-called default setting of the parameters in a file called \emph{control\textunderscore default}.
 First we define the data (initialize the DFT matrix) and specify the filter  length:
\begin{Schunk}
\begin{Sinput}
> weight_func <- matrix(rep(1:6,2),ncol=2)
> L <- 2
\end{Sinput}
\end{Schunk}
Given these two entries (DFT and filter length), the default-settings are as follows:
\begin{Schunk}
\begin{Sinput}
> d<-0
> lin_eta<-F
> lambda<-0
> Lag<-0
> eta<-0
> i1<-F
> i2<-F
> weight_constraint<-rep(1/(ncol(weight_func)-1),ncol(weight_func)-1)
> lambda_cross<-lambda_smooth<-0
> lambda_decay<-c(0,0)
> lin_expweight<-F
> shift_constraint<-rep(0,ncol(weight_func)-1)
> grand_mean<-F
> b0_H0<-NULL
> c_eta<-F
> weights_only<-F
> weight_structure<-c(0,0)
> white_noise<-F
> synchronicity<-F
> cutoff<-pi
> lag_mat<-matrix(rep(0:(L-1),ncol(weight_func)),nrow=L)
> troikaner<-F
\end{Sinput}
\end{Schunk}
This particular configuration will be used extensively in Chapter \ref{mse_sec}; it corresponds to the basic MSE criterion 
 (i.e., no customization) without regularization, without design constraints, and without any {\em a priori} knowledge.
  Also, this configuration presumes a   common identical sampling frequency (i.e., no mixed frequency data)
 and the absence of data revisions. The default settings can be obtained by sourcing the corresponding R file:

\begin{Schunk}
\begin{Sinput}
> source(file=paste(path.pgm,"control_default.r",sep=""))
\end{Sinput}
\end{Schunk}
For later use we   source a convenient plotting function:
\begin{Schunk}
\begin{Sinput}
> source(file=paste(path.pgm,"mplot_func.r",sep=""))
\end{Sinput}
\end{Schunk}

\subsubsection{Selected Calls: Classic MSE, Customization and Regularization}

Selected calls of the classic MSE  criterion -- as well as calls utilizing the customization or regularization features --
 are available through dedicated functions in the MDFA package: 
\begin{Schunk}
\begin{Sinput}
> head(MDFA_mse)
\end{Sinput}
\begin{Soutput}
1 function (L, weight_func, Lag, Gamma) 
2 {                                     
3     cutoff <- pi                      
4     lin_eta <- F                      
5     lambda <- 0                       
6     eta <- 0                          
\end{Soutput}
\begin{Sinput}
> head(MDFA_mse_constraint)
\end{Sinput}
\begin{Soutput}
1 function (L, weight_func, Lag, Gamma, i1, i2, weight_constraint, 
2     shift_constraint)                                            
3 {                                                                
4     cutoff <- pi                                                 
5     lin_eta <- F                                                 
6     lambda <- 0                                                  
\end{Soutput}
\begin{Sinput}
> head(MDFA_cust)
\end{Sinput}
\begin{Soutput}
1 function (L, weight_func, Lag, Gamma, cutoff, lambda, eta)                  
2 {                                                                           
3     lin_eta <- F                                                            
4     weight_constraint <- rep(1/(ncol(weight_func) - 1), ncol(weight_func) - 
5         1)                                                                  
6     lambda_cross <- lambda_smooth <- 0                                      
\end{Soutput}
\begin{Sinput}
> head(MDFA_cust_constraint)
\end{Sinput}
\begin{Soutput}
1 function (L, weight_func, Lag, Gamma, cutoff, lambda, eta, i1, 
2     i2, weight_constraint, shift_constraint)                   
3 {                                                              
4     lin_eta <- F                                               
5     lambda_cross <- lambda_smooth <- 0                         
6     lambda_decay <- c(0, 0)                                    
\end{Soutput}
\begin{Sinput}
> head(MDFA_reg)
\end{Sinput}
\begin{Soutput}
1 function (L, weight_func, Lag, Gamma, cutoff, lambda, eta, lambda_cross,    
2     lambda_decay, lambda_smooth, troikaner = F)                             
3 {                                                                           
4     lin_eta <- F                                                            
5     weight_constraint <- rep(1/(ncol(weight_func) - 1), ncol(weight_func) - 
6         1)                                                                  
\end{Soutput}
\begin{Sinput}
> head(MDFA_reg_constraint)
\end{Sinput}
\begin{Soutput}
1 function (L, weight_func, Lag, Gamma, cutoff, lambda, eta, lambda_cross,      
2     lambda_decay, lambda_smooth, i1, i2, weight_constraint, shift_constraint, 
3     troikaner = F)                                                            
4 {                                                                             
5     lin_eta <- F                                                              
6     lin_expweight <- F                                                        
\end{Soutput}
\end{Schunk}
The heads of the corresponding functions differ in the number of additional arguments available 
when going from specific (MSE) to generic (reg).  The following chapters of the book provide an understanding of
 the use of these functions.



%----------------------------------------

%\SweaveInput{LPP}

%----------------------------------------

%\SweaveInput{MDFA_basic}
%  note: MDFA_basic replaces Mean_square

%----------------------------------------




\chapter{Multivariate Direct Filter Analysis for Non-stationary Processes}
\label{chap:int}

 We now extend the basic MDFA of Chapter \ref{chap:basic}  by considering
 the method's application to  non-stationary processes.  
 Section \ref{sec:constraint} introduces the idea of filter constraints
arising from time-varying means, a form of non-stationarity.
 This treatment is generalized in Section \ref{sec:non-stat}
  by the definition of non-stationary processes, and theory for the corresponding
   model-based filters is developed.  Finally, the MDFA criterion for
    non-stationary processes is discussed in Section \ref{sec:mdfa-nonstat}.
 

\section{Constrained MDFA}
\label{sec:constraint}

 Various constraints upon the concurrent filter can be envisioned, 
   and imposing such strictures results in  a constrained MDFA. 
   A chief case of interest arises when the 
    data process has a time-varying mean (which is a form of  non-stationarity);
  then it is necessary to impose additional filter constraints -- otherwise
   the filter error will not have mean zero.    To see why, 
   Write $\Delta (L) = \Psi (L) - \widehat{\Psi} (L)$ as the discrepancy filter,
   so that we see  from (\ref{eq:dfa-error})  
   that $\EE [ E_t ] = \Delta (L) \, \EE [ X_t ]$; 
   by Definition \ref{def:lpp}, we require
 that $\EE [ E_t ] = 0$ for any LPP.  
  If $\EE [ X_t] = 0$ then this condition is always satisfied, but
   for most time series of interest the mean will be nonzero, and is typically
    time-varying.  For such cases additional constraints on $\Delta (L)$ must be imposed,
    which implicitly amount to constraints on $\widehat{\Psi} (L)$.
    
\begin{Example}    {\bf Constant Mean.}  \rm
\label{exam:constant.mean}
  If $\EE [ X_t ] = \mu$, some nonzero constant,  then we require $\Delta (1) = 0$.
  This is because the mean of the filter error is
  \[
   \Delta (B) \, \EE [ X_t] = \Delta(B) \, \mu = \sum_j \delta (j) \, \mu =
   \Delta (1) \, \mu,
  \]
  and this is zero only if $\Delta (1) = 0$.  This is called a Level Constraint (LC).
\end{Example}  

\begin{Example}    {\bf Linear Mean.}  \rm
\label{exam:linear.mean}
  Suppose that $\EE [ X_t ] = \mu \, t$, where $\mu$ is a nonzero slope
 of a linear time trend.  Then it is required that  $\partial {\Delta} (1) = 0$
  in addition to the LC,  which is seen as follows:
  \[
   \Delta (B) \, \EE [ X_t] = \Delta(B) \, \mu \, t =   \mu \, \sum_j \delta (j) \, (t-j)
   = \mu \, \left(t \, \sum_j \delta (j) - \sum_j j \,\delta (j) \right)
    = \mu \, t \, \Delta(1) - \mu \, \partial \Delta (1).
  \]
  This mean of the filter error  is zero only if both $\Delta(1)=0$ and
  $\partial \Delta (1)=0$; the latter condition is called the
   Time-Shift Constraint (TSC).  
\end{Example}  

     Hence, for linear means we obtain
 three fundamental types of constraints: LC, TSC, and Level plus 
 Time-Shift Constraint (LTSC), which combines both LC and TSC.
  Using the fact that $\Delta (L) = \Psi (L) - \widehat{\Psi} (L)$,
   these three constaints can be described as follows:
\begin{align*}
 \mbox{LC} : &  \;  \Delta (1) = 0 \quad \mbox{or} \quad \Psi (1) = \widehat{\Psi} (1) \\
 \mbox{TSC} : &  \;   \partial {\Delta} (1) = 0 \quad \mbox{or} \quad 
 \partial {\Psi} (1) = \partial {\widehat{\Psi}} (1)  \\
 \mbox{LTSC} : &  \;  \Delta (1) = 0,  \,  \partial {\Delta} (1) = 0 \quad 
 \mbox{or} \quad \Psi (1) = \widehat{\Psi} (1), \; \partial {\Psi} (1) =
 \partial {\widehat{\Psi}} (1).
\end{align*}
 In the case of  concurrent filters of form  (\ref{eq:conc.filter}), 
 LC is accomplished by demanding that 
  $\sum_{j=0}^{q-1} \widehat{\psi} (j) = \Psi(1)$.   More generally, we consider  linear constraints  formulated via
\begin{equation}
\label{eq:concurrent-constrain}
  \vartheta = R \, \varphi + Q,
\end{equation}
 where $R$ is $n q \times n r$ and $\varphi$ is $n r \times 1$ dimensional, consisting of 
 free parameters; $Q$ is a matrix of constants, and is $n q \times 1$ dimensional.


\begin{Illustration}  {\bf Level Constraint (LC).}   \rm
\label{ill:lc}
 Note that $\sum_{j=0}^{q-1} \widehat{\psi} (j) = \Psi(1)$ implies that
\begin{equation}
\label{eq:lc-gamma0}
 \widehat{\psi} (0) = \Psi(1) - \sum_{j=1}^{q-1} \widehat{\psi} (j).
\end{equation}
 Hence  $ \varphi^{\prime}  = [ \widehat{\psi} (1), \widehat{\psi} (2), \ldots, \widehat{\psi} (q-1) ] $ and
\[
	R  = \left[ \begin{array}{ccc} -1 & \ldots & -1 \\ 1 & 0 & 0 \\
		\vdots & \ddots & \vdots \\ 0 & 0 & 1  \end{array} \right]  \otimes 1_n \qquad
	Q = \left[ \begin{array}{c} \Psi (1) \\ 0 \\ \vdots \\ 0 \end{array} \right].
\]
\end{Illustration}
 
 
\begin{Illustration}  {\bf Time Shift Constraint (TSC).}   \rm
\label{ill:tsc}
   The constraint is $\partial {\Psi} (1) = \partial \widehat{\Psi} (1)
   = \sum_{j=0}^{q-1} j \, \widehat{\psi} (j)$,
 or $\widehat{\psi} (1)  = \partial {\Psi} (1)  -  \sum_{j=2}^{q-1} j \, \widehat{\psi} (j) $.
 Hence  $ \varphi^{\prime}  = [ \widehat{\psi} (0), \widehat{\psi} (2), \ldots, \widehat{\psi} (q-1) ] $ and
\[
	R  = \left[ \begin{array}{cccc} 1 & 0 &  \ldots &  0  \\  0 & -2  &  -3  & \ldots  \\
		0 & 1 & 0 & \ldots \\ 
		\vdots & \ddots & \vdots & \vdots \\ 0 & \ldots & 0 & 1 \end{array} \right] \otimes 1_n \qquad
	Q = \left[ \begin{array}{c} 0 \\ \partial {\Psi} (1) \\ 0 \\ \vdots \\ 0 \end{array} \right].
\]
\end{Illustration}
 
 
\begin{Illustration}  {\bf Level and Time Shift Constraint (LTSC).}  \rm
\label{ill:ltsc}
   Take the Time Shift Constraint formula for $\widehat{\psi} (1)$,
 and plug this into (\ref{eq:lc-gamma0}), to obtain
\begin{align*}
 \widehat{\psi} (0)  & = \Psi (1) - \left( \partial {\Psi} (1)  -  \sum_{j=2}^{q-1} j  \, \widehat{\psi} (j) \right) -  \sum_{j=2}^{q-1} 
 \widehat{\psi} (j)  \\
	& = \Psi (1) -  \partial {\Psi} (1)  +  \sum_{j=2}^{q-1} (j-1)  \, \widehat{\psi} (j).
\end{align*}
 Hence  $ \varphi^{\prime}  = [  \widehat{\psi} (2), \ldots, \widehat{\psi} (q-1)  ] $ and
\[
	R  = \left[ \begin{array}{cccc} 1 & 2  &  3  &   \ldots    \\  -2  & -3  &  -4  & \ldots  \\
		 1  & 0 & \ldots & 0 \\ 
		\vdots & \ddots & \vdots & \vdots \\ 0 & \ldots & 0 & 1 \end{array} \right]  \otimes 1_n \qquad
	Q = \left[ \begin{array}{c} \Psi (1) - \partial {\Psi} (1)  \\  \partial {\Psi} (1) \\ 0 \\ \vdots \\ 0 \end{array} \right].
\]
\end{Illustration}



 More generally, we can envision an LPP involving $m$ linear constraints on 
  $\vartheta$, taking the form
 $   A = [ J \otimes 1_n ] \, \vartheta$, where $J$ is $m \times q$ 
 dimensional ($m < q$) and $A$ is $n m \times 1$ dimensional.
 (The LC, TSC, and LTSC examples all have this form.)  In order to express 
 this constraint in the form 
 (\ref{eq:concurrent-constrain}), we use the Q-R decomposition 
 (Golub and Van Loan, 1996) of $J$, writing
 $J = C \, G \, \Pi$ for an orthogonal matrix $C$ (which is $m \times m$ dimensional),
 a rectangular upper triangular matrix $G$
 (which is $m \times q$ dimensional), and a permuation matrix $\Pi$ 
 (which is $q \times q$ dimensional).  
 Standard matrix software such as $\textsc{R}$ will provide the Q-R decomposition $J$,
 and should produce the rank of $J$ as  a by-product --
 if this is less than $m$, then there are redundancies in the 
 constraints that should first be eliminated. 
 
 \begin{Exercise} {\bf QR Decomposition.} \rm
 \label{exer:qr.constraint}
 HERE
 \end{Exercise}
 
 Hence  proceeding with a full rank $J$, we partition $G$ as $G = [ G_1 \, G_2]$ 
 such that $G_1$ has $m$ columns and $G_2$
 has $q-m$ columns.  This quantity $q-m$ corresponds to the number 
 of free coefficient matrices, and is therefore the same as $r$.
 The Q-R decomposition guarantees that $G_1$ is an upper triangular matrix, 
 and moreover it is invertible.  Therefore
\[
  \left[ G_1^{-1} \, C^{-1} \otimes 1_n \right] \, A  = 
  \left( \left[ 1_m , \, G_1^{-1} \, G_2 \right] \, \Pi \otimes 1_n  \right) \, \vartheta,
\]
 and the action of $\Pi$ (together with the tensor product) amounts 
 to a   permutation of the elements of $\vartheta$.
  Let the output of this permutation be denoted
\[
   \left[ \begin{array}{l} \overline{\vartheta} \\ \underline{\vartheta} \end{array} \right]
   = \left( \Pi \otimes 1_n \right) \, \vartheta,
\]
 where $\overline{\vartheta}$ is $n m \times 1$ dimensional and
 $\underline{\vartheta}$ is $n r \times 1$ dimensional.  
 Then  by substitution we can solve for $\overline{\vartheta}$ in terms 
 of $\underline{\vartheta}$:
\[
   \overline{\vartheta} =  \left[ G_1^{-1} \, C^{-1} \otimes 1_n \right] \, A - 
   \left[  G_1^{-1} \, G_2  \otimes 1_n   \right] \, \underline{\vartheta}.
\]
 Therefore we recognize the free variables $\varphi = \underline{\vartheta}$, 
 and obtain $R$ and $Q$ in (\ref{eq:concurrent-constrain}) via
\begin{align*}
   R & = \Pi^{-1} \, \left[ \begin{array}{c} - G_1^{-1} \, G_2 \\ 
   1_{r} \end{array} \right] \otimes 1_n  \\
  Q & = \left( \Pi^{-1}  \, \left[ \begin{array}{c}  G_1^{-1} \, C^{-1} \\ 0 \end{array} \right] \otimes 1_n  \right) \, A.
\end{align*}
  These formulas allow one to compute the   form (\ref{eq:concurrent-constrain}) 
   from given constraints, and
 an analytical solution to the resulting MDFA criterion  be obtained from the following result.

\begin{Proposition}
\label{prop:mdfa.quadsoln-constrain}
 The minimizer of the  MDFA criterion given by   (\ref{eq:mdfa-criterion})
 with respect to  $\mathcal{G}$ -- consisting of all length $q$ concurrent filters 
 subject to  linear constraints of the form (\ref{eq:concurrent-constrain}) -- is
\begin{equation}
\label{eq:phi.soln-constrained}
 \varphi =  { \left[ R^{\prime} \, B \, R \right] }^{-1} \, R^{\prime} \,
 \left( b - B \, Q \right).
\end{equation}
 The minimal value is  
\begin{equation}
\label{eq:opt.val.mdfa-constrained}
{ \langle \Psi (z) \, G \, { \Psi (z) }^* \rangle }_0 
  + {(b - B \, Q)}^{\prime} \, \left( 
  R \, { \left[ R^{\prime} \, B \, R \right] }^{-1} \, R^{\prime} 
  + B^{-1} \right) \, ( b - B \, Q) -   b^{\prime} \, B^{-1} \, b.
\end{equation}
\end{Proposition}

\paragraph{Proof of Proposition \ref{prop:mdfa.quadsoln-constrain}.}
 Substituting (\ref{eq:concurrent-constrain}) in (\ref{eq:mdfa-crit.linear}) yields
\begin{align*}
  D_{\Psi} (\vartheta, G) &  = \varphi^{\prime} \,  \left[ R^{\prime} \, B \, R \right] \,  \varphi 
  + \left[ Q^{\prime} \, B \, R - b^{\prime} \, R \right] \, \varphi + \varphi^{\prime} \,
   \left[ R^{\prime} \, B \, Q - R^{\prime} \, b \right]  \\
 & + Q^{\prime} \, B \, Q  - Q^{\prime} \, b - b^{\prime} \, Q  + { \langle \Psi (z) \, G \, { \Psi (z) }^* \rangle }_0.
\end{align*}
  Now by applying the method of proof in Proposition \ref{prop:mdfa.quadsoln}, we obtain 
  the formula (\ref{eq:phi.soln-constrained}) for $\varphi$.  Plugging back into $D_{\Psi} (\vartheta, G)$
 yields the minimal value (\ref{eq:opt.val.mdfa-constrained}).  $\quad \Box$


\vspace{.5cm}

For computation, we utilize the same approximations to $B$ and $b$ as discussed 
in  Chapter \ref{chap:basic},
 obtaining the constrained MDFA filter $\vartheta$ via (\ref{eq:phi.soln-constained})
 followed by (\ref{eq:concurrent-constrain}).

\begin{Exercise} {\bf  Constrained MDFA for White Noise with Linear Trend.} \rm
\label{exer:wntrend-mdfa}
This exercise applies the constrained MDFA in the case of an ideal low-pass filter
 (cf. Example \ref{exam:ideal-low}) 
 applied to a white noise process that exhibits a linear trend.
 Simulate a sample of size $T=2500$ from a
  bivariate white noise process with 
  $\Sigma$ equal to the identity, but with a linear trend  given by
  \[
   \left[ \begin{array}{c} 1 \\ 2 \end{array} \right] + t \, 
   \left[ \begin{array}{c} -.002 \\ .001 \end{array} \right].
  \]
   Apply the   ideal low-pass filter (cf. Example \ref{exam:ideal-low}) with 
  $\mu = \pi/6$ to the sample (truncate the filter to $1000$ coefficients on each side).  
 Use the moving average filter  MDFA  with LC, TSC, and LTSC constraints
  (Proposition \ref{prop:mdfa.quadsoln-constrain}),
  as well as unconstrained MDFA  (Proposition \ref{prop:mdfa.quadsoln}), to find the best
 concurrent filter, setting $q= 30$. 
  (Hint: compute the periodogram from OLS residuals obtained by regressing the simulation
   on a constant plus time.)
 Apply this concurrent filter 
 to the simulation, and compare the relevant portions to the ideal trend.
 Also determine the in-sample performance, in comparison to the criterion value
 (\ref{eq:opt.val.mdfa}).   Target the trends for both time series.
\end{Exercise}

\begin{Schunk}
\begin{Sinput}
> # Simulate a Gaussian WN of sample size 2500:
> T <- 2500
> N <- 2
> levels <- c(1,2)
> slopes <- c(-2,1)/1000
> innovar.matrix <- diag(N)
> x.sim <- NULL
> for(t in 1:T)
+ {
+ 	x.next <- levels + slopes*t + t(chol(innovar.matrix)) %*% rnorm(N)
+ 	x.sim <- cbind(x.sim,x.next)
+ }
> x.sim <- ts(t(x.sim))
> time.trend <- seq(1,T)
> sim.ols <- lm(x.sim ~ time.trend)
> x.resid <- sim.ols$residuals
> # construct and apply low pass filter
> mu <- pi/6
> len <- 1000
> lp.filter <- c(mu/pi,sin(seq(1,len)*mu)/(pi*seq(1,len)))
> lp.filter <- c(rev(lp.filter),lp.filter[-1])
> x.trend.ideal <- filter(x.sim,lp.filter,method="convolution",sides=2)[(len+1):(T-len),]
> # get MDFA concurrent filter
> q <- 30
> Grid <- T
> m <- floor(Grid/2)
> # The Fourier frequencies
> freq.ft <- 2*pi*Grid^{-1}*(seq(1,Grid) - (m+1))
> # frf for ideal low-pass
> frf.psi <- rep(0,Grid)
> frf.psi[abs(freq.ft) <= mu] <- 1
> frf.psi <- matrix(frf.psi,nrow=1) %x% diag(N) 	  
> frf.psi <- array(frf.psi,c(N,N,Grid))
> spec.hat <- mdfa.pergram(x.resid,1)	
> lp.mdfa.uc <- mdfa.unconstrained(frf.psi,spec.hat,q)
> lp.mdfa.lc <- mdfa.levelconstraint(frf.psi,spec.hat,q)
> lp.mdfa.tsc <- mdfa.tsconstraint(frf.psi,spec.hat,q)
> lp.mdfa.ltsc <- mdfa.ltsconstraint(frf.psi,spec.hat,q)
> # case 1: apply the unconstrained MDFA concurrent filter 
> x.trend.mdfa11 <- filter(x.sim[,1],lp.mdfa.uc[[1]][1,1,],method="convolution",sides=1)
> x.trend.mdfa12 <- filter(x.sim[,2],lp.mdfa.uc[[1]][1,2,],method="convolution",sides=1)
> x.trend.mdfa21 <- filter(x.sim[,1],lp.mdfa.uc[[1]][2,1,],method="convolution",sides=1)
> x.trend.mdfa22 <- filter(x.sim[,2],lp.mdfa.uc[[1]][2,2,],method="convolution",sides=1)
> x.trend.mdfa <- cbind(x.trend.mdfa11 + x.trend.mdfa12,x.trend.mdfa21 + x.trend.mdfa22)
> x.trend.mdfa <- x.trend.mdfa[(len+1):(T-len),] 
> # compare in-sample performance
> print(c(mean((x.trend.ideal[,1] - x.trend.mdfa[,1])^2),
+ 	mean((x.trend.ideal[,2] - x.trend.mdfa[,2])^2)))
\end{Sinput}
\begin{Soutput}
[1] 0.5040313 2.0202019
\end{Soutput}
\begin{Sinput}
> # compare to criterion value
> diag(lp.mdfa.uc[[2]])
\end{Sinput}
\begin{Soutput}
[1] 0.2736893 0.2782641
\end{Soutput}
\begin{Sinput}
> # case 2: apply the lc MDFA concurrent filter 
> x.trend.mdfa11 <- filter(x.sim[,1],lp.mdfa.lc[[1]][1,1,],method="convolution",sides=1)
> x.trend.mdfa12 <- filter(x.sim[,2],lp.mdfa.lc[[1]][1,2,],method="convolution",sides=1)
> x.trend.mdfa21 <- filter(x.sim[,1],lp.mdfa.lc[[1]][2,1,],method="convolution",sides=1)
> x.trend.mdfa22 <- filter(x.sim[,2],lp.mdfa.lc[[1]][2,2,],method="convolution",sides=1)
> x.trend.mdfa <- cbind(x.trend.mdfa11 + x.trend.mdfa12,x.trend.mdfa21 + x.trend.mdfa22)
> x.trend.mdfa <- x.trend.mdfa[(len+1):(T-len),] 
> # compare in-sample performance
> print(c(mean((x.trend.ideal[,1] - x.trend.mdfa[,1])^2),
+ 	mean((x.trend.ideal[,2] - x.trend.mdfa[,2])^2)))
\end{Sinput}
\begin{Soutput}
[1] 0.06799313 0.07528884
\end{Soutput}
\begin{Sinput}
> # compare to criterion value
> diag(lp.mdfa.lc[[2]])
\end{Sinput}
\begin{Soutput}
[1] 1.742760 1.615715
\end{Soutput}
\begin{Sinput}
> # case 3: apply the tsc MDFA concurrent filter 
> x.trend.mdfa11 <- filter(x.sim[,1],lp.mdfa.tsc[[1]][1,1,],method="convolution",sides=1)
> x.trend.mdfa12 <- filter(x.sim[,2],lp.mdfa.tsc[[1]][1,2,],method="convolution",sides=1)
> x.trend.mdfa21 <- filter(x.sim[,1],lp.mdfa.tsc[[1]][2,1,],method="convolution",sides=1)
> x.trend.mdfa22 <- filter(x.sim[,2],lp.mdfa.tsc[[1]][2,2,],method="convolution",sides=1)
> x.trend.mdfa <- cbind(x.trend.mdfa11 + x.trend.mdfa12,x.trend.mdfa21 + x.trend.mdfa22)
> x.trend.mdfa <- x.trend.mdfa[(len+1):(T-len),] 
> # compare in-sample performance
> print(c(mean((x.trend.ideal[,1] - x.trend.mdfa[,1])^2),
+ 	mean((x.trend.ideal[,2] - x.trend.mdfa[,2])^2)))
\end{Sinput}
\begin{Soutput}
[1] 0.4489955 2.8995745
\end{Soutput}
\begin{Sinput}
> # compare to criterion value
> diag(lp.mdfa.tsc[[2]])
\end{Sinput}
\begin{Soutput}
[1] 0.2735489 0.2779873
\end{Soutput}
\begin{Sinput}
> # case 4: apply the ltsc MDFA concurrent filter 
> x.trend.mdfa11 <- filter(x.sim[,1],lp.mdfa.ltsc[[1]][1,1,],method="convolution",sides=1)
> x.trend.mdfa12 <- filter(x.sim[,2],lp.mdfa.ltsc[[1]][1,2,],method="convolution",sides=1)
> x.trend.mdfa21 <- filter(x.sim[,1],lp.mdfa.ltsc[[1]][2,1,],method="convolution",sides=1)
> x.trend.mdfa22 <- filter(x.sim[,2],lp.mdfa.ltsc[[1]][2,2,],method="convolution",sides=1)
> x.trend.mdfa <- cbind(x.trend.mdfa11 + x.trend.mdfa12,x.trend.mdfa21 + x.trend.mdfa22)
> x.trend.mdfa <- x.trend.mdfa[(len+1):(T-len),] 
> # compare in-sample performance
> print(c(mean((x.trend.ideal[,1] - x.trend.mdfa[,1])^2),
+ 	mean((x.trend.ideal[,2] - x.trend.mdfa[,2])^2)))
\end{Sinput}
\begin{Soutput}
[1] 0.08670634 0.10045010
\end{Soutput}
\begin{Sinput}
> # compare to criterion value
> diag(lp.mdfa.ltsc[[2]])
\end{Sinput}
\begin{Soutput}
[1] 1.721584 1.590321
\end{Soutput}
\end{Schunk}
 
 




HERE  add exercise on ideal low-pass with various constraints 
  repeat with VAR(1) with linear trend

% 
% We suppose the true process is a VAR($1$), and apply the MB trend filter defined in Example \ref{exam:trend-i1},
%   where the parameters are given by
% \[
%  \Sigma_{\mu} = \left[ \begin{array}{ll} 
%    2.32 \cdot 10^{-4} &  5.04 \cdot 10^{-4} \\
%    5.04 \cdot 10^{-4}  & 34.73 \cdot 10^{-4}  \end{array}  \right]
%  \qquad  \Sigma_{\iota} = \left[ \begin{array}{ll}
%         110.44 \cdot 10^{-5} &  7.17 \cdot 10^{-5}  \\
%         7.17 \cdot 10^{-5} & 128.57 \cdot 10^{-5}   \end{array} \right].
% \]
%   Because the trend variance for the second component is $15$ times larger than that of the first component,
%  the correspoding trend filter  does less smoothing.  
%  
% \begin{figure}[htb!]
% \centering
% \includegraphics[]{petrolVAR1trends}
% \caption{\baselineskip=10pt Bivariate  trend  filter applied to VAR($1$) simulation (grey), with trends in black. }
% \label{fig:petrol.var1trends}
% \end{figure}
% 
% 
%   We seek to solve the corresponding trend extraction LPP.    First, we can use the optimal solution  (\ref{eq:var1.lpp-opt})
% given in   Illustration \ref{ill:var1}, supposing that we know  that the VAR($1$) is correctly specified.  
%  Second, we can use MDFA, proceeding as if we do not know the true process is a VAR($1$), as we would in practice, 
% and hence use the periodogram;  MDFA should be able to replicate the optimal solution, so long as the filter class
%  $\mathcal{G}$ is sufficiently rich.    The VAR($1$)  is defined by
% \[
%   X_t =  \left[ \begin{array}{ll}  1.0  & 0.5 \\    -0.2  &  0.3  \end{array} \right] \, X_{t-1} + \epsilon_t,
% \]
%  with stationary initialization, and $\{ \epsilon_t \}$ a Gaussian white noise of identity innovation variance.
%   Operationally, we simulate this process with sample size $4500$.  Then the 
%   ideal trends $\Psi (B) X_t$ are produced by truncating the  MB  filter to length $4001$ (it is symmetric, so the indices
%  range between $-2000$ and $2000$) and applying to the  simulation, only retaining the central $500$ data points,
%  as displayed in Figure \ref{fig:petrol.var1trends}.   (In this way we can dispense with edge effects, and the extra
%  $4000$ observations are not used in the MDFA.)     The grey lines of Figure \ref{fig:petrol.var1trends}
%  are the central $500$ observations of the VAR($1$)
%  simulation, and the  black  line is the target.     We wish to use MDFA (setting $q=30$) 
%  with various constraints (LC, TSC, LTSC) 
% to obtain a   real-time estimate,  comparing  the result  to the optimal solution given by implementing  Illustration \ref{ill:var1}.  
%   In that case  we find that
% \[
%   A_{\Psi} (\Phi) = \left[ \begin{array}{ll}  0.317  &  0.218 \\    -0.054  &  0.027  \end{array} \right]
% \]
%  by direct calculation, 
% and hence the optimal filter is easily computed.  The in-sample MSEs of the various methods are displayed in Table \ref{tab:petrol.var1.mdfa}.
%   Note that the basic MDFA (no constraints) replicates the optimal filter, as their MSE is the same up to negligible   error.
%   When imposing a level constraint (LC and LTSC) there is a loss to the MDFA performance, which makes sense given that the optimal
%  filter does {\it not} impose a level constraint -- in fact, the value of the optimal concurrent filter at frequency zero is
% \[
%   \widehat{\Psi} (1) =  \left[ \begin{array}{ll}  0.914 &  0.251 \\    -0.030  &  0.842  \end{array} \right],
% \]
%  which is quite different from $1_2$.  On the other hand, the time shift constraint alone (TSC) has little impact on the performance of MDFA,
%  because $\partial \widehat{\Psi} (1) \approx 0 \cdot 1_2$, i.e., the optimal filter already has this property of zero time shift.
% 
%  
% 
% \begin{table}[!htb]
% \centering
% \begin{tabular}{clllll}
% \hline
%   Series  &  LPP Opt  &  MDFA Basic &  MDFA LC &  MDFA TSC   &  MDFA LTSC \\
%   1 & .2404  &  .2388  &  .2877  &   .2363  & .4367  \\
%   2 & .0224 &   .0217 &   .0229  &   .0216  & .0264 \\
% \hline
% \end{tabular}
% \caption{\baselineskip=10pt  LPP MSE for bivariate VAR($1$) process --  with target trend
%  given by the LLM MB trend -- for various concurrent filters: LPP Opt is the optimal filter,
%  whereas the MDFA filters are labeled according to the constraints imposed. }
% \label{tab:petrol.var1.mdfa}
% \end{table}
%  




\section{Background on Non-stationary Vector Time Series }
\label{sec:non-stat}

We next consider processes that when differenced are 
stationary, which are the most common type occuring in econometrics and finance.  
 This type of non-stationary process substantially broadens
  the possible types of applications over the stationary processes
   considered in Chapters \ref{chap:lpp} and \ref{chap:basic}.
  Also, as such processes typically can have a time-varying mean,
  they also necessitate the use of filter constraints such as those
   considered in Section \ref{sec:constraint}.
  
 We suppose that there exists a polynomial $\delta (L)$
  that reduces each component series of $\{ X_t \}$ to a stationary
   time series (which is allowed to have a non-zero constant mean),
   and suppose this is the minimal degree polynomial that accomplishes
    this reduction.  We write $\partial X_t = \delta (L) \, X_t$ for
  the stationary, differenced time series.
    
HERE :  do examples with $1-L$  and $1-L^2$, derive time-varying
  spectral rep (from old notes on evol spectra, and DFA paper).
  
  
  HERE : state the following as a proposition
    
So we can write
\[
 X_t = \sum_{j=1}^d A_{j, t+d} \, X_{j-d} + \int_{-\pi}^{\pi}
 \frac{ e^{i \omega t} - \sum_{j=1}^d A_{j,t+d} \, e^{-i \omega
 (d-j)} }{ \delta (e^{-i \omega}) } \; d \ZZ (\omega),
\]
 where $d = \sum_{\ell=1}^m q_{\ell}$ and each $A_{j, t+d}$ is a
 time varying function for each $j$, and the $x_{j-d}$ are initial
 values.  This representation is chiefly useful when $t > 1$, though
 it is still valid when $t \leq 0$.  The $\ZZ (\omega)$ is the
 orthogonal increments process in the spectral representation of
 $\partial X_t$. 

 Each of the time-varying functions is in the null
 space of $\delta (B)$, i.e., $\delta (B) A_{j, t+d} = 0$ for $1 \leq
 j \leq d$, where the backshift operator works on the $t+d$ index.
 As a consequence, we can rewrite each $A_{j,t+d}$ as a linear
 combination of the basis functions of the null space of $\delta
 (B)$, which yields a more convenient representation.  Let the basis
 functions be $\phi_j (t) $ for $1 \leq j \leq d$; the existence and
 form of such functions are a basic staple of difference equation
 theory, treated briefly in Brockwell and Davis (1991).  Then we can
 write $A_{j,t+d} = \sum_{k=1}^d G_{jk} \phi_k (t)$ for each $1 \leq
 j \leq d$, for some coefficients $G_{jk}$.  It follows that
\[
 \sum_{j=1}^d A_{j,t+d} B^{d-j} = \sum_{k=1}^d \phi_k (t)  \; \left(
 \sum_{j=1}^d G_{jk} B^{d-j} \right).
\]
 Each expression in parentheses on the right hand side is a degree
 $d-1$ polynomial in $B$, and will henceforth be denoted as $p^{(k)}
 (B)$.   Substituting the new formulation, we obtain
\[
 x_t = \sum_{j=1}^d \phi_j (t) p^{(j)} (B) \, x_{0} + \int_{-\pi}^{\pi}
 \frac{ e^{i \omega t} - \sum_{j=1}^d \phi_j (t) \, p^{(j)} ( e^{-i \omega
 } )}{ \delta (e^{-i \omega}) } \; d \ZZ (\omega),
\]
 where $p^{(j)} (B)$ acts on $x_0$ by shifting the time index $t=0$
 back in time for each power of $B$.  This representation is now
 extremely convenient, because application of any factor of
 $\delta(B)$ will annihilate a corresponding basis function (when
 roots are repeated, some basis functions will also be transformed
 into others that are instead annihilated). 

   
  HERE material on WK : signal and noise spectra non-stat case, and basic formulas
    in freq domain
    
  HERE illustrations by LLM, STM, and Example 5 of E and S paper
  
  

\section{Error Criterion and Computation}
\label{sec:mdfa-nonstat}

HERE material from E and S paper section 4.3

% We here consider difference-stationary vector time series, which means there exists a scalar differencing polynomial $\delta (L)$ such
%  that $\partial X_t = \delta (L) X_t$ is mean zero and covariance stationary.  
%  Examination of (\ref{eq:dfa-error}) indicates that the error process is not stationary unless we make certain assumptions
%  about $\Delta (L) = \Psi (L) - \widehat{\Psi} (L)$.     It is necessary that we can factor $\delta (L)$ from $\Delta (L)$, i.e., there exists
%  $\widetilde{\Delta } (L)$ such that
% \begin{equation}
%  \label{eq:delta.factor}
%   \Delta (L) = \widetilde{\Delta } (L) \, \delta (L),
% \end{equation}
%  as otherwise we cannot guarantee that $\{ E_t \}$ will be stationary.  However, (\ref{eq:delta.factor}) is sufficient to guarantee
%  that the filter error be stationary, because
% \[
%   E_t = \widetilde{\Delta} (L) \, \partial X_t
% \]
%  in such a case.   We next discuss a set of filter constraints that guarantee (\ref{eq:delta.factor}), beginning with a lemma
%  that discusses factorization of filters.  We say a filter $\Psi (L)$ is absolutely convergent if $\sum_{j \in \ZZ} \| \psi (j) \| < \infty$
%  for a given matrix norm $\| \cdot \|$.
% 
% \begin{Proposition}
% \label{prop:filter-decompose}
%  Any linear filter $\Psi (L)$ can be expressed as
% \[
%   \Psi (L) = \Psi (\zeta) + (L - \zeta) \, \Psi^{\sharp} (L)
% \]
%  for any $\zeta \in \CC$  such that $| \zeta | = 1$, 
%   and an absolutely convergent filter $\Psi^{\sharp} (L)$, so long as  $\partial \Psi (L) $ is absolutely convergent.
%  If in addition $ \partial \partial \Psi (L) = \sum_{ j \in \ZZ} j (j-1) \, \psi (j) \, L^j$
%    is absolutely convergent, then there also exists an absolutely convergent filter $\Psi^{\flat} (L)$ 
%  such that
% \[
%  \Psi (L) = \Psi (\zeta) + \partial \Psi (\zeta) \, (L- \zeta) \, \overline{\zeta} + {(L - \zeta)}^2 \, \Psi^{\flat} (L).
% \]
% \end{Proposition}
% 
%  Note that if $\Psi (\zeta) = 0$, it follows from Proposition \ref{prop:filter-decompose} that $L-\zeta$ can be factored from
%   $\Psi (L)$.  Similarly, ${(L- \zeta)}^2$ can be factored from $\Psi (L)$ is $\Psi(\zeta) = \partial \Psi (\zeta) =0$.
% 
% \begin{Definition} \rm
% \label{def:filter-noise}
%  For $\omega \in [-\pi, \pi]$, a filter $\Psi (L)$ annihilates $\omega$-noise of order $1$ if $\Psi (e^{-i \omega}) = 0$,
%  and annihilates $\omega$-noise of order $2$ if in addition $\partial \Psi (e^{-i \omega}) = 0$.
% \end{Definition}
% 
% 
% Hence, we have the following immediate corollary of Proposition \ref{prop:filter-decompose}.
% 
% \begin{Corollary}
%  \label{cor:filter-noise}
%   If a filter $\Psi (L)$ annihilates $\omega$-noise of order $1$ and $\partial \Psi (L)$ is absolutely convergent, then
% \[
%   \Psi (L) = (L- e^{-i \omega}) \, \Psi^{\sharp} (L).
% \]
%  If a filter $\Psi (L)$ annihilate $\omega$-noise of order $2$,  and $\partial \partial \Psi (L)$ is absolutely convergent, then
% \[
%   \Psi (L) = {(L- e^{-i \omega}) }^2 \, \Psi^{\flat} (L).
% \]
% \end{Corollary}
% 
%  We can apply Corollary \ref{cor:filter-noise} to factor a noise-differencing polynomial $\delta^N (L)$ from $\Delta (L)$:
%  for each $\omega$ such that the target filter $\Psi (L)$ annihilate $\omega$-noise of order $d$, we impose the constraint
%  that $\widehat{\Psi} (L)$ shall have the same property, and hence ${(L- e^{-i \omega})}^d$ can be factored from both
%  filters.   For instance, if noise frequencies are $\omega_{\ell}$ with multiplicities $d_{\ell}$, then repeated application of 
%  Corollary \ref{cor:filter-noise} yields
% \[
%  \Psi (L) = \prod_{\ell} {(L -  e^{-i \omega_{\ell}})}^{d_{\ell}} \, \Psi^{\natural} (L)
%    = \delta^N (L) \, \Psi^{\star} (L)
% \]
%  for some residual filter $\Psi^{\natural} (L)$, where $\Psi^{\star} (L) = \prod_{\ell} -e^{-i \omega_{\ell} d_{\ell}} \, \Psi^{\natural} (L)$
%  and $\delta^N (L) = \prod_{\ell} (1 - e^{i \omega_{\ell}} \, L)$.
%  By imposing the same linear constraints on $\widehat{\Psi} (L)$, we likewise obtain $\widehat{\Psi} (L) = \delta^N (L) \, \widehat{\Psi}^{\star} (L)$,
%  and hence 
% \begin{equation}
%  \label{eq:delta-noise}
% \Delta (L) = \left(  {\Psi}^{\star} (L) - \widehat{\Psi}^{\star} (L) \right) \, \delta^N (L).
% \end{equation}
%   So if $\delta (L) = \delta^N (L)$, then (\ref{eq:delta.factor}) holds at once.  More generally, a given process' differencing polynomial
%  may be factored into relatively prime polynomials $\delta^N (z)$ and $\delta^S (z)$, which correspond to noise and signal dynamics
%  respectively -- see Bell (1984) and McElroy (2008a).  Many  signal extraction filters $\Psi (L)$   have the property that they
%  annihilate $\omega$-noise of the appropriate order, such that $\delta^N (L)$ can be factored; in addition, the noise filter $1_N - \Psi (L)$
%  has the same property with respect to the signal frequencies, i.e., $\delta^S (L)$ can be factored from $1_N - \Psi (L)$ in the same manner.
%  Hence  $1_N -  \Psi (L) =   \delta^S (L) \, \Psi^{\diamond} (L)$ for some factor $\Psi^{\diamond} (L)$,
%  and imposing the same constraints on the concurrent filter yields
% \[
%   \Delta (L) = (1_N - \widehat{\Psi} (L)) - (1_N - \Psi (L)) = \left(  \widehat{\Psi}^{\diamond} (L) - \Psi^{\diamond} (L)  \right) \, \delta^S (L).
% \]
%   However, (\ref{eq:delta-noise}) also holds, and the roots of $\delta^S (z)$ and $\delta^N (z)$ are distinct (because the polynomials
%  are relatively prime by assumption), and hence $\delta (L) = \delta^N (L) \, \delta^S (L)$ must be a factor.  Therefore,
%  $\widetilde{\Delta} (L) =  (  \widehat{\Psi}^{\diamond} (L) - \Psi^{\diamond} (L)   )/ \delta^N (L)$, and (\ref{eq:delta.factor}) holds.
% 
% In summary, given a factorization of $\delta (z)$ into signal and noise differencing polynomials, the noise constraints and signal constraints
%  on $\Psi (L)$ must also be imposed upon $\widehat{\Psi} (L)$, and this ensures that $\{ E_t \}$ will be stationary with mean zero.  
%  If $\omega$ satisfies $\delta^N (e^{-i \omega}) = 0$, then we impose that $\widehat{\Psi} (L)$ annihilates $\omega$-noise of order
%  given by the multiplicity of the root in $\delta^N (z)$.  Otherwise, if $\omega$ satisfies $\delta^S (e^{-i \omega})$ then we impose
%  that $\widehat{\Psi} (e^{-i \omega}) = \Psi (e^{-i \omega})$ (if the root is simple -- if a double root, then also impose that
%  $\partial \widehat{\Psi} (e^{-i \omega}) = \partial \Psi (e^{-i \omega})$).  In practice, we must determine the real and imaginary  parts of each such constraint, and write the corresponding constraints on $\widehat{\Psi} (L)$ in the form $A = [J \otimes 1_N] \, \vartheta$ for
%   filters of form (\ref{eq:conc.filter}), applying the methodology of the previous subsection.  
%   With these constraints in play, the formula (\ref{eq:dfa-mvar}) holds with $\Psi (z) - \widehat{\Psi} (z)$ replaced by $\widetilde{\Delta} (z)$
%  and $F$ being the spectral density of $\{ \partial X_t \}$, i.e., we define the nonstationary MDFA criterion 
%  function as $\det D_{\Psi } (\vartheta, G)$ for
% \begin{equation}
% \label{eq:mdfa-criterion-nonstat}
%  D_{\Psi} (\vartheta, G) =     { \langle  \widetilde{\Delta} (z)   \,   G \,  {\widetilde{\Delta} (z) }^*   \rangle }_0
%  = { \langle  \left[ \Psi (z) -   \widehat{\Psi}_{\vartheta} (z) \right] \,   G \, {|\delta (z) |}^{-2} \,
%   {  \left[ \Psi (z) -  \widehat{\Psi}_{\vartheta} (z) \right] }^{*} \rangle }_0.
% \end{equation}
%   The second expression in (\ref{eq:mdfa-criterion-nonstat}) utilizes (\ref{eq:delta.factor}), and employs the understanding
%  that poles in ${\delta (z) }^{-1}$ are exactly canceled out by the corresponding zeros in $\Psi (z) - \widehat{\Psi} (z)$.
%   Moreover, the ratio $(\Psi (z) - \widehat{\Psi} (z))/\delta (z) = \widetilde{\Delta} (z)$ is bounded in $\omega$ for $z = e^{-i \omega}$,
%  as the previous discussion guarantees.  As a matter of convenience, given that the frequencies of singularity in
%  ${|\delta (z) |}^{-2}$ are a set of Lebesgue measure zero, calculation of $D_{\Psi} (\vartheta, G)$ can proceed by using
%  the second expression, computing the numerical integration over only those frequencies where $\delta (z)$ is nonzero.
%   Whereas the theoretical filter error MSE is given by $D_{\Psi, F}$, with $F$ being the spectral density of $\{ \partial X_t \}$,
%  for estimation we approximate the integral over Fourier frequencies, and utilize the periodogram of the differenced data for $G$.
%  Again, we omit any contributions to the sum arising from Fourier frequencies that correspond to zeros of $\delta (z)$, as such an omission
%  only results in a loss of order $T^{-1}$.  (The alternative is to compute the quantities $\widetilde{\Delta} (z)$ at Fourier frequencies,
%  using the factorization results of Corollary  \ref{cor:filter-noise}; this is not worth the effort in practical applications.)

HERE revisit section 1 exercises now with RW and diff - VAR(1), using LTSC.  Petrol ex


% Exercise 1
% 
% # Simulate a Gaussian VAR(1)  
% T.sim <- 500 + 2*len
% N <- 2
% phi.matrix <- rbind(c(1,.5),c(-.2,.3))
% innovar.matrix <- diag(N)
% true.psidelta <- var1.par2psi(phi.matrix,100)
% gamma.0 <- matrix(solve(diag(N^2) - phi.matrix %x% phi.matrix) %*% 
% 	matrix(innovar.matrix,ncol=1),nrow=N)
% x.init <- t(chol(gamma.0)) %*% rnorm(N)
% x.next <- x.init
% x.sim <- NULL
% for(t in 1:T.sim)
% {
% 	x.next <- phi.matrix %*% x.next + rnorm(N)
% 	x.sim <- cbind(x.sim,x.next)
% }
% x.sim <- ts(t(x.sim))
% x.acf <- acf(x.sim,type="covariance",plot=FALSE,lag.max=T.sim)[[1]]
% x.acf <- aperm(aperm(x.acf,c(3,2,1)),c(2,1,3))
% 
% # construct and apply BW filter
% bw.filter <- wk.trend[[1]]
% x.trend.bw11 <- filter(x.sim[,1],bw.filter[1,1,],method="convolution",sides=2)
% x.trend.bw12 <- filter(x.sim[,2],bw.filter[1,2,],method="convolution",sides=2)
% x.trend.bw21 <- filter(x.sim[,1],bw.filter[2,1,],method="convolution",sides=2)
% x.trend.bw22 <- filter(x.sim[,2],bw.filter[2,2,],method="convolution",sides=2)
% x.trend.bw <- cbind(x.trend.bw11 + x.trend.bw12,x.trend.bw21 + x.trend.bw22)
% x.trend.bw <- x.trend.bw[(len+1):(T.sim-len),] 
%  
% 
% # visualize
% pdf(file="petrolVAR1trends.pdf",width=4, height=4.6)
% par(oma=c(2,0,0,0),mar=c(2,4,2,2)+0.1,mfrow=c(2,1),cex.lab=.8)
% plot(ts(x.sim[(len+1):(T.sim-len),1]),ylab="Series 1",xlab="",yaxt="n",xaxt="n",col=grey(.7))
% axis(1,cex.axis=.5)
% axis(2,cex.axis=.5)
% lines(x.trend.bw[,1],col=1,lwd=2)
% #lines(ts(x.sim[(len+1):(T.sim-len),1]))
% plot(ts(x.sim[(len+1):(T.sim-len),2]),ylab="Series 2",xlab="",yaxt="n",xaxt="n",col=grey(.7))
% axis(1,cex.axis=.5)
% axis(2,cex.axis=.5)
% lines(x.trend.bw[,2],col=1,lwd=2)
% #lines(ts(x.sim[(len+1):(T.sim-len),2]))
% mtext("Time", side = 1, line = 1,outer=TRUE)
% dev.off()
% 
% # get LPP soln using true VAR(1)
% A.next <- diag(2)
% A.phi <- matrix(0,2,2)
% for(j in 1:len)
% {
% 	A.next <- A.next %*% phi.matrix
% 	A.phi <- A.phi + bw.filter[,,(len+1-j)] %*% A.next
% }
% bw.lpp <- bw.filter[,,(len+1):(2*len+1)]
% bw.lpp[,,1] <- bw.lpp[,,1] + A.phi
% 
% 
% # get MDFA concurrent filter
% q <- 30
% Grid <- T.sim
% frf.trend <- mdfa.frf(wk.trend[[1]],len,Grid)
% spec.hat <- mdfa.pergram(x.sim,1)
% # choose one of the four constraint scenarios for MDFA	
% bw.mdfa <- mdfa.unconstrained(frf.trend,spec.hat,q)
% bw.mdfa <- mdfa.levelconstraint(frf.trend,spec.hat,q)
% bw.mdfa <- mdfa.tsconstraint(frf.trend,spec.hat,q)
% bw.mdfa <- mdfa.ltsconstraint(frf.trend,spec.hat,q)
%  
% # apply the MDFA concurrent filter
% x.trend.mdfa11 <- filter(x.sim[,1],bw.mdfa[[1]][1,1,],method="convolution",sides=1)
% x.trend.mdfa12 <- filter(x.sim[,2],bw.mdfa[[1]][1,2,],method="convolution",sides=1)
% x.trend.mdfa21 <- filter(x.sim[,1],bw.mdfa[[1]][2,1,],method="convolution",sides=1)
% x.trend.mdfa22 <- filter(x.sim[,2],bw.mdfa[[1]][2,2,],method="convolution",sides=1)
% x.trend.mdfa <- cbind(x.trend.mdfa11 + x.trend.mdfa12,x.trend.mdfa21 + x.trend.mdfa22)
% x.trend.mdfa <- x.trend.mdfa[(len+1):(T.sim-len),] 
% 
% # construct and apply concurrent BW filter
% conc.filter <- conc.trend
% x.trend.conc11 <- filter(x.sim[,1],rev(conc.filter[1,1,]),method="convolution",sides=1)
% x.trend.conc12 <- filter(x.sim[,2],rev(conc.filter[1,2,]),method="convolution",sides=1)
% x.trend.conc21 <- filter(x.sim[,1],rev(conc.filter[2,1,]),method="convolution",sides=1)
% x.trend.conc22 <- filter(x.sim[,2],rev(conc.filter[2,2,]),method="convolution",sides=1)
% x.trend.conc <- cbind(x.trend.conc11 + x.trend.conc12,x.trend.conc21 + x.trend.conc22)
% x.trend.conc <- x.trend.conc[(len+1):(T.sim-len),] 
% 
% # construct and apply optimal LPP filter
% x.trend.lpp11 <- filter(x.sim[,1],bw.lpp[1,1,],method="convolution",sides=1)
% x.trend.lpp12 <- filter(x.sim[,2],bw.lpp[1,2,],method="convolution",sides=1)
% x.trend.lpp21 <- filter(x.sim[,1],bw.lpp[2,1,],method="convolution",sides=1)
% x.trend.lpp22 <- filter(x.sim[,2],bw.lpp[2,2,],method="convolution",sides=1)
% x.trend.lpp <- cbind(x.trend.lpp11 + x.trend.lpp12,x.trend.lpp21 + x.trend.lpp22)
% x.trend.lpp <- x.trend.lpp[(len+1):(T.sim-len),] 
% 
% 
% 
% # visualize
% par(oma=c(2,0,0,0),mar=c(2,4,2,2)+0.1,mfrow=c(2,1),cex.lab=.8)
% plot(ts(x.trend.bw[,1]),ylab="",xlab="",yaxt="n",xaxt="n",col=1,lwd=1,
% 	ylim=c(min(x.trend.bw[,1])-4,max(x.trend.bw[,1])))
% axis(1,cex.axis=.5)
% axis(2,cex.axis=.5)
% lines(x.trend.mdfa[,1]-2,col=grey(.4),lwd=1,lty=1)
% lines(x.trend.lpp[,1]-4,col=grey(.6),lwd=1,lty=1)
% plot(ts(x.trend.bw[,2]),ylab="",xlab="",yaxt="n",xaxt="n",col=1,lwd=1,
% 	ylim=c(min(x.trend.bw[,2])-4,max(x.trend.bw[,2])))
% axis(1,cex.axis=.5)
% axis(2,cex.axis=.5)
% lines(x.trend.mdfa[,2]-2,col=grey(.4),lwd=1,lty=1)
% lines(x.trend.lpp[,2]-4,col=grey(.6),lwd=1,lty=1)
% mtext("Time", side = 1, line = 1,outer=TRUE)
%  
% # compare in-sample performance: MDFA for series, and then Conc for series
% print(c(mean((x.trend.bw[,1] - x.trend.mdfa[,1])^2),
% 	mean((x.trend.bw[,2] - x.trend.mdfa[,2])^2)))
% print(c(mean((x.trend.bw[,1] - x.trend.conc[,1])^2),
% 	mean((x.trend.bw[,2] - x.trend.conc[,2])^2)))
% print(c(mean((x.trend.bw[,1] - x.trend.lpp[,1])^2),
% 	mean((x.trend.bw[,2] - x.trend.lpp[,2])^2)))
% 
% # compare to criterion value
% print(diag(bw.mdfa[[2]]))
% 
% 

% 
% 
% ##########################################################
% ### Exercise 2: replication - simulate bivariate LLM and apply MDFA
% 
% # Simulate a Gaussian LLM  
% psi.sim <- c(2.17150287559847+1i, -8.36795922528+1i, -6.04133725367594+1i, 
% 0.0648981656699+1i, -6.80849700177184+1i, -6.66004335288479+1i, 
% -0.00016098322952+1i, 0.00051984185863+1i)
% psi.sim[7:8] <- c(0+1i,0+1i)
% psi.sim <- sigex.par2psi(param.sim,flag.default,mdl)
% T.sim <- 500 + 2*len
% N <- 2
% burnin <- 0
% init <- matrix(rnorm(2*N),ncol=N)
% 
%  
% x.sim <- sigex.sim(psi.sim,mdl,T.sim,burnin,Inf,init)
% plot(ts(x.sim))
% 
% # construct and apply BW filter
% bw.filter <- wk.trend[[1]]
% x.trend.bw11 <- filter(x.sim[,1],bw.filter[1,1,],method="convolution",sides=2)
% x.trend.bw12 <- filter(x.sim[,2],bw.filter[1,2,],method="convolution",sides=2)
% x.trend.bw21 <- filter(x.sim[,1],bw.filter[2,1,],method="convolution",sides=2)
% x.trend.bw22 <- filter(x.sim[,2],bw.filter[2,2,],method="convolution",sides=2)
% x.trend.bw <- cbind(x.trend.bw11 + x.trend.bw12,x.trend.bw21 + x.trend.bw22)
% x.trend.bw <- x.trend.bw[(len+1):(T.sim-len),] 
% 
% # visualize
% pdf(file="petrolLLMtrendsNull.pdf",width=4, height=4.6)
% par(oma=c(2,0,0,0),mar=c(2,4,2,2)+0.1,mfrow=c(2,1),cex.lab=.8)
% plot(ts(x.sim[(len+1):(T.sim-len),1]),ylab="Consumption",xlab="",yaxt="n",xaxt="n",col=grey(.7))
% axis(1,cex.axis=.5)
% axis(2,cex.axis=.5)
% lines(x.trend.bw[,1],col=1,lwd=1)
% plot(ts(x.sim[(len+1):(T.sim-len),2]),ylab="Imports",xlab="",yaxt="n",xaxt="n",col=grey(.7))
% axis(1,cex.axis=.5)
% axis(2,cex.axis=.5)
% lines(x.trend.bw[,2],col=1,lwd=1)
% mtext("Time", side = 1, line = 1,outer=TRUE)
% dev.off()
% 
% # get MDFA concurrent filter for I(1) case
% q <- 30
% x.diff <- diff(x.sim)
% spec.hat <- mdfa.pergram(x.diff,c(1,-1))
% Grid <- T.sim - 1
% frf.trend <- mdfa.frf(wk.trend[[1]],len,Grid)
% #bw.mdfa <- mdfa.levelconstraint(frf.trend,spec.hat,q)[[1]]
% constraints.mdfa <- mdfa.getconstraints(frf.trend,0,NULL,q)
% bw.mdfa <- mdfa.filter(frf.trend,spec.hat,constraints.mdfa[[1]],constraints.mdfa[[2]])[[1]]
% 
% # apply the MDFA concurrent filter
% x.trend.mdfa11 <- filter(x.sim[,1],bw.mdfa[1,1,],method="convolution",sides=1)
% x.trend.mdfa12 <- filter(x.sim[,2],bw.mdfa[1,2,],method="convolution",sides=1)
% x.trend.mdfa21 <- filter(x.sim[,1],bw.mdfa[2,1,],method="convolution",sides=1)
% x.trend.mdfa22 <- filter(x.sim[,2],bw.mdfa[2,2,],method="convolution",sides=1)
% x.trend.mdfa <- cbind(x.trend.mdfa11 + x.trend.mdfa12,x.trend.mdfa21 + x.trend.mdfa22)
% x.trend.mdfa <- x.trend.mdfa[(len+1):(T.sim-len),] 
% 
% # construct and apply concurrent BW filter
% conc.filter <- conc.trend
% x.trend.conc11 <- filter(x.sim[,1],rev(conc.filter[1,1,]),method="convolution",sides=1)
% x.trend.conc12 <- filter(x.sim[,2],rev(conc.filter[1,2,]),method="convolution",sides=1)
% x.trend.conc21 <- filter(x.sim[,1],rev(conc.filter[2,1,]),method="convolution",sides=1)
% x.trend.conc22 <- filter(x.sim[,2],rev(conc.filter[2,2,]),method="convolution",sides=1)
% x.trend.conc <- cbind(x.trend.conc11 + x.trend.conc12,x.trend.conc21 + x.trend.conc22)
% x.trend.conc <- x.trend.conc[(len+1):(T.sim-len),] 
% 
% # visualize
% pdf(file="petrolLLMrealtimeNull.pdf",width=4, height=4.6)
% par(oma=c(2,0,0,0),mar=c(2,4,2,2)+0.1,mfrow=c(2,1),cex.lab=.8)
% plot(ts(x.trend.bw[,1]),ylab="Consumption",xlab="",yaxt="n",xaxt="n",col=1,lwd=1,
% 	ylim=c(min(x.trend.bw[,1])-2/2,max(x.trend.bw[,1])))
% axis(1,cex.axis=.5)
% axis(2,cex.axis=.5)
% lines(x.trend.mdfa[,1]-1/2,col=grey(.4),lwd=1,lty=1)
% lines(x.trend.conc[,1]-2/2,col=grey(.6),lwd=1,lty=1)
% plot(ts(x.trend.bw[,2]),ylab="Imports",xlab="",yaxt="n",xaxt="n",col=1,lwd=1,
% 	ylim=c(min(x.trend.bw[,2])-2,max(x.trend.bw[,2])))
% axis(1,cex.axis=.5)
% axis(2,cex.axis=.5)
% lines(x.trend.mdfa[,2]-1,col=grey(.4),lwd=1,lty=1)
% lines(x.trend.conc[,2]-2,col=grey(.6),lwd=1,lty=1)
% mtext("Time", side = 1, line = 1,outer=TRUE)
% dev.off()
%  
% # compare in-sample performance: MDFA for series, and then Conc for series
% print(c(mean((x.trend.bw[,1] - x.trend.mdfa[,1])^2),
% 	mean((x.trend.bw[,2] - x.trend.mdfa[,2])^2)))
% print(c(mean((x.trend.bw[,1] - x.trend.conc[,1])^2),
% 	mean((x.trend.bw[,2] - x.trend.conc[,2])^2)))
% 
% 



HERE  HP exercise with connection to STM, and MB replication.  Ndc ex

HERE CF apply to RW

HERE  Starts exercise
%  note: MDFA_con replaces Constraints,
%    includes material from Replication

%------------------------------------------------------------------


\def\pf{{\bf Proof. }}
\def\logimplies{\Rightarrow}
\def\convinlaw{\stackrel{{\cal L}}{\Longrightarrow }}
\def\convinp{\stackrel{P}{\longrightarrow }}
\def\convas{\stackrel{a.s.}{\longrightarrow }}
\def\convv{\stackrel{v}{\longrightarrow}}
\def\asymp{\stackrel{{\mathbb P}}{\sim}}
\def\RR{\mathbb R}
\def\ZZ{\mathbb Z}
\def\QQ{\mathbb Q}
\def\NN{\mathbb N}
\def\MM{\mathbb M}
\def\LL{\mathbb L}
\def\EE{\mathbb E}
\def\PP{\mathbb P}
\def\DD{\mathbb D}
\def\WW{\mathbb W}
\def\FF{\mathbb F}
\def\II{\mathbb I}
\def\FF{\mathbb F}
\def\ttheta{\widetilde{\theta}}
\def\tTheta{\widetilde{\Theta}}
\def\tsig{\widetilde{\sigma}^2}
\def\tc{\widetilde{c}}
\def\etheta{\widehat{\theta}}
\def\eTheta{\widehat{\Theta}}
\def\esig{\widehat{\sigma}^2}
\def\ptheta{\underline{\theta}}
\def\pTheta{\underline{\Theta}}
\def\psig{\underline{\sigma}^2}

\def\eqinlaw{\stackrel{{\cal L}}{=}}
\def\tends{\rightarrow}
\def\tendsinf{\rightarrow\infty}
\def\isodynamo{\Leftrightarrow}





\chapter{Multivariate Direct Filter Analysis for Co-integrated Processes}
\label{chap:coint}

Chapter \ref{chap:int} provided the basic treatment of MDFA for non-stationary
 processes, but here we make an extension to treat co-integrated processes.
 This results in a change in the types of constraints that are enforced
  at signal frequencies.  Section \ref{sec:coint-zero} develops the simplest such
  case, where there is trend (or classic) co-integration, whereas Section
  \ref{sec:coint-gen} gives the general treatment.
 
\section{Co-integration at Frequency Zero} 
 \label{sec:coint-zero}

HERE Marc's material, VAR(1) coint example

HERE examples from paper Petrol and Ndc revisted with coint constraints

\section{A General Treatment of Co-integration}
\label{sec:coint-zero}
 
% 
%  We first illustrate this point through dynamic factor component models.  Let
%  the differencing polynomial be $\delta (B) = \prod_{\ell=1}^p {(1 -
%  e^{i \omega_{\ell}} B )}^{q_{\ell}}$, where $q_{\ell}$ is the
%  multiplicity of each unit root at frequency $\omega_{\ell}$.  When $\omega_{\ell}$ is not $0$ or $\pi$,
%  we know a conjugate factor must appear in $\delta (B)$.  By a convenient abuse of notation, we denote
% the pairing of such conjugate factors by ${(1 - e^{i \omega_{\ell} } B)}^{q_{\ell}}$, with the understanding that
%  $q_{\ell} $ is even and denotes the produce of conjugate factors.
% 
% Suppose that the data process can be written as the sum of
%  non-stationary latent processes, each of which has differencing
%  polynomial ${(1 - e^{i \omega_{\ell}} B )}^{q_{\ell}}$, plus a
%  residual stationary process.  We write this as
% \begin{equation}
%  \label{eq:chapnstat_structural}
%  x_t = \sum_{\ell=1}^p s^{(\ell)}_t + s^{(0)}_t,
% \end{equation}
%  where ${(1 - e^{i \omega_{\ell}} B )}^{q_{\ell}} s^{(\ell)}_t$ is
%  stationary for each $1 \leq \ell \leq p$, and $s^{(0)}_t$ is
%  stationary as well.  Let the
%  reduced polynomials $\delta^{(\ell)} (B) = \delta (B) \, {(1 -
%  e^{i \omega_{\ell}} B )}^{-q_{\ell}}$ be defined.  Then applying
%  $\delta (B)$ to the structural equation (\ref{eq:chapnstat_structural}) yields
% \[
%  \partial x_t  : = \delta^{(\ell)} (B) x_t = \sum_{\ell=1}^p \, \delta^{(\ell)} (B) \partial
%  s^{(\ell)}_t + \delta (B) s^{(0)}_t.
% \]
%  Here the $\partial$ notation before a process refers to the
%  suitably differenced version of that process, which is stationary.
%   Each stationary latent process $\partial s^{(\ell)}_t$ may have
%   singularities in its spectral density matrix, such that it can be
%   represented as $\Lambda^{(\ell)}$ times some $c^{(\ell)}_t$, a
%  stationary process of reduced dimension with spectral density
%  matrix invertible at all frequencies.  Such a latent process is
%  governed by a dynamic factor model (DFM), with $\Lambda^{(\ell)} =
%  I_m$ recovering the general case.  We actually require
%  $\Lambda^{(0)} = I_m$ in order to guarantee that the spectrum of
%  $\partial x_t$ is non-singular except at a finite number of
%  frequencies.
% 
%  Suppose that $\beta$ is a vector such that $\beta^{\prime}
%  \Lambda^{(k)} = 0$ for some $1 \leq k \leq p$.  Then
% \[
%  \beta^{\prime} \, \partial x_t = \sum_{\ell \neq k} \,
%  \beta^{\prime} \Lambda^{(\ell)} \, \delta^{(\ell)} (B)
%  c^{(\ell)}_t + \beta^{\prime} \delta (B) s^{(0)}_t,
% \]
%  and note that ${(1 - e^{i \omega_{k}} B )}^{q_{k}}$ can be factored
%  out of all terms on the right hand side.  Hence $\beta^{\prime}
%  x_t$ only requires $\delta^{(k)} (B)$ differencing to become
%  stationary; the frequency $\omega_k$ co-integrating vector $\beta$
%  reduces the order of non-stationarity by the factor ${(1 - e^{i \omega_{k}} B
%  )}^{q_{k}}$.  Moreover, if $\beta$ is in the left null space of
%  several factor loadings $\Lambda^{(\ell)}$, the order of
%  non-stationarity can be reduced further.  In an extreme case,
%  $\beta^{\prime} \Lambda^{(\ell)} = 0$ for $1 \leq \ell \leq p$, so
%  that $\beta^{\prime} x_t$ is stationary; however, whether or not
%  the factor loadings have a non-trivial intersection of left null
%  space depends on each process.
% 
%  We now proceed with a general treatment of vector non-stationary
%  processes to explore what types of filter constraints are necessary
%  when co-integrating vectors are present.  Crucially supposing that the
%  differencing operator $\delta (B)$ is the same for each component
%  series, we can write
% \[
%  x_t = \sum_{j=1}^d A_{j, t+d} \, x_{j-d} + \int_{-\pi}^{\pi}
%  \frac{ e^{i \omega t} - \sum_{j=1}^d A_{j,t+d} \, e^{-i \omega
%  (d-j)} }{ \delta (e^{-i \omega}) } \; d \ZZ (\omega),
% \]
%  where $d = \sum_{\ell=1}^m q_{\ell}$ and each $A_{j, t+d}$ is a
%  time varying function for each $j$, and the $x_{j-d}$ are initial
%  values.  This representation is chiefly useful when $t > 1$, though
%  it is still valid when $t \leq 0$.  The $\ZZ (\omega)$ is the
%  orthogonal increments process in the spectral representation of
%  $\partial x_t$. 
% 
%  Each of the time-varying functions is in the null
%  space of $\delta (B)$, i.e., $\delta (B) A_{j, t+d} = 0$ for $1 \leq
%  j \leq d$, where the backshift operator works on the $t+d$ index.
%  As a consequence, we can rewrite each $A_{j,t+d}$ as a linear
%  combination of the basis functions of the null space of $\delta
%  (B)$, which yields a more convenient representation.  Let the basis
%  functions be $\phi_j (t) $ for $1 \leq j \leq d$; the existence and
%  form of such functions are a basic staple of difference equation
%  theory, treated briefly in Brockwell and Davis (1991).  Then we can
%  write $A_{j,t+d} = \sum_{k=1}^d G_{jk} \phi_k (t)$ for each $1 \leq
%  j \leq d$, for some coefficients $G_{jk}$.  It follows that
% \[
%  \sum_{j=1}^d A_{j,t+d} B^{d-j} = \sum_{k=1}^d \phi_k (t)  \; \left(
%  \sum_{j=1}^d G_{jk} B^{d-j} \right).
% \]
%  Each expression in parentheses on the right hand side is a degree
%  $d-1$ polynomial in $B$, and will henceforth be denoted as $p^{(k)}
%  (B)$.   Substituting the new formulation, we obtain
% \[
%  x_t = \sum_{j=1}^d \phi_j (t) p^{(j)} (B) \, x_{0} + \int_{-\pi}^{\pi}
%  \frac{ e^{i \omega t} - \sum_{j=1}^d \phi_j (t) \, p^{(j)} ( e^{-i \omega
%  } )}{ \delta (e^{-i \omega}) } \; d \ZZ (\omega),
% \]
%  where $p^{(j)} (B)$ acts on $x_0$ by shifting the time index $t=0$
%  back in time for each power of $B$.  This representation is now
%  extremely convenient, because application of any factor of
%  $\delta(B)$ will annihilate a corresponding basis function (when
%  roots are repeated, some basis functions will also be transformed
%  into others that are instead annihilated). 
% 
%  Suppose that we left multiply by $\beta^{\prime}$,
%  which is a co-integrating vector at frequency $\omega_k$:
% \begin{equation}
%  \label{eq:co-intRep}
%   \beta^{\prime} x_t = \sum_{j=1}^d \phi_j (t) p^{(j)} (B) \, \beta^{\prime} \, x_{0} + \int_{-\pi}^{\pi}
%  \frac{ e^{i \omega t} - \sum_{j=1}^d \phi_j (t) \, p^{(j)} ( e^{-i \omega
%  } )}{ \delta (e^{-i \omega}) } \; \beta^{\prime} \, d \ZZ
%  (\omega).
% \end{equation}
%   From our previous discussion, we know that the result is a non-stationary
%  process with differencing operator $\delta^{(k)} (B)$; this implies
%  that there should be a cancelation of $\beta^{\prime} \, d\ZZ
%  (\omega)$ with the ${(1 - e^{i \omega_{k}} e^{-i\omega}
%  )}^{q_{k}}$ term in $\delta (e^{-i \omega})$.  As a result, we
%  have the following spectral formalization of the co-integrating
%  relation:
% \begin{equation}
% \label{eq:co-intRel}
%  \beta^{\prime} \, d\ZZ (\omega) = {(1 - e^{i \omega_{k}} e^{-i\omega}
%  )}^{q_{k}} \, d\ZZ^{(k)} (\omega),
% \end{equation}
%  where $d \ZZ^{(k)} (\omega)$ is the orthogonal increments measure
%  of another stationary invertible process.  This condition
%  (\ref{eq:co-intRel}) is readily satisfied by the latent dynamic
%  factor process discussed earlier, which is exemplary of the general
%  situation of interest.  The extreme case, where the co-integrating
%  vector lies in all the left null spaces of the component processes,
%  allows us to factor $\delta (e^{-i \omega})$ completely from
%  $\beta^{\prime} d \ZZ (\omega)$, though such a property need not
%  hold in practice.  
% 
% In order to see the full effect of condition
%  (\ref{eq:co-intRel}) on $\beta^{\prime} x_t$, we re-organize terms
%  in equation (\ref{eq:co-intRep}).  Let us suppose, without loss of
%  generality, that frequency $\omega_k$ has corresponding basis
%  functions $\phi_1, \cdots, \phi_{q_k}$, so that the first $q_k$
%  basis functions are annihilated by ${(1 - e^{i \omega_k}
%  B)}^{q_k}$.  Then we can write
% \begin{align*}
%  \beta^{\prime} x_t & = \sum_{j= q_k + 1}^d \phi_j (t) p^{(j)} (B) \, \beta^{\prime} \, x_{0}
%   + \int_{-\pi}^{\pi} \frac{ e^{i \omega t} - \sum_{j= q_k + 1}^d \phi_j (t) \, p^{(j)} ( e^{-i \omega
%  } )}{ \delta^{(k)} (e^{-i \omega}) } \, d \ZZ^{(k)}
%  (\omega) \\
%  & + \sum_{j=1}^{q_k} \phi_j (t) \,  \left(p^{(j)}
%  (B)  \beta^{\prime} \,  x_0 - \int_{-\pi}^{\pi} \frac{ p^{(j)} (e^{-i \omega}) }{
%  \delta^{(k)} (e^{-i \omega}) } \, d\ZZ^{(k)} (\omega) \right).
% \end{align*}
%  The first two terms are immediately recognized as the deterministic
%  and stochastic portions respectively of a non-stationary process
%  that has $\delta^{(k)} (B)$ for differencing operator.  The third
%  term is left over, and consists of deterministic time series that
%  are in the null space of ${(1 - e^{i \omega_k}
%  B)}^{q_k}$.  To see this, observe that for the third term the expression in parentheses is
% stochastic, but does not depend on time $t$, so that the resulting series is predictable.
% 
% 
%  It is true that $\delta^{(k)} (B)$
%  always divides $p^{(j)} (B)$, and hence the stochastic portion of
%  the third term is well-defined.  We cannot prove that the
%  coefficients of the $\phi_j (t)$ for $1 \leq j \leq q_k$ must be
%  zero, as counter-examples are easy to construct; consider two series that
%  have a common stochastic trend with null vector $\beta^{\prime} =
%  [1, \, 1]$, but whose underlying linear deterministic trends have
% different slopes.  
% %(Such a bivariate series might not be considered
% % co-integrated, because $\beta^{\prime} x_t$ equals a stationary
% % process plus a linear drift term.)
%   In our analysis henceforth, we
%  will assume that this third term is identically zero.
%  
% 
% 
%  This is the general treatment of co-integration.  Now consider the
%  filter error $\epsilon_t = y_t - \widehat{y}_t$.  Let $\Delta(z) = \Gamma
%  (z) - \widehat{\Gamma} (z)$, so that
% \[
%  \epsilon_t = \sum_{j=1}^d \Delta(B) \phi_j (t) p^{(j)} (B) \, x_{0} + \int_{-\pi}^{\pi}
%  \frac{ e^{i \omega t} \, \Delta (e^{-i \omega})
%  - \sum_{j=1}^d \Delta (B) \phi_j (t) \, p^{(j)} ( e^{-i \omega
%  } )}{ \delta (e^{-i \omega}) } \; d \ZZ (\omega),
% \]
%  where $\Delta (B)$ acts only upon the basis functions $\phi_j (t)$.
%   In order to write this expression, we really need the common
%   differencing operators assumption.  Note that $\Delta (B) $ is a
%   row vector of filters, and it gets multiplied by the initial value
%   vectors and the orthogonal increments process $d\ZZ(\omega)$.
%   Clearly the error process is stationary if all the basis functions
%   are annihilated by $\Delta (B)$, because in that case we must be
%   able to factor $\Delta (B) = \tau (B) \delta (B)$ (where $\tau (B)$ is
%   a $1 \times m$ multivariate filter) and $\epsilon_t
%   = \int_{-\pi}^{\pi} e^{i \lambda t } \, \tau (e^{-i \lambda}) \,
%   d\ZZ (\lambda)$.  This is the case of full filter constraints,
%   analogous to the stationary case considered above. 
% 
% We next consider some natural properties of target and
% concurrent filters.  Let us first factor $\delta (B) = \delta^S (B)
% \delta^N (B)$ according to signal and noise unit roots.  We will
% henceforth suppose that $\delta^S (B) = {(1 - e^{i \omega k}
% B)}^{q_k}$ for some unit root $\zeta_k = e^{-i \omega k}$ of
% multiplicity $q_k$.  Thus $\delta^N (B) = \delta^{(k)} (B)$.  Both
% $\Gamma (B)$ and $\widehat{\Gamma} (B)$ should preserve signal basis functions,
% which are those $\phi_j (t)$ corresponding to the unit root
% $\zeta_k$.  In order to preserve all these functions (i.e., act as
% the identity filter on them all) when multiplicity $q_k$ is present,
% we must have that
% \[
%  \frac{ \Gamma (z) - \Gamma (\zeta_k) }{ {( z- \zeta_k)}^{q_k} }
%  \qquad  \frac{ \widehat{\Gamma} (z) - \widehat{\Gamma} (\zeta_k) }{ {( z- \zeta_k)}^{q_k} }
% \]
%  are both bounded in $z$.  Equivalently, the differences $\Gamma (z) - \Gamma (\zeta_k)$ and 
% $  \widehat{\Gamma} (z) - \widehat{\Gamma} (\zeta_k)$ are each
%  divisible by $\delta^S (z)$.  We call this the {\it  signal preservation}
%  property of the filters.  For example, the signal extraction
%  filters described in McElroy and Trimbur (2012) always satisfy this
%  sort of condition.  In addition, because signal extraction filters
%  must eradicate all basis functions associated with noise
%  frequencies, it follows that $\delta^N (z)$ must be a factor of
%  $\Gamma (z) $ and $\widehat{\Gamma} (z)$.  We call this the {\it noise annihilation}
%  property of the filters.  Introduce the notation 
% \[
%  \Gamma^{N,k} (z) = \Gamma (\zeta_k)  \delta^N (z) / \delta^N
%   (\zeta_k)  \qquad \widehat{\Gamma}^{N,k} =  \widehat{\Gamma} (\zeta_k)  \delta^N (z) / \delta^N
%   (\zeta_k).
% \]
%  Then we can write
% \begin{equation}
% \label{eq:deltaErrdecomp}
%  \Delta (z)  =
%   \left( \frac{ \Gamma (z) - \Gamma^{N,k} (z) }{ \delta (z) } \right) \; \delta (z)
%   - \left( \frac{ \widehat{\Gamma} (z) - \widehat{\Gamma}^{N,k} (z) }{ \delta (z) } \right) \; \delta (z)  
%   + \left( \frac{ \Gamma (\zeta_k) - \widehat{\Gamma} (\zeta_k) }{ \delta^N
%   (\zeta_k) } \right) \; \delta^N (z).
% \end{equation}
%  We claim that the first two expressions involve a bounded rational function times
%  $\delta (z)$.  To see this true, observe that we only have to check
%  boundedness of $[ \Gamma (z) - \Gamma^{N,k} (z) ]/\delta (z)$ at $z$ values that are either roots of
%   $\delta^N (B)$ or $\delta^S (B)$ -- the same argument applies to
%   the second term involving $\widehat{\Gamma}$.  For a signal unit root, we
%   have $z = \zeta_k$, and boundedness follows from the signal
%   preservation property.  For a noise unit root, observe that we may
%   always factor $\delta^N (B)$ from $\Gamma (B)$ by the noise
%   annihilation property.  As for the third term of (\ref{eq:deltaErrdecomp}), it is
%   also always well-defined by the noise annihilation property.
% 
%   It is paramount that $\Delta (B)$ reduce the non-stationary
%   process to stationarity, and this can only happen in two ways:
%   first, a $\delta (B)$ can be factored out, which accomplishes the
%   requirement by differencing.  Second, the filter may have a linear
%   combination that is a co-integrating vector (associated with the
%   single signal frequency, which is important!), which together with
%   noise differencing accomplishes the requirement as well.  Note
%   that application of a co-integrating vector alone only removes
%   signal non-stationarity, and noise non-stationarity will remain.
%   The above decomposition for $\Delta (B)$ accomplishes this (under
%   the signal preservation and noise annihilation properties) if
%  \begin{equation}
%   \label{eq:co-intCond}
%  \frac{ \Gamma (\zeta_k) - \widehat{\Gamma} (\zeta_k) }{ \delta^N
%   (\zeta_k) } = \beta^{\prime}
%  \end{equation}
%   for some vector $\beta$ to be described.  If we impose that
%   $\beta$ is the zero vector, then we obtain the first case above,
%   where $\Delta (B)$ maintains stationarity by full differencing.
%   If instead we relax this to only imposing that $\beta = \beta_k$
%   be a co-integrating vector for the signal, then we obtain the
%   second case above.  Because there may be many choices for
%   $\beta_k$, depending on the co-integrating rank (the dimension of
%   the left null space of the factor loading matrix in the latent
%   dynamic factors formulation), this is a milder condition that may
%   allow for more flexibility in filter estimation.  Note that since
%   $\Gamma (\zeta_k) / \delta^N (\zeta_k)$ is a given quantity,
%   imposing (\ref{eq:co-intCond}) amounts to setting $\widehat{\Gamma}
%   (\zeta_k) / \delta^N (\zeta_k) = \beta^{\prime} + \Gamma (\zeta_k) /
%   \delta^N (\zeta_k)$ for a known co-integrating $\beta$.
% 
% We next develop the consequences of (\ref{eq:co-intCond}), where
%   $\beta$ is either zero or a signal co-integrating vector (the
%   second case will reduce to the first when we set $\beta = 0$ in
%   the following formulas).  Let $c_t = \delta^N (B) \beta^{\prime}
%   x_t$, which by our prior expression for $\beta^{\prime} x_t$ is
%   shown to be equal to $\int_{-\pi}^{\pi} e^{i \omega t} \; d
%   \ZZ^{(k)} (\omega)$.  The signal extraction error is
% \[
%  \epsilon_t  = \int_{-\pi}^{\pi} e^{i \omega t} \,
%   \left( \frac{ \Gamma (z) - \Gamma^{N,k} (z)   }{ \delta (z) } \right) \; d\ZZ (\omega)  - \int_{-\pi}^{\pi} e^{i \omega t} \,
%   \left( \frac{ \widehat{\Gamma} (z) - \widehat{\Gamma}^{N,k} (z) }{ \delta (z) } \right) \; d\ZZ (\omega) 
%  + \int_{-\pi}^{\pi} e^{i \omega t} \; d  \ZZ^{(k)} (\omega).
% \]
%  Its variance involves a spectral density matrix that combines
%  information from the differenced series $\partial s_t$ as well as
%  the noise-differenced co-integrated series $c_t$.  We now suppose
%  that the joint spectral density matrix of these series is available
%  to us, which is certainly possible in the case of latent dynamic
%  factor models.  Letting $h_c$, $h_{c \partial x}$, and $h_{\partial
%  x}$ denote the spectra and cross-spectra, we have the joint spectra
%  for ${[c_t, \partial x_t^{\prime}]}^{\prime}$ is
% \[
%  h(\omega) = \left[ \begin{array}{ll} h_c (\omega) & h_{\partial x
%  c} (\omega) \\ h_{c \partial x} (\omega) & h_{\partial x} (\omega)
%  \end{array} \right].
% \]
%   Then the signal extraction variance is ${(2\pi)}^{-1}$ times the
%   integral of
% \[
%  \left[ 1, - \left( \frac{ \Gamma (z) - \Gamma^{N,k} (z) }{ \delta (z) } \right) + 
%    \left( \frac{ \widehat{\Gamma} (z) - \widehat{\Gamma}^{N,k} (z) }{ \delta (z) } \right)  \right] \; h (\omega) \;
%   { \left[ 1, - \left( \frac{ \Gamma (\overline{z}) - \overline{\Gamma^{N,k}} (\overline{z})  }{ \delta (\overline{z}) } \right) +
%  \left( \frac{ \widehat{ \Gamma} (\overline{z}) -
%   \overline{\widehat{\Gamma}^{N,k} } (\overline{z}) }{ \delta (\overline{z}) } \right)
%   \right] }^{\prime}.
%  \]
%  Substituting the periodogram for $h$ at this point will allow
%  empirical estimation, where we impose the co-integrating relations
%  on $\widehat{\Gamma}$ and then optimize.
% 
%  In the case that $\beta$ is the zero vector, we are merely imposing full filter constraints by (\ref{eq:co-intCond}), and the multivariate DFA (M-DFA) condition
%   can be simplified.  Introduce the notation
% \[
%     \Gamma^{\delta,k} (z) =  \frac{ \Gamma (z) - \Gamma^{N,k} (z) }{ \delta (z) } \qquad
%   \widehat{\Gamma}^{\delta, k} (z) =  \frac{ \widehat{\Gamma} (z) - \widehat{\Gamma}^{N,k} (z) }{ \delta (z) }.
% \]
%   Although the former quantity can be computed from a knowledge of the target filter and the goals of analysis, the latter quantity is obliquely related to the parameters of the proposed concurrent filter.  In terms of these quantities, the M-DFA MSE is
% \[
%  \langle   \left[   \Gamma^{\delta, k} (z) -  \widehat{\Gamma}^{\delta, k} (z)  \right]   \; h_{\partial x} \;
%   { \left[  \Gamma^{\delta, k} (\overline{z}) -  \widehat{\Gamma}^{\delta, k} (\overline{z})    \right] }^{\prime} \rangle.
% \]
%  The filter conditions can be rephrased in terms of coefficient constraints, as in the stationary case.  


 
%  note: includes material from Cointegrated

%----------------------------------------

%\SweaveInput{Custom}
%  note: Custom replaces ATS and ATS_multivariate

%------------------------------------------

%\SweaveInput{Regularization}

%----------------------------------------

%\SweaveInput{Vintage}
%  note: Vintage replaces Filter_revisions

%----------------------------------------------



\chapter{Multivariate Direct Filter Analysis for Mixed Frequency Processes}
\label{chap:mix}

So far we have assumed that the explanatory series for a given target are observed
 at the same sampling frequency, but here we entertain the possibility that
 the target can be expressed as a multivariate filter acting on input series of
differeing sampling frequencies, e.g., a monthly and a quarterly series.
 Section \ref{sec:mix-svf} introduces the key insight, viz. the device of vector
  embedding, which allows us to jointly analyze high- and low-frequency time series.
  Then Section \ref{sec:mix-mdfa} shows how these ideas can be paired with the basic
   MDFA methodology.
 
\section{Sampling Frequency and Vector Embedding}
\label{sec:mix-svf}

\begin{Schunk}
\begin{Sinput}
> T <- 100
> N <- 2
> phi.matrix <- rbind(c(1,.5),c(-.2,.3))
> innovar.matrix <- diag(2)
> true.psidelta <- var1.par2psi(phi.matrix,100)
> gamma.0 <- matrix(solve(diag(4) - phi.matrix %x% phi.matrix) %*% 
+ 	matrix(innovar.matrix,ncol=1),nrow=2)
> x.init <- t(chol(gamma.0)) %*% rnorm(2)
> x.next <- x.init
> x.sim <- NULL
> for(t in 1:T)
+ {
+ 	x.next <- phi.matrix %*% x.next + rnorm(2)
+ 	x.sim <- cbind(x.sim,x.next)
+ }
> x.sim <- ts(t(x.sim),frequency=12)
> y.sim <- mdfa.embed(x.sim,4)
> #x.acf <- acf(x.sim,type="covariance",plot=FALSE,lag.max=T)[[1]]
> #x.acf <- aperm(aperm(x.acf,c(3,2,1)),c(2,1,3))
\end{Sinput}
\end{Schunk}



\begin{Schunk}
\begin{Sinput}
>  mu <- pi/6
>  len <- 175
>  lp.hifilter <- c(mu/pi,sin(seq(1,len)*mu)/(pi*seq(1,len)))
>  lp.hifilter <- c(rev(lp.hifilter),lp.hifilter[-1])
>  lp.lowfilter <- mdfa.embed(ts(as.matrix(lp.hifilter),frequency=12),4)
>  lp.lowfilter <- array(lp.lowfilter,c(39,3,3))
\end{Sinput}
\end{Schunk}

\section{Mixed Frequency Targets and Optimal Concurrent Filters}
\label{sec:mix-mdfa}



% 
% # construct and apply low pass filter
% x.trend.ideal <- filter(x.sim,lp.filter,method="convolution",sides=2)[(len+1):(T-len),]
% 
% # get MDFA concurrent filter
% q <- 20
% Grid <- T
% m <- floor(Grid/2)
% # The Fourier frequencies
% freq.ft <- 2*pi*Grid^{-1}*(seq(1,Grid) - (m+1))
% # frf for ideal low-pass
% frf.psi <- rep(0,Grid)
% frf.psi[abs(freq.ft) <= mu] <- 1
% frf.psi <- matrix(frf.psi,nrow=1) %x% diag(N) 	  
% frf.psi <- array(frf.psi,c(N,N,Grid))
% spec.hat <- mdfa.pergram(x.sim,1)	
% lp.mdfa <- mdfa.unconstrained(frf.psi,spec.hat,q)
%  
% # apply the MDFA concurrent filter
% x.trend.mdfa11 <- filter(x.sim[,1],lp.mdfa[[1]][1,1,],method="convolution",sides=1)
% x.trend.mdfa12 <- filter(x.sim[,2],lp.mdfa[[1]][1,2,],method="convolution",sides=1)
% x.trend.mdfa13 <- filter(x.sim[,3],lp.mdfa[[1]][1,3,],method="convolution",sides=1)
% x.trend.mdfa21 <- filter(x.sim[,1],lp.mdfa[[1]][2,1,],method="convolution",sides=1)
% x.trend.mdfa22 <- filter(x.sim[,2],lp.mdfa[[1]][2,2,],method="convolution",sides=1)
% x.trend.mdfa23 <- filter(x.sim[,3],lp.mdfa[[1]][2,3,],method="convolution",sides=1)
% x.trend.mdfa31 <- filter(x.sim[,1],lp.mdfa[[1]][3,1,],method="convolution",sides=1)
% x.trend.mdfa32 <- filter(x.sim[,2],lp.mdfa[[1]][3,2,],method="convolution",sides=1)
% x.trend.mdfa33 <- filter(x.sim[,3],lp.mdfa[[1]][3,3,],method="convolution",sides=1)
% x.trend.mdfa <- cbind(x.trend.mdfa11 + x.trend.mdfa12 + x.trend.mdfa13,
% 	x.trend.mdfa21 + x.trend.mdfa22 + x.trend.mdfa23,
% 	x.trend.mdfa31 + x.trend.mdfa32 + x.trend.mdfa33)
% x.trend.mdfa <- x.trend.mdfa[(len+1):(T-len),] 
% @
% 
% <<echo=False>>=
% # visualize
% file = paste("mdfa_trivar1_filtering.pdf", sep = "")
% pdf(file = paste(path.out,file,sep=""), paper = "special", 
%     width = 6, height = 6)
% par(oma=c(2,0,0,0),mar=c(2,4,2,2)+0.1,mfrow=c(3,1),cex.lab=.8)
% plot(ts(x.trend.ideal[,1]),ylab="",xlab="",yaxt="n",xaxt="n")
% axis(1,cex.axis=.5)
% axis(2,cex.axis=.5)
% lines(x.trend.mdfa[,1],col=2)
% plot(ts(x.trend.ideal[,2]),ylab="",xlab="",yaxt="n",xaxt="n")
% axis(1,cex.axis=.5)
% axis(2,cex.axis=.5)
% lines(x.trend.mdfa[,2],col=2)
% plot(ts(x.trend.ideal[,3]),ylab="",xlab="",yaxt="n",xaxt="n")
% axis(1,cex.axis=.5)
% axis(2,cex.axis=.5)
% lines(x.trend.mdfa[,3],col=2)
% mtext("Time", side = 1, line = 1,outer=TRUE)
% invisible(dev.off())
% @
% 
% <<echo=True>>=
% # compare in-sample performance
% print(c(mean((x.trend.ideal[,1] - x.trend.mdfa[,1])^2),
% 	mean((x.trend.ideal[,2] - x.trend.mdfa[,2])^2),
% 	mean((x.trend.ideal[,3] - x.trend.mdfa[,3])^2)))
% 
% # compare to criterion value
% diag(lp.mdfa[[2]])
% 
% # compute gain and phase delay functions
% frf.psi <- frf.psi[1,1,]
% gain.psi <- abs(frf.psi)
% phased.psi <- Arg(frf.psi)/freq.ft
% lp.frf <- mdfa.frf(lp.mdfa[[1]],0,T)
% lp.gain1 <- abs(lp.frf[1,1,])
% lp.gain2 <- abs(lp.frf[2,2,])
% lp.gain3 <- abs(lp.frf[3,3,])
% lp.phased1 <- -Arg(lp.frf[1,1,])/freq.ft
% lp.phased2 <- -Arg(lp.frf[2,2,])/freq.ft
% lp.phased3 <- -Arg(lp.frf[3,3,])/freq.ft
% @
% 
% <<echo=False>>=
% # visualize
% file = paste("mdfa_trivar1_freqdomain.pdf", sep = "")
% pdf(file = paste(path.out,file,sep=""), paper = "special", 
%     width = 6, height = 6)
% par(oma=c(2,0,0,0),mar=c(2,4,2,2)+0.1,mfrow=c(3,1),cex.lab=.8)
% plot(ts(gain.psi,start=-1,frequency=m),col=1,ylim=c(0,1),main="Gain",
% 	ylab="",xlab="",yaxt="n",xaxt="n")
% axis(1,cex.axis=.5)
% axis(2,cex.axis=.5)
% lines(ts(lp.gain1,start=-1,frequency=m),col="orange")
% lines(ts(lp.gain2,start=-1,frequency=m),col="green")
% lines(ts(lp.gain3,start=-1,frequency=m),col="violet")
% plot(ts(phased.psi,start=-1,frequency=m),col=1,
% 	ylim=c(0,max(na.exclude(lp.phased1),na.exclude(lp.phased2),
% 	na.exclude(lp.phased3))),main="Phase Delay",
% 	ylab="",xlab="",yaxt="n",xaxt="n")
% axis(1,cex.axis=.5)
% axis(2,cex.axis=.5)
% lines(ts(lp.phased1,start=-1,frequency=m),col="orange")
% lines(ts(lp.phased2,start=-1,frequency=m),col="green")
% lines(ts(lp.phased3,start=-1,frequency=m),col="violet")
% plot(ts(rep(NA,T),start=-1,frequency=m),col=1,
% 	ylim=c(0,max(Re(spec.hat[1,1,]),Re(spec.hat[2,2,]),Re(spec.hat[3,3,]))/6),
% 	main="Periodogram",ylab="",xlab="",yaxt="n",xaxt="n")
% axis(1,cex.axis=.5)
% axis(2,cex.axis=.5)
% lines(ts(Re(spec.hat[1,1,]),start=-1,frequency=m),col="orange")
% lines(ts(Re(spec.hat[2,2,]),start=-1,frequency=m),col="green")
% lines(ts(Re(spec.hat[3,3,]),start=-1,frequency=m),col="violet")
% mtext("Cycles", side = 1, line = 1,outer=TRUE)
% invisible(dev.off())
% @ 
% 
% <<exercise_mdfa_trivar1.filtering,echo=False>>=
% file = paste("mdfa_trivar1_filtering.pdf", sep = "")
% cat("\\begin{figure}[htb!]")
% cat("\\begin{center}")
% cat("\\includegraphics[]{", file, "}\n",sep = "")
% cat("\\caption{Ideal trends (black) for the trivariate VAR(1)
% 	with real-time MDFA trends (red) overlaid, for series one (upper panel),
% 	series two (center panel), and series three (bottom panel).", sep = "")
% cat("\\label{fig:trivar1.trends}}", sep = "")
% cat("\\end{center}")
% cat("\\end{figure}")
% @
% 
% <<exercise_mdfa_trivar1.filtering,echo=False>>=
% file = paste("mdfa_trivar1_freqdomain.pdf", sep = "")
% cat("\\begin{figure}[htb!]")
% cat("\\begin{center}")
% cat("\\includegraphics[]{", file, "}\n",sep = "")
% cat("\\caption{Gain functions (upper panel), 
% 	Phase Delay Functions (center panel), and Periodograms (bottom panel)
% 	 for series one (orange), two (green), and three (violet).", sep = "")
% cat("\\label{fig:trivar1.freqdomain}}", sep = "")
% cat("\\end{center}")
% cat("\\end{figure}")
% @


%--------------------------------------------------------------------





\chapter{Summary and Links}

\section{Survey of MDFA Optimization Criteria}

\section{Consistency and Efficiency: a Tale of Two Philosophies}

\subsection{Knowing the Truth: Omniscience}

\subsection{Believing in Truth: Faith and Fatalism}

\subsection{From Truth to Effectiveness: Emphasizing Performances}


\begin{thebibliography}{}

\bibitem{} Golub and Van Loan, 1996

\bibitem{}  Lutkepohl 2007

\bibitem{}  Brockwell and Davis 1991

\bibitem{} Baxter and King 1999

\bibitem{} Self and Liang 1987

\bibitem{} Hosoya and Taniguchi 1982

\bibitem{} Taniguchi and Kakizawa 2000

\bibitem{} McElroy and Trimbur 2015

\bibitem{} McElroy 2008

\bibitem{} Roy McElroy Linton 2019

\bibitem{} McElroy and Findley 2015

\bibitem{} Wildi 2008




\end{thebibliography}



\end{document}


OLD IDEAS

%-------------------------------------------

%\SweaveInput{Exotic_pub}

%----------------------------------------------

%\SweaveInput{Inference}

%------------------------------------------------------------------
 
%\SweaveInput{Adaptive}

%--------------------------------------------------------------------

%\SweaveInput{Seasonal_adjustment}
%--------------------------------------------------------------------

%\SweaveInput{Appendix}


