% -*- mode: noweb; noweb-default-code-mode: R-mode; -*-
\documentclass[a4paper]{article}

\title{Direct Filter Approach (DFA)}
\author{Marc Wildi}

\SweaveOpts{echo=FALSE}
\usepackage{a4wide}
\usepackage{amsmath}
\usepackage{float}
\usepackage[utf8]{inputenc}
\usepackage{hyperref}
\newtheorem{Proposition}{Proposition}

\begin{document}

\maketitle

%\SweaveOpts{prefix.string=c:/wia/tmp/bar}

<<label=init>>=
library(tseries)
library(fGarch)

# Set paths
path.main<-"C:\\wia_desktop\\Projekte\\2013\\Unterricht\\Eco2\\"
path.pgm<-paste(path.main,"R\\",sep="")
path.out<-paste(path.main,"Latex\\",sep="")
#path.out<-"C:\\wia_desktop\\Vorträge\\2013\\Meielisalp\\"
path.dat<-paste(path.main,"Data\\",sep="")

#-----------------------------------------------------------------
# Source estimation routine
#source(paste(path.pgm,"examples.r",sep=""))
@
\tableofcontents

\section{Introduction}

\subsection{Bird's Eye Perspective}

At the end of the previous course on SARIMA-models\footnote{\url{http://blog.zhaw.ch/idp/sefblog/index.php?/archives/352-Introduction-to-SARIMA-Models-Script-and-R-Code.html}} I stated

\begin{quote}
SARIMA-models are an approved long-term state-of-the-art approach to forecasting.
However, they assume fixed coefficients and iid innovations. Also,
estimation of unknown coefficients relies on short-term (one-step ahead mean-square) forecast performances which are not always
suitable when inferring future `trends' or turning-points. Finally, univariate approaches fail to capture
potentially interesting comovements across time series.
In the upcoming course we discuss more general
univariate and multivariate real-time signal-extraction methods (DFA/MDFA), conditional heteroscedasticity models (ARCH/GARCH/EGARCH/TGARCH) and
adaptive approaches ((M)DFA/State-Space models\footnote{\url{http://blog.zhaw.ch/idp/sefblog/index.php?/archives/353-Introduction-to-State-Space-Models-Script-and-R-Code-with-some-effective-Tweaks.html}}) which address  some of the potential flaws of the SARIMA-methodology.
\end{quote}

As claimed I here propose and analyze more general forecasting and nowcasting procedures which emphasize mid- and
long-term dynamics of a time series (cycles, trends). Besides ordinary mean-square
performances I'll address the detection of `turning-points' by controlling explicitly Timeliness and Smoothness
characteristics of real-time filters based on a new, original and general optimization paradigm. In contrast to the
somewhat rigid mean-square orthodoxy, I provide additional flexibility (and assign responsibility) to the user by
proposing a comprehensive interface which allows to implement specific research priorities: the ordinary mean-square
solution is but one possible outcome of the proposed \emph{customized} optimization criteria. I also replicate (up to arbitrary precision)
classical approaches -- model-based, HP, CF, Henderson,... -- and propose customizations thereof: proponents of a
particular approach
can replicate their `baby' by DFA and enhance true out-of-sample performances, staying firmly into their preferred paradigm.
DFA assimilates whatever is fed to her (she's a lady) and refines the meal, if desired. \\

This book breaks the rules by going beyond the frontiers of
classic dichotomic trade-offs: it proposes solutions which outperform theoretically best mean-square (model-based) filters
-- assuming knowledge of the true data-generating process (DGP) -- in terms of Timeliness and Smoothness, Mean-Shift and Selectivity,
Peak-Correlation and Curvature; \emph{simultaneously}; out-of-sample. In other words: I make full (ab)use of the fundamental
ATS-trilemma (to be defined...). On a lighter note,
our results on (real-time) seasonal-adjustment (SA) bear a sensible touch of controversy, as well. \\ %We also analyze methods which can handle (conditional) heteroscedasticity as typically found in finance data.



I'm proposing an atheoretical empirical introduction to a sophisticated `hot' forecasting problem, whose complexity
is systematically underrated in the time-series literature. Since the true `effective' structure of the problem cannot be
obtained in the time-domain (ATS-trilemma), I'll have to introduce frequency-domain concepts. No theory! Just simple
straightforward examples based on unsophisticated -- primitive and inelegant but graspable -- R-code (man do I hate
these cryptic lines of monosyllabic R-code!). You don't have to be a time series expert: everything
is self-contained and intuitive. Each graph, each table, each result is obtained solely by the
code contained in this book. You do not have to download tons of series, not a single one in fact: for convenience,
everything is based on simulated data as generated by R\footnote{Don't blame me on using artificial data. Not me!}.
The book is generated in the Sweave-environment: it is completely reproducible at any location and at any time-point by
any R-user. \\

\subsection{Potentially Interesting Links}

\begin{itemize}
\item I maintain a comprehensive Blog on the topic where you can find applications to financial trading and real-time
macro-indicators: \url{http://blog.zhaw.ch/idp/sefblog/}. There, you will find tons of material on MDFA
(a multivariate extension of the DFA).
\item Chris maintains a nice Blog on trading applications of MDFA, see
\url{http://imetricablog.com/}.
\item Real-time macro-applications can be seen on \url{http://www.idp.zhaw.ch/usri} as well as on
\url{http://www.idp.zhaw.ch/euri}.
\item For those in need of theoretical background and further applications/research reports I may refer to
\begin{itemize}
\item My `good ole' DFA-book \url{http://blog.zhaw.ch/idp/sefblog/index.php?/archives/168-RTSE-My-Good-Old-Book.html}. It dates
back to 2008: at that time I didn't have a computationally fast algorithm and numerical processing was a nightmare. It contains
a lot more information on non-stationary DGP's, see chapter 6.
\item The `trilemma paper' on DFA, co-authored with Tucker
McElroy \url{http://blog.zhaw.ch/idp/sefblog/index.php?/archives/271-Trilemma-Paper-Latest-Sweaved-Version-Available.html#extended}.
A very clean and sober presentation (thanks to Tucker) leading systematically from one-step ahead forecasting to full
customization.
\item The only officially recognized reference for the multivariate DFA (MDFA):
my `elements vade-mecum' \url{http://blog.zhaw.ch/idp/sefblog/index.php?/archives/259-Elements-of-Forecasting-and-Signal-Extraction.html}
(go down the
chronological list and download the latest version of the working vade-mecum).
\item A report on a common research with the OECD (application of MDFA): \url{http://blog.zhaw.ch/idp/sefblog/index.php?/archives/340-I-MDFA-and-OECD-CLIs.html}.
\item An interresting independent research from Ginters (application of MDFA): \url{http://blog.zhaw.ch/idp/sefblog/index.php?/archives/341-Another-Article-on-I-MDFA-FORECASTING-AND-SIGNAL-EXTRACTION-WITH-REGULARISED-MULTIVARIATE-DIRECT-FILTER-APPROACH.html}.
\item And, finally, let me conclude on an infinite regress of this book on itself (and R-code as `Rtangled' from itself too):
\url{http://blog.zhaw.ch/idp/sefblog/index.php?/archives/359-DFA-the-Standard-is-Released.html}.
\end{itemize}
\end{itemize}
Let me now adopt the majestic `We' when speaking of
Ourself.\\

\subsection{Disclaimer}

This oeuvre summarizes recent material and tutorials published in a loose unsystematic manner on SEFBlog. It is a comprehensive
unifying tutorial on DFA. The first version was released on May 1 2013 (worker's day is a pure coincidence: We could have released her
on Christmas as well): although the Sweave-environment helps in sorting out `dirt', this original version is certainly
contaminated by an unknown number of more or less minor bugs. We will comment modifications in later revisions.
We are happy to collect feedback and perfect the subject-matter. Obviously,
We are not English Grammarian (should We use the plural here?).
And We did not apply automatic spell-facilities. Absorb the poison `as is'.





\section{Frequency Domain}


Frequency-domain and Time-domain offer general frameworks (spaces) to work on time series data. Whereas
both perspectives are equally general, the frequency-domain offers some crucial structural
advantages which allow for a straightforward and practically
relevant generalization of the mean-square (one-step ahead) orthodoxy.


\subsection{Orthonormal Basis: Decomposition of Periodic Functions into Sines and Cosines}

\subsubsection{The Generic Ideal Low-Pass Filter} \label{lowpass}

Any `well-behaved' periodic function $\Gamma(\omega)$, with periodicity $2\pi$ i.e. $\Gamma(\omega)=
\Gamma(\omega\pm k2\pi)$ for any integer $k$, can be decomposed into a sum of sines and cosines. As
a generic example We here consider
\begin{eqnarray*}
\Gamma(\omega,\textrm{cutoff})=\left\{\begin{array}{cc}1& \omega\in 2k\pi+[-\textrm{cutoff},\textrm{cutoff}]\\0&\textrm{otherwise} \end{array} \right.
\end{eqnarray*}
where $0<\textrm{cutoff}<\pi$, see fig.\ref{z_Gamma_t}. If not explicitly needed, We can suppress the $\textrm{cutoff}$-term
in the notation and write $\Gamma(\omega)$ instead to signify that $\Gamma(\cdot)$ is a one-parametric function in the \emph{variable}
$\omega$, whereby the shape of the function is specified by the \emph{fixed} parameter $\textrm{cutoff}$.
<<echo=True>>=
cutoff<-pi/6
K<-1200
Gamma_h<-((0:K)*pi/K)<cutoff
Gamma<-rep(c(Gamma_h[length(Gamma_h):2],Gamma_h),3)
file = paste("z_Gamma_t.pdf", sep = "")
pdf(file = paste(path.out,file,sep=""), paper = "special", width = 6, height = 6)
par(mfrow=c(2,1))
plot(Gamma,col="blue",main="Periodic Gamma",xlab="",ylab="",type="l",axes="F")
axis(1,at=c(c(1,0,0,0)+(length(Gamma)/3)*(0:(length(Gamma)/(length(Gamma)/3))),
length(Gamma)/2),labels=c("-3pi","-pi","pi","3pi","0"))
axis(2)
box()
plot(Gamma_h,col="blue",main="Gamma in [0,pi]",xlab="",ylab="",type="l",axes="F")
axis(1,at=c(1,(K/6)*1:6),labels=c("0","pi/6","2pi/6","3pi/6","4pi/6","5pi/6","pi"))
axis(2)
box()
dev.off()
@
<<label=z_Gamma_t.pdf,echo=FALSE,results=tex>>=
  file = paste("z_Gamma_t.pdf", sep = "")
  cat("\\begin{figure}[H]")
  cat("\\begin{center}")
  cat("\\includegraphics[height=6in, width=4in]{", file, "}\n",sep = "")
  cat("\\caption{Ideal lowpass with cutoff pi/6", sep = "")
  cat("\\label{z_Gamma_t}}", sep = "")
  cat("\\end{center}")
  cat("\\end{figure}")
@
Note that We may restrict the periodic $\Gamma(\cdot)$ to the range $[-\pi,\pi]$ because of
periodicity (all other values are redundant). Moreover, since it is an even function
($\Gamma(\omega)=\Gamma(-\omega)$) We just need to plot the function for $\omega\in [0,\pi]$, as
done in the bottom graph of fig.\ref{z_Gamma_t}. $\Gamma(\cdot)$ will play a crucial role as an ideal lowpass \emph{target}
which is used to generalize traditional forecast criteria, see later sections. The frequency band $|\omega|<\textrm{cutoff}$
is called \emph{passband}: $\Gamma(\omega)=1$ in the passband; the frequency band $|\omega|>\textrm{cutoff}$
is called \emph{stopband}: $\Gamma(\omega)=0$ in the stopband. \\

Suppose that $\Gamma(\omega)$ can be decomposed into a weighted sum of cosines and sines
\begin{eqnarray*}
\Gamma(\omega)&=&\sum_{k=-\infty}^\infty \gamma_k\exp(-ik\omega)
\end{eqnarray*}
where $\exp(-i\omega)=\cos(-\omega)+i\sin(-\omega)$ is the complex exponential function on the unit-circle
$|\exp(-i\omega)|=1$ and where $\gamma_k$ are called \emph{Fourier coefficients}.
We would be interested in finding the Fourier coefficients $\gamma_k$ associated
to this hypothetical decomposition. Consider
\begin{eqnarray}
\frac{1}{2\pi}\int_{-\pi}^{\pi} \Gamma(\omega)\exp(ij\omega)d\omega&=&\frac{1}{2\pi}\int_{-\pi}^{\pi} \sum_{k=-\infty}^\infty \gamma_k\exp(-ik\omega)\exp(ij\omega)d\omega\label{ift}\\
&=&\sum_{k=-\infty}^\infty \gamma_k \frac{1}{2\pi}\int_{-\pi}^{\pi} \exp(-ik\omega)\exp(ij\omega)d\omega\nonumber\\
&=&\gamma_j\nonumber
\end{eqnarray}
where the last equality follows from the fact that the integral vanishes if
$\exp(-ik\omega)\exp(ij\omega)\not=1$, by periodicity of the complex
exponential\footnote{Exchanging integral and the infinite sum is not trivial but
We could approximate $\Gamma(\cdot)$ arbitrarily well by a finite sum which would allow
for the swap.}. The transformation \ref{ift} mapping $\Gamma(\omega)$ to its Fourier coefficients $\gamma_j$, $j=-\infty,...,0,,\infty$ is
called \emph{inverse Fourier transformation}.\\

We have thus found a way to compute the Fourier coefficients $\gamma_k$. Specifically, in the case
of the above lowpass We find
\begin{eqnarray}
\gamma_j&=&\frac{1}{2\pi}\int_{-\pi}^{\pi} \Gamma(\omega)\exp(ij\omega)d\omega\nonumber\\
&=&\frac{1}{2\pi}\int_{-\textrm{cutoff}}^{\textrm{cutoff}} \exp(ij\omega)d\omega\nonumber\\
&=&\frac{1}{2\pi}\left.\frac{\exp(ij\omega)}{ij}\right|_{-\textrm{cutoff}}^{\textrm{cutoff}}\nonumber\\
&=&\left\{\begin{array}{cc}\displaystyle{\frac{\sin(j\cdot\textrm{cutoff})}{j\pi}}&j\not= 0\\
\displaystyle{\frac{\textrm{cutoff}}{\pi}}&j=0\end{array}\right.  \label{gamma_kkk}
\end{eqnarray}
It follows that
\begin{eqnarray}
\Gamma(\omega)&=&\sum_{k=-\infty}^\infty \gamma_k\exp(-ik\omega) \nonumber\\
&=&\frac{1}{\pi}\sum_{k=-\infty}^\infty\frac{\sin(k\cdot\textrm{cutoff})}{k} \exp(-ik\omega)\nonumber\\
&=&\frac{\textrm{cutoff}}{\pi}+\frac{2}{\pi}\sum_{k=1}^\infty\frac{\sin(k\cdot\textrm{cutoff})}{k} \cos(k\omega)\label{lwhfsw}
\end{eqnarray}
Now consider what we just did: we replicated a discontinuous step-function by a sum of
infinitely differentiable trigonometric functions. Weird! But... what happens in the discontinuity?


\subsubsection{Exercises}\label{ilpe}

\begin{enumerate}
\item Compute the first 1000 (thousand) Fourier coefficients of the lowpass function with
cutoff$=\pi/6$. Make a plot, see fig.\ref{z_gamma}.
<<echo=True>>=
cutoff<-pi/6
# Order of approximation : 10, 100, 1000, 10000
ord<-K
# Compute coefficients gamma
gamma<-c(cutoff/pi,(1/pi)*sin(cutoff*1:ord)/(1:ord))
sum(gamma)+sum(gamma[2:ord])
# Plot
file = paste("z_gamma.pdf", sep = "")
pdf(file = paste(path.out,file,sep=""), paper = "special",
width = 6, height = 6)
par(mfrow=c(2,1))
plot(c(gamma[min(1000,ord):2],gamma[1:min(1000,ord)]),col="black",
main="Fourier coefficients of Gamma: -1000:1000",xlab="",ylab="",
type="l",axes="F")
axis(1,at=c(0:4)*min(1000,ord)/2,labels=c(-min(1000,ord),-min(1000,ord)/2,0,min(1000,ord)/2,min(1000,ord)))
axis(2)
box()
plot(c(gamma[min(1000,ord/10):2],gamma[1:min(1000,ord/10)]),col="black",
main="Fourier coefficients of Gamma: -100:100",xlab="",ylab="",
type="l",axes="F")
axis(1,at=c(0:4)*ord/20,labels=c(-ord/10,-ord/20,0,ord/20,ord/10))
axis(2)
box()
dev.off()
@
<<label=z_gamma.pdf,echo=FALSE,results=tex>>=
  file = paste("z_gamma.pdf", sep = "")
  cat("\\begin{figure}[H]")
  cat("\\begin{center}")
  cat("\\includegraphics[height=6in, width=4in]{", file, "}\n",sep = "")
  cat("\\caption{Fourier coefficients of Gamma: from -1000 to 1000 (top) and -100 to 100 (bottom)", sep = "")
  cat("\\label{z_gamma}}", sep = "")
  cat("\\end{center}")
  cat("\\end{figure}")
@
We can recognize the damped sinusoidal with frequency cutoff$=\pi/6$. Note, however, that the Fourier coefficients decay
slowly -- at a \emph{linear} rate -- which is due to the fact that Our ideal lowpass function is discontinuous: rapid
(instantaneous) changes require high-frequencies $\cos(k\omega)$, $k$ large, in \ref{lwhfsw} to be weighted strongly,
thus Fourier-coefficients $\gamma_k$ cannot decay too rapidly.
\item Compute the finite approximation
\[\hat{\Gamma}_m(\omega):=\frac{\textrm{cutoff}}{\pi}+\frac{2}{\pi}\sum_{k=1}^m\frac{\sin(k\cdot\textrm{cutoff})}{k} \cos(k\omega)
\]
of $\Gamma(\omega)$ of orders
$m=10,100,1000,10000$ and check graphically that the finite sums converge to the discontinuous
$\Gamma(\omega)$, see fig.\ref{z_Gamma_a}. Hint: compute $\hat{\Gamma}_m(\omega_k)$ on an equidistant discrete frequency grid
$\omega_k=\displaystyle{\frac{k\pi}{N}}$, $k=0,...,N$, where $N$ is a `large' multiple of 6 ($N=1200$ in the following code).
Note that $\omega_{N/6}=\pi/6$ such that We can track the singularity in $\omega=\pi/6$.
<<echo=True>>=
cutoff<-pi/6
# Order of approximation : 10, 100, 1000, 10000
ord<-K
# Compute coefficients gamma
gamma<-c(cutoff/pi,(1/pi)*sin(cutoff*1:ord)/(1:ord))
# Compute finite sum
# len1: Number of frequency ordinates (resolution of discrete grid in [-pi,pi])
len1<-K
Gamma_hat<-0:len1
for (k in 0:len1)#k<-0
{
	omegak<-k*pi/len1
	Gamma_hat[k+1]<-gamma%*%(cos(omegak*0:ord)*c(1,rep(2,ord)))
}
file = paste("z_Gamma_a.pdf", sep = "")
pdf(file = paste(path.out,file,sep=""), paper = "special",
width = 6, height = 6)
plot(Gamma_hat,type="l",axes=F,
main="Gamma (blue) and finite approximation (black)",xlab="",ylab="")
lines(Gamma_h,col="blue")
axis(1, at=c(0,1+1:6*len1/6),labels=c("0","pi/6","2pi/6","3pi/6",
"4pi/6","5pi/6","pi"))
axis(2)
box()
dev.off()
@
<<label=z_Gamma_a.pdf,echo=FALSE,results=tex>>=
  file = paste("z_Gamma_a.pdf", sep = "")
  cat("\\begin{figure}[H]")
  cat("\\begin{center}")
  cat("\\includegraphics[height=4in, width=4in]{", file, "}\n",sep = "")
  cat("\\caption{Finite approximation m=1000 (black) vs. Gamma (blue)", sep = "")
  cat("\\label{z_Gamma_a}}", sep = "")
  cat("\\end{center}")
  cat("\\end{figure}")
@
For $m=1000$ the approximation is pretty good. Visible approximation errors are concentrated in the vicinity of the
singularity $\omega=\pi/6$ (so-called Gibbs-phenomenon).
\item What happens in the discontinuity $\omega=\pi/6$? Hint: the finite sums are infinitely
differentiable and therefore We must have a continuous transition from the passband ($\hat{\Gamma}(\omega)\approx 1$) to
the stopband ($\hat{\Gamma}(\omega)\approx 0$).
<<echo=True>>=
Gamma_hat[1+round(len1/6)]
@
We can see that $\hat{\Gamma}_m(\pi/6)\approx 0.5$, the mean value or midposition between 1 (passband on the left) and 0
(stopband on the right).
\item Generate a white noise sequence $\epsilon_t$ of length 300  and apply the Fourier coefficients $\gamma_k=\frac{\sin(k\cdot\textrm{cutoff})}{k}$,
$k=-100,...,0,...,100$, of the lowpass to the white noise sequence
\[y_t=\sum_{k=-100}^{100}\gamma_k\epsilon_{t-k}\]
Note that $y_t$ can be computed for $t=100,...,200$ only. Compare $\epsilon_t$ and $y_t$ graphically.
<<echo=True>>=
len<-300
set.seed(10)
eps<-rnorm(len)
z<-cumsum(eps)
y<-rep(NA,len)
yc<-y
for (k in 100:200)#k<-0
{
	y[k]<-gamma[1:100]%*%eps[k+(0:99)]+gamma[2:100]%*%eps[k-(1:99)]
	yc[k]<-gamma[1:100]%*%z[k+(0:99)]+gamma[2:100]%*%z[k-(1:99)]
}
file = paste("z_Gamma_n.pdf", sep = "")
pdf(file = paste(path.out,file,sep=""), paper = "special",
width = 6, height = 6)
par(mfrow=c(2,1))
ts.plot(as.ts(eps),type="l",axes=F,
main="Noise (blue) and lowpassed noise(red)",xlab="",
ylab="",col="blue")
lines(y,col="red",lwd=2)
#axis(1, at=as.integer(c(0,1+1:6*len/6)))#,labels=as.integer(c(0,1+1:6*len/6)))
axis(2)
box()
ts.plot(as.ts(z),type="l",axes=F,
main="Random-walk (blue) and lowpassed random-walk(red)",xlab="",
ylab="",col="blue")
lines(yc,col="red",lwd=2)
#axis(1, at=as.integer(c(0,1+1:6*len/6)))#,labels=as.integer(c(0,1+1:6*len/6)))
axis(2)
box()
dev.off()
@
<<label=z_Gamma_n.pdf,echo=FALSE,results=tex>>=
  file = paste("z_Gamma_n.pdf", sep = "")
  cat("\\begin{figure}[H]")
  cat("\\begin{center}")
  cat("\\includegraphics[height=4in, width=4in]{", file, "}\n",sep = "")
  cat("\\caption{Noise (blue) and lowpassed noise (red)", sep = "")
  cat("\\label{z_Gamma_n}}", sep = "")
  cat("\\end{center}")
  cat("\\end{figure}")
@
We observe that $y_t$ (red line) tracks $\epsilon_t$ but many of the short-term random movements of the noise are eliminated or damped: $y_t$ is
much smoother than the original $\epsilon_t$.
\end{enumerate}



\subsubsection{Building General Real-Valued Functions}

We here briefly suggest that one can approximate general `well-behaved' (periodic) functions arbitrarily well by relying
on suitable linear combinations of the above ideal lowpass function. To see this We first define a so-called ideal passband function
\begin{eqnarray*}
\Gamma_{pb}(\omega,\textrm{cutoff}_1,\textrm{cutoff}_2)=I_{\{\textrm{cutoff}_1\leq |\omega|\leq \textrm{cutoff}_2\}}
\end{eqnarray*}
where $\textrm{cutoff}_1<\textrm{cutoff}_2$ and where $I_{\{\textrm{cutoff}_1\leq |\omega|\leq \textrm{cutoff}_2\}}\displaystyle{\left\{\begin{array}{cc}1 &\textrm{cutoff}_1\leq |\omega|\leq \textrm{cutoff}_2\\
0& \textrm{otherwise}\end{array}\right.}$ is the indicator function.
The bandpass function can be obtained as a simple linear combination of two lowpass functions:
\[\Gamma_{pb}(\omega,\textrm{cutoff}_1,\textrm{cutoff}_2)=\Gamma(\omega,\textrm{cutoff}_2)-\Gamma(\omega,\textrm{cutoff}_1)\]
As for the lowpass function above, We frequently suppress the
\emph{fixed} parameters $\textrm{cutoff}_1,\textrm{cutoff}_2$ in Our notation, if not explicitly required, and write
$\Gamma_{pb}(\omega)$ instead, to signify that the bandpass
is a function of the \emph{variable} $\omega$. A bandpass with $\textrm{cutoff}_1=\frac{\pi}{6}, \textrm{cutoff}_2=\frac{2\pi}{6}$ is
plotted in fig.\ref{z_Gamma_pb}.
<<echo=True>>=
cutoff_1<-pi/6
cutoff_2<-2*pi/6
K<-1200
Gamma_pb<-((0:K)*pi/K)>cutoff_1&((0:K)*pi/K)<cutoff_2
file = paste("z_Gamma_pb.pdf", sep = "")
pdf(file = paste(path.out,file,sep=""), paper = "special", width = 6, height = 6)
plot(Gamma_pb,col="blue",main="Gamma passband in [0,pi]",xlab="",
ylab="",type="l",axes="F")
axis(1,at=c(1,(K/6)*1:6),labels=c("0","pi/6","2pi/6","3pi/6",
"4pi/6","5pi/6","pi"))
axis(2)
box()
dev.off()
@
<<label=z_Gamma_pb.pdf,echo=FALSE,results=tex>>=
  file = paste("z_Gamma_pb.pdf", sep = "")
  cat("\\begin{figure}[H]")
  cat("\\begin{center}")
  cat("\\includegraphics[height=4in, width=4in]{", file, "}\n",sep = "")
  cat("\\caption{Ideal passband with cutoffs pi/60 and pi/6", sep = "")
  cat("\\label{z_Gamma_pb}}", sep = "")
  cat("\\end{center}")
  cat("\\end{figure}")
@
The Fourier coefficients of $\Gamma_{pb}(\omega,\textrm{cutoff}_1,\textrm{cutoff}_2)$ can be obtained directly, by inverse
Fourier transformation \ref{ift}, or by
\begin{equation}\label{fc_bp_a}
\gamma_{j,pb}(\textrm{cutoff}_1,\textrm{cutoff}_2)=\gamma_{j}(\textrm{cutoff}_2)-\gamma_{j}(\textrm{cutoff}_1)
\end{equation}
where $\gamma_{j}(\textrm{cutoff}_1)$ and $\gamma_{j}(\textrm{cutoff}_2)$ are the Fourier coefficients of the corresponding lowpass
functions\footnote{The inverse Fourier transformation is a linear functional.}. \\


By scaling and adding bandpasses We are in a position to approximate any well-behaved (periodic) function arbitrarily well, see
fig.\ref{z_cos_pb}.
<<echo=True>>=
len1<-1200
f<-cos(2*pi*(1:len1)/len1)
file = paste("z_cos_pb.pdf", sep = "")
pdf(file = paste(path.out,file,sep=""), paper = "special", width = 6, height = 6)
par(mfrow=c(3,1))
resolution<-10
plot(f,col="black",
main=paste("Approximation of Cosine by bp-functions: resolution=",resolution,sep=""),
xlab="",ylab="",type="l",axes="F")
for (i in 0:resolution)
  lines(cos(2*pi*(i+0.5)/resolution)*((1:len1)>(i)*len1/resolution&(1:len1)<
  (i+1)*len1/resolution),col="blue")
resolution<-33
axis(1,at=c(1,(K/6)*1:6),labels=c("0","pi/6","2pi/6","3pi/6","4pi/6","5pi/6","pi"))
axis(2)
box()
resolution<-33
plot(f,col="black",
main=paste("Approximation of Cosine by bp-functions: resolution=",resolution,sep=""),
xlab="",ylab="",type="l",axes="F")
for (i in 0:resolution)
  lines(cos(2*pi*(i+0.5)/resolution)*((1:len1)>(i)*len1/resolution&(1:len1)<
  (i+1)*len1/resolution),col="red")
axis(1,at=c(1,(K/6)*1:6),labels=c("0","pi/6","2pi/6","3pi/6","4pi/6","5pi/6","pi"))
axis(2)
box()
resolution<-100
plot(f,col="black",
main=paste("Approximation of Cosine by bp-functions: resolution=",resolution,sep=""),
xlab="",ylab="",type="l",axes="F")
for (i in 0:resolution)
  lines(cos(2*pi*(i+0.5)/resolution)*((1:len1)>(i)*len1/resolution&(1:len1)<
  (i+1)*len1/resolution),col="green")
axis(1,at=c(1,(K/6)*1:6),labels=c("0","pi/6","2pi/6","3pi/6","4pi/6","5pi/6","pi"))
axis(2)
box()
dev.off()
@
<<label=z_cos_pb.pdf,echo=FALSE,results=tex>>=
  file = paste("z_cos_pb.pdf", sep = "")
  cat("\\begin{figure}[H]")
  cat("\\begin{center}")
  cat("\\includegraphics[height=6in, width=4in]{", file, "}\n",sep = "")
  cat("\\caption{Approximation of cosine by bandpass functions", sep = "")
  cat("\\label{z_cos_pb}}", sep = "")
  cat("\\end{center}")
  cat("\\end{figure}")
@
Since any passband function can be decomposed into cosines/sines We infer that any `well behaved' periodic function $f(\omega)$ can be decomposed
too
\begin{equation}\label{ftfg}
f(\omega)=\sum_{k=-\infty}^\infty f_k \exp(-ik\omega)
\end{equation}
by linear combination of the decomposed passband functions. The Fourier coefficients $f_k$ can be obtained either directly,
by inverse Fourier transformation \ref{ift}, or by
linear combination of the Fourier coefficients corresponding to the bandpass functions in the discrete approximation
of $f(\omega)$ (and going to the limit of the resolution). If $f(\omega)$ is a real-valued function, then
$f_k=f_{-k}$ and the imaginary parts cancel
on both sides of \ref{ftfg} thus leaving
\[f(\omega)=f_0+2\sum_{k=1}^\infty f_k \cos(k\omega)\]

\subsubsection{Exercises}

\begin{enumerate}
\item Verify that the Fourier coefficient defined in \ref{fc_bp_a} replicate the corresponding bandpass function. Specifically,
approximate $\Gamma_{pb}(\omega,\textrm{cutoff}_1,\textrm{cutoff}_2)$ by finite Fourier approximations of order 100 and 1000 and
check the quality of the approximation, see fig.\ref{z_Gammapb_a}.
<<echo=True>>=
# Compute coefficients gamma of the two lowpass filters
gamma_1<-c(cutoff_1/pi,(1/pi)*sin(cutoff_1*1:ord)/(1:ord))
gamma_2<-c(cutoff_2/pi,(1/pi)*sin(cutoff_2*1:ord)/(1:ord))
# Compute finite sum (approximation)
Gamma_hat_pb<-0:len1
for (k in 0:len1)#k<-0
{
	omegak<-k*pi/len1
	Gamma_hat_pb[k+1]<-(gamma_2-gamma_1)%*%
  (cos(omegak*0:ord)*c(1,rep(2,ord)))
}
file = paste("z_Gammapb_a.pdf", sep = "")
pdf(file = paste(path.out,file,sep=""), paper = "special",
width = 6, height = 6)
plot(Gamma_hat_pb,type="l",axes=F,
main="Passband Gamma (blue) and finite approximation (black)",xlab="",
ylab="")
lines(Gamma_pb,col="blue")
axis(1, at=c(0,1+1:6*len1/6),labels=c("0","pi/6","2pi/6","3pi/6",
"4pi/6","5pi/6","pi"))
axis(2)
box()
dev.off()
@
<<label=z_Gammapb_a.pdf,echo=FALSE,results=tex>>=
  file = paste("z_Gammapb_a.pdf", sep = "")
  cat("\\begin{figure}[H]")
  cat("\\begin{center}")
  cat("\\includegraphics[height=4in, width=4in]{", file, "}\n",sep = "")
  cat("\\caption{Finite approximation m=1000 (black) vs. passband Gamma (blue)", sep = "")
  cat("\\label{z_Gammapb_a}}", sep = "")
  cat("\\end{center}")
  cat("\\end{figure}")
@
\end{enumerate}
We observe once again the Gibbs phenomenon at the edges of the two discontinuities $\pi/6$ and $2\pi/6$ but otherwise
the fit is good. Selecting larger approaximation errors would result in arbitrary good approximations.


\subsection{Discrete Fourier Transformation (DFT) and Inverse Discrete Transformation (IDFT)} \label{dft_idft_dft}


A time series $x_t$, $t=1,...,T$ can be transformed into the frequency domain by the so-called DFT:
\begin{eqnarray}\label{dft}
\Xi_{TX}(\omega):=\frac{1}{\sqrt{2\pi T}}\sum_{t=1}^Tx_t\exp(-it\omega)
\end{eqnarray}
The DFT $\Xi_{TX}(\omega)$ of $x_t$ is a periodic function and it is generally specified on the discrete grid
$\omega_k=\displaystyle{\frac{k2\pi}{T}}$, where $k=-T/2,...,0,...,T/2$ for even $T$. For odd $T$ one uses $T'=T-1$ in these
expressions instead of $T$.  \\

The DFT is equivalent to the data $x_t$, in informational terms, and in fact the data can be recovered from the DFT by applying the inverse transformation
\begin{eqnarray}\label{idft}
x_t&=&\frac{\sqrt{2\pi}}{\sqrt{ T}}\sum_{k=-[T/2]}^{[T/2]} w_k\Xi_{TX}(\omega_k)\exp(it\omega_k )\\
&=&\frac{\sqrt{2\pi}}{\sqrt{ T}}\left(\Xi_{TX}(0)+2\sum_{k=1}^{[T/2]} w_k\Re\left(\Xi_{TX}(\omega_k)\exp(it\omega_k )\right)\right)\label{idft2}
\end{eqnarray}
where $[T/2]=\displaystyle{\left\{\begin{array}{cc}T/2& T\textrm{~even}\\(T-1)/2&T\textrm{~odd}\end{array}\right.}$ and where \\
$w_k=\left\{\begin{array}{cc}1&,[-T/2]\leq k\leq [T/2] \textrm{~if~} T \textrm{~is ~odd}\\
\left\{\begin{array}{cc}1&|k|<T/2\\1/2&|k|=T/2\end{array}\right.&\textrm{~if~} T \textrm{~is ~even}
\end{array}\right.$: please do not confound the weights $w_k$ and the frequencies $\omega_k$.  In practice, the effect of the weights $w_k$ is negligible
and therefore We generally omit them. However, they are necessary when replicating the data $x_t$ \emph{exactly} by the IDFT. The second equation
\ref{idft2} is due to the fact that the real part of $\Xi_{TX}(\omega_k)\exp(it\omega_k )$ is even whereas
its imaginary part is odd if the data $x_t$ is real.\\

Strictly speaking the above identity \ref{idft} is a tautological number identity which applies to any sequence of numbers $x_t$, $t=1,...,T$
irrespective of `model' assumptions.
From a substantial perspective the identity shows that the data $x_t$ can be decomposed into a linear combination of sines and cosines
as weighted by the DFT: $\Xi_{TX}(\omega_k)$ determines the weight of $\exp(it\omega_k )=\cos(t\omega_k)+i\sin(t\omega_k)$ in the decomposition
of $x_t$. If $\left| \Xi_{TX}(\omega_k)\right|$ is `large' (in relative terms), then the corresponding component $\exp(it\omega_k )$
will dominate in $x_t$ and We should see a cycle with the corresponding frequency $\omega_k$ in the time series $x_t$. This decomposition
of the data in the frequency-domain is crucial because it will enable to design filters with `desirable' properties and the resulting
\emph{Direct Filter Approach} (DFA) will generalize the traditional forecasting paradigm to a whole bunch of new practically relevant
prospective estimation problems.  \\

Note that  $\exp(it\omega_k )$ is a periodic function and therefore the identity \ref{idft} implies
\begin{eqnarray*}
\hat{x}_{T+t}&=&\frac{\sqrt{2\pi}}{\sqrt{ T}}\sum_{k=-[T/2]}^{[T/2]} w_k\Xi_{TX}(\omega_k)\exp(i(t+T)\omega_k )\\
&=&\frac{\sqrt{2\pi}}{\sqrt{ T}}\sum_{k=-[T/2]}^{[T/2]} w_k\Xi_{TX}(\omega_k)\exp(it\omega_k )\exp(iT\omega_k )\\
&=&\frac{\sqrt{2\pi}}{\sqrt{ T}}\sum_{k=-[T/2]}^{[T/2]} w_k\Xi_{TX}(\omega_k)\exp(it\omega_k )\\
&=&x_t
\end{eqnarray*}
where We used the fact that $\exp(iT\omega_k )=\exp\left(iT\displaystyle{\frac{k2\pi}{T}}\right)=1$ if $T$ is even (and similarly for odd $T$).
Therefore, the DFT \ref{dft} implicitly assumes that the data is periodic. This is a very unfortunate assumption because it would imply
that the best forecast $\hat{x}_{T+1}$ of $x_{T+1}$ is the first observation $x_1$: a very bad rule in typical economic applications. Fortunately
We can affranchise from this unrealistic assumption since We shall use the DFT or the periodogram as \emph{weighting-functions} in a general optimization problem:
in this case the unrealistic periodicity assumption does not interfer anymore with the relevant forecast problem, assuming some elementary
precaution applies\footnote{Non-stationary trending time series must be trimmed to stationarity (by differencing) before applying the DFT, see
later sections.}\\




\subsubsection{Exercises}

\begin{enumerate}
\item Generate a realization of length $T=301$ (odd number: $w_k=1$ for all $k$) of the seasonal SAR(1)-process
\[y_t=0.9y_{t-12}+\epsilon_t\]
Compute the DFT and plot the data, the acf as well as the absolute value of the DFT $\left|\Xi_{TX}(\omega_k)\right|$, see
fig.\ref{z_sar1}.
<<echo=True>>=
len<-301
set.seed(10)
# Seasonal AR(1)
model<-list(order=c(12,0,0),ar=c(rep(0,11),0.9))
y<-arima.sim(model,n=len)
# DFT
DFT<-0:(len/2)
DFT_c<-0:(len/2)
for (k in 0:(len/2))
{
	cexp <- complex(arg=-(1:len)*2*pi*k/len)
# Complex conjugate: this will be used for computing the IDFT
	cexpt <- complex(arg=(1:len)*2*pi*k/len)
	four<-sum(cexp*y*sqrt(1/(2*pi*len)))
# Complex conjugate: this will be used for computing the IDFT
	fourc<-sum(cexpt*y*sqrt(1/(2*pi*len)))
	DFT[k+1]<-four
# Complex conjugate: this will be used for computing the IDFT
	DFT_c[k+1]<-fourc
}
file = paste("z_sar1.pdf", sep = "")
pdf(file = paste(path.out,file,sep=""), paper = "special",
width = 6, height = 6)
par(mfrow=c(3,1))
ts.plot(y,main="Data: SAR(1)")
acf(y)
plot(abs(DFT),type="l",axes=F,col="blue",main="DFT")
axis(1,at=1+0:6*len/12,labels=c("0","pi/6","2pi/6","3pi/6",
"4pi/6","5pi/6","pi"))
axis(2)
box()
dev.off()
@
<<label=z_sar1.pdf,echo=FALSE,results=tex>>=
  file = paste("z_sar1.pdf", sep = "")
  cat("\\begin{figure}[H]")
  cat("\\begin{center}")
  cat("\\includegraphics[height=4in, width=4in]{", file, "}\n",sep = "")
  cat("\\caption{Seasonal process: data (top), acf (middle) and DFT (bottom)", sep = "")
  cat("\\label{z_sar1}}", sep = "")
  cat("\\end{center}")
  cat("\\end{figure}")
@
The strong seasonality induces large slowly decaying peaks of the acf at seasonal lags $k\cdot 12$, $k=1,2,...$. The
absolute DFT in the bottom plot shows that the data is dominated by cycles of frequencies $k \pi/6$, $k=0,1,...,6$
with durations $\frac{2\pi}{k\pi}=\infty, 12,6, 4 ,3, 2.4$ and $2$ months (zero-frequency is a particulat cycle with infinite duration).
In a way the (absolute value of the) DFT is more informative than the acf about the dominant components in the series.
\item Verify the identity \ref{idft} for the above SAR(1).
<<echo=True>>=
z<-1:len
zh<-z
for (k in 1:len)#k<-1
{
	cexp <- complex(arg=(0:(len/2))*2*pi*k/len)
# Complex canjugate
	cexpt <- complex(arg=-(0:(len/2))*2*pi*k/len)
# Account for weights w_k
	if (abs(len/2-as.integer(len/2))<0.1)
	{
		cexp<-cexp*c(rep(1,length(cexp)-1),(1/2))
		cexpt<-cexpt*c(rep(1,length(cexp)-1),(1/2))
	}
# IDFT
	z[k]<-sum(cexp[1:length(cexp)]*DFT[1:length(DFT)]*sqrt((2*pi)/len))+
  sum(cexpt[2:length(cexpt)]*DFT_c[2:length(DFT_c)]*sqrt((2*pi)/len))
  zh[k]<-sum(c(cexp[1]*DFT[1]*sqrt((2*pi)/len),2*Re(cexp[2:length(cexp)]*
  DFT[2:length(DFT)]*sqrt((2*pi)/len))))
}
cbind(y,z,zh)[(len-30):len,]
@
The R-objects are `automatically' complex since We work with
complex -valued numbers but all imaginary parts vanish, as expected. The identical columns (We restrict the above list to the last 30 observations to save space)
confirm the tautological number identities \ref{idft} and \ref{idft2}.


\item Generate a realization of length $T=101$ (odd number: $w_k=1$ for all $k$) of the MA(1)-process
\[x_t=\mu+\epsilon_t+b_1\epsilon_{t-1}\]
for $b_1=-0.9$, $b_1=0$ and $b_1=0.9$ and plot the absolute values $\left|\Xi_{TX}(\omega_k)\right|$ of the corresponding DFT's,
see fig.\ref{z_ma1_dft}.
<<echo=True>>=
# MA(1)
model_1<-list(order=c(0,0,1),ma=-0.9)
model_3<-list(order=c(0,0,1),ma=0.9)
set.seed(10)
y_1<-arima.sim(model_1,n=len)
set.seed(10)
y_2<-rnorm(len)
set.seed(10)
y_3<-arima.sim(model_3,n=len)
# DFT-----------
DFT_1<-0:(len/2)
DFT_2<-0:(len/2)
DFT_3<-0:(len/2)
for (k in 0:(len/2))
{
	cexp <- complex(arg=-(1:len)*2*pi*k/len)
	DFT_1[k+1]<-sum(cexp*y_1*sqrt(1/(2*pi*len)))
	DFT_2[k+1]<-sum(cexp*y_2*sqrt(1/(2*pi*len)))
	DFT_3[k+1]<-sum(cexp*y_3*sqrt(1/(2*pi*len)))
}
file = paste("z_ma1_dft.pdf", sep = "")
pdf(file = paste(path.out,file,sep=""), paper = "special",
width = 6, height = 6)
par(mfrow=c(3,1))
plot(abs(DFT_1),type="l",axes=F,col="blue",main="|DFT|: b_1=-0.9",xlab="")
axis(1,at=1+0:6*len/12,labels=c("0","pi/6","2pi/6","3pi/6",
"4pi/6","5pi/6","pi"))
axis(2)
box()
plot(abs(DFT_2),type="l",axes=F,col="blue",main="|DFT|: b_1=0",xlab="")
axis(1,at=1+0:6*len/12,labels=c("0","pi/6","2pi/6","3pi/6",
"4pi/6","5pi/6","pi"))
axis(2)
box()
plot(abs(DFT_3),type="l",axes=F,col="blue",main="|DFT|: b_1=0.9",xlab="")
axis(1,at=1+0:6*len/12,labels=c("0","pi/6","2pi/6","3pi/6",
"4pi/6","5pi/6","pi"))
axis(2)
box()
dev.off()
@
<<label=z_ma1_dft.pdf,echo=FALSE,results=tex>>=
  file = paste("z_ma1_dft.pdf", sep = "")
  cat("\\begin{figure}[H]")
  cat("\\begin{center}")
  cat("\\includegraphics[height=4in, width=4in]{", file, "}\n",sep = "")
  cat("\\caption{MA(1): positive (top), zero (middle) and negative (bottom) MA-coefficients", sep = "")
  cat("\\label{z_ma1_dft}}", sep = "")
  cat("\\end{center}")
  cat("\\end{figure}")
@
For $b_1=-0.9$ (top graph) We observe that high-frequency components are weighted more heavily than low-frequency
components and conversely for $b_1=0.9$ (bottom graph). The white noise $b_1=0$ seems to have equally weighted
components.

\item \label{z_ma1s_dft_ex}Compute 100 realizations of the above MA-processes, average the absolute DFT's and plot the averages, see
fig.\ref{z_ma1s_dft}.
<<echo=True>>=
# MA(1)
model_1<-list(order=c(0,0,1),ma=-0.9)
model_3<-list(order=c(0,0,1),ma=0.9)
# AR(2)
pers=0.99;
phi=pi/2;
a1=2*pers*cos(phi);
a2= -pers^2;
model<-list(order=c(2,0,0),ar=c(a1,a2))
simanz<-100
DFT<-matrix(nrow=len/2+1,ncol=3)
abs_dft<-matrix(rep(0,3*(1+(len-1)/2)),nrow=len/2+1,ncol=3)
# Simulation runs
for (i in 1:simanz)
{
  set.seed(i)
  y_1<-arima.sim(model_1,n=len)
  set.seed(i)
  y_2<-rnorm(len)
  set.seed(i)
  y_3<-arima.sim(model_3,n=len)
  # DFT-----------
  for (k in 0:(len/2))
  {
  	cexp <- complex(arg=-(1:len)*2*pi*k/len)
  	DFT[k+1,1]<-sum(cexp*y_1*sqrt(1/(2*pi*len)))
  	DFT[k+1,2]<-sum(cexp*y_2*sqrt(1/(2*pi*len)))
  	DFT[k+1,3]<-sum(cexp*y_3*sqrt(1/(2*pi*len)))
  }
  abs_dft<-abs_dft+abs(DFT)
}
abs_dft<-abs_dft/simanz
  
file = paste("z_ma1s_dft.pdf", sep = "")
pdf(file = paste(path.out,file,sep=""), paper = "special",
width = 6, height = 6)
par(mfrow=c(3,1))
ymin<-0
ymax<-max(abs_dft[,1])
plot(abs_dft[,1],type="l",axes=F,col="blue",main="|DFT|: b_1=-0.9",
xlab="",ylim=c(ymin,ymax))
axis(1,at=1+0:6*len/12,labels=c("0","pi/6","2pi/6","3pi/6","4pi/6",
"5pi/6","pi"))
axis(2)
box()
ymax<-max(abs_dft[,2])
plot(abs_dft[,2],type="l",axes=F,col="blue",main="|DFT|: b_1=0",xlab="",
ylim=c(ymin,ymax))
axis(1,at=1+0:6*len/12,labels=c("0","pi/6","2pi/6","3pi/6","4pi/6",
"5pi/6","pi"))
axis(2)
box()
ymax<-max(abs_dft[,3])
plot(abs_dft[,3],type="l",axes=F,col="blue",main="|DFT|: b_1=0.9",
xlab="",ylim=c(ymin,ymax))
axis(1,at=1+0:6*len/12,labels=c("0","pi/6","2pi/6","3pi/6","4pi/6",
"5pi/6","pi"))
axis(2)
box()
dev.off()
@
<<label=z_ma1s_dft.pdf,echo=FALSE,results=tex>>=
  file = paste("z_ma1s_dft.pdf", sep = "")
  cat("\\begin{figure}[H]")
  cat("\\begin{center}")
  cat("\\includegraphics[height=4in, width=4in]{", file, "}\n",sep = "")
  cat("\\caption{MA(1): positive (top), zero (middle) and negative (bottom) MA-coefficients", sep = "")
  cat("\\label{z_ma1s_dft}}", sep = "")
  cat("\\end{center}")
  cat("\\end{figure}")
@
Averaging the absolute DFT's leads to a clearer picture by damping realization-specific noise disturbances: positive
autocorrelation ($b_1>0$) emphasizes low-frequency components whereas negative autocorrelation ($b_1<0$) means stronger
high-frequency components. In contrast, the white noise is characterized by a flat absolute DFT: all frequency-components are weighted
equally; likewise, all colours are weighted equally in white-light, from which the (physical) analogy is taken.
\item To conclude We analyze an AR(2)-process with a strong cycle
\[y_t=0.99y_{t-1}-0.9801y_{t-2}+\epsilon_t\]
Looking at the roots of the characteristic polynomial
<<echo=True>>=
abs(polyroot(c(0.9801,-0.99,1)))
Arg(polyroot(c(0.9801,-0.99,1)))/pi
2*pi/Arg(polyroot(c(0.9801,-0.99,1)))
@
We find the absolute value to be large (0.99). The frequency of the cycle is $\pi/3$ with periodicity 6. These characteristics
should be reflected by the DFT.
<<echo=True>>=
# AR(2)
pers=0.99;
phi=pi/3;
a1=2*pers*cos(phi)
a2= -pers^2
model<-list(order=c(2,0,0),ar=c(a1,a2))
set.seed(10)
y_ar2<-arima.sim(model,n=len)
# DFT-----------
DFT_ar2<-0:(len/2)
for (k in 0:(len/2))
{
	cexp <- complex(arg=-(1:len)*2*pi*k/len)
	DFT_ar2[k+1]<-sum(cexp*y_ar2*sqrt(1/(2*pi*len)))
}
file = paste("z_ar2_dft.pdf", sep = "")
pdf(file = paste(path.out,file,sep=""), paper = "special",
width = 6, height = 6)
par(mfrow=c(2,1))
ts.plot(y,xlab="",main="Data: AR(2)")
plot(abs(DFT_ar2),type="l",axes=F,col="blue",
main="|DFT| cyclical AR(2)",xlab="")
axis(1,at=1+0:6*len/12,labels=c("0","pi/6","2pi/6","3pi/6","4pi/6",
"5pi/6","pi"))
axis(2)
box()
dev.off()
@
<<label=z_ar2_dft.pdf,echo=FALSE,results=tex>>=
  file = paste("z_ar2_dft.pdf", sep = "")
  cat("\\begin{figure}[H]")
  cat("\\begin{center}")
  cat("\\includegraphics[height=4in, width=4in]{", file, "}\n",sep = "")
  cat("\\caption{AR(2): data (top) and abs(DFT) (bottom)", sep = "")
  cat("\\label{z_ar2_dft}}", sep = "")
  cat("\\end{center}")
  cat("\\end{figure}")
@
\item Compute and plot the IDFT of a so-called spectral line
\[\Xi_{TX}(\omega_k)=\left\{\begin{array}{cc}1&k=k_1\\0&\textrm{otherwise}\end{array}\right.\]
as well as a double line
\[\Xi_{TX}(\omega_k)=\left\{\begin{array}{cc}1&k=k_1\\
2&k=k_2\\
0&\textrm{otherwise}\end{array}\right.\]
For Our example We assume that $T=1000$ and $k_1=10, k_2=20$.
<<echo=True>>=
len<-1000
z_1<-1:len
z_2<-z
DFT_1<-rep(0,1+len/2)
DFT_1[10]<-1
DFT_2<-DFT_1
DFT_2[20]<-2
for (k in 1:len)#k<-1
{
	cexp <- complex(arg=(0:(len/2))*2*pi*k/len)
# Weights w_k
	if (abs(len/2-as.integer(len/2))<0.1)
	{
		cexp<-cexp*c(rep(1,length(cexp)-1),(1/2))
	}
# IDFT
  z_1[k]<-sum(c(cexp[1]*DFT_1[1]*sqrt((2*pi)/len),
  2*Re(cexp[2:length(cexp)]*DFT_1[2:length(DFT_1)]*sqrt((2*pi)/len))))
  z_2[k]<-sum(c(cexp[1]*DFT_2[1]*sqrt((2*pi)/len),
  2*Re(cexp[2:length(cexp)]*DFT_2[2:length(DFT_2)]*sqrt((2*pi)/len))))
}

file = paste("z_ls.pdf", sep = "")
pdf(file = paste(path.out,file,sep=""), paper = "special", width = 6, height = 6)
par(mfrow=c(2,1))
ts.plot(z_1,xlab="",main="IDFT of single line spectrum")
ts.plot(z_2,xlab="",main="IDFT of double line spectrum")
dev.off()
@
<<label=z_ls.pdf,echo=FALSE,results=tex>>=
  file = paste("z_ls.pdf", sep = "")
  cat("\\begin{figure}[H]")
  cat("\\begin{center}")
  cat("\\includegraphics[height=4in, width=4in]{", file, "}\n",sep = "")
  cat("\\caption{IDFT applied to single (top) and double (bottom) line spectra", sep = "")
  cat("\\label{z_ls}}", sep = "")
  cat("\\end{center}")
  cat("\\end{figure}")
@
A single line spectrum in the frequency-domain corresponds to a single cosine function with the corresponding frequency. A double line
corresponds to the weighted sum of two cosine functions. Arbitrary data can be generated or replicated by more complex line spectra:
this is the `core message' of the DFT and the IDFT. Moreover, We do not require any possibly unrealistic model assumption: the tautological
essence of the identity confers illimited generality to the decomposition.
\item Consider the non-stationary integrated time series
\[y_t=y_{t-1}+\epsilon_t\]
where $\epsilon_t$ is a white noise sequence with $\sigma^2=1$. Generate a realization of length 300 of the above time series,
compute the DFT \ref{dft} and the IDFT \ref{idft} and verify the identity \ref{idft}. Plot the absolute DFT and the data, see fig.\ref{z_ns}.
<<echo=True>>=
set.seed(10)
len<-300
y<-cumsum(rnorm(len))
# DFT-----------
DFT<-0:(len/2)
for (k in 0:(len/2))
{
	cexp <- complex(arg=-(1:len)*2*pi*k/len)
	DFT[k+1]<-sum(cexp*y*sqrt(1/(2*pi*len)))
}
# IDFT
z<-y
for (k in 1:len)#k<-1
{
	cexp <- complex(arg=(0:(len/2))*2*pi*k/len)
# Weights w_k
	if (abs(len/2-as.integer(len/2))<0.1)
	{
		cexp<-cexp*c(rep(1,length(cexp)-1),(1/2))
	}
# IDFT
  z[k]<-sum(c(cexp[1]*DFT[1]*sqrt((2*pi)/len),2*Re(cexp[2:length(cexp)]*
  DFT[2:length(DFT)]*sqrt((2*pi)/len))))
}
cbind(y,z)[(len-20):len,]
file = paste("z_ns.pdf", sep = "")
pdf(file = paste(path.out,file,sep=""), paper = "special",
width = 6, height = 6)
par(mfrow=c(2,1))
ts.plot(y,xlab="",main="Non-stationary data")
plot(abs(DFT),type="l",axes=F,col="blue",
main="|DFT| non-stationary data",xlab="")
axis(1,at=1+0:6*len/12,labels=c("0","pi/6","2pi/6","3pi/6",
"4pi/6","5pi/6","pi"))
axis(2)
box()
dev.off()
@
<<label=z_ns.pdf,echo=FALSE,results=tex>>=
  file = paste("z_ns.pdf", sep = "")
  cat("\\begin{figure}[H]")
  cat("\\begin{center}")
  cat("\\includegraphics[height=4in, width=4in]{", file, "}\n",sep = "")
  cat("\\caption{Non-stationary data (top) and absolute DFT (bottom)", sep = "")
  cat("\\label{z_ns}}", sep = "")
  cat("\\end{center}")
  cat("\\end{figure}")
@
The non-stationarity (stochastic trend) of $y_t$ is refelected by a huge peak of $|\textrm{DFT}|$ towards frequency zero:
the low-frequency components dominate the data.

\item \label{period_e}Compute $\hat{x}_{T+1},\hat{x}_{T+2},...,\hat{x}_{T+601}$ based on the IDFT \ref{idft} and plot the `forecasts'.
<<echo=True>>=
# IDFT
z<-rep(y,3)
for (k in len+1:(2*len))#k<-1
{
	cexp <- complex(arg=(0:(len/2))*2*pi*k/len)
# Weights w_k
	if (abs(len/2-as.integer(len/2))<0.1)
	{
		cexp<-cexp*c(rep(1,length(cexp)-1),(1/2))
	}
# IDFT
  z[k]<-sum(c(cexp[1]*DFT[1]*sqrt((2*pi)/len),
  2*Re(cexp[2:length(cexp)]*DFT[2:length(DFT)]*sqrt((2*pi)/len))))
}
z<-Re(z)
file = paste("z_nsf.pdf", sep = "")
pdf(file = paste(path.out,file,sep=""), paper = "special",
width = 6, height = 6)
ts.plot(c(z[1:len],rep(NA,2*len)),xlab="",
main="Non-stationary data (black) and forecasts based on IDFT (blue)")
lines(c(rep(NA,len),z[len+1:(2*len)]),col="blue",lwd=1)
dev.off()
@
<<label=z_nsf.pdf,echo=FALSE,results=tex>>=
  file = paste("z_nsf.pdf", sep = "")
  cat("\\begin{figure}[H]")
  cat("\\begin{center}")
  cat("\\includegraphics[height=4in, width=4in]{", file, "}\n",sep = "")
  cat("\\caption{Non-stationary data (black) and IDFT forecasts", sep = "")
  cat("\\label{z_nsf}}", sep = "")
  cat("\\end{center}")
  cat("\\end{figure}")
@
The above forecasts confirm the periodicity argument. Obviously, periodicity heavily conflicts with the non-stationarity
of the data because it implies an unrealistic disruption of the dominating trend component at the end of the time series.



\end{enumerate}

\subsection{Periodogram}\label{periodogram_s}

The \emph{periodogram} $I_{TX}(\omega_k)$ is defined in terms of the squared absolute DFT:
\begin{eqnarray}
I_{TX}(\omega_k)=\left|\Xi_{TX}(\omega_k)\right|^2
\end{eqnarray}
The periodogram is less informative than the DFT because the phase angle of the complex number $\Xi_{TX}(\omega_k)$ is lost/ignored. In fact
one can show that the periodogram is the DFT of the \emph{sample autocovariance function} $\hat{R}(k)$:
\begin{equation}\label{per3}
I_{TX}(\omega_k)= \left\{\begin{array}{ccc}\displaystyle{
\frac{1}{2\pi} \sum_{j=-(T-1)}^{T-1} \hat{R}(j) \exp(-ij\omega_k)}&,&|k|=1,...,T/2\\
\displaystyle{\frac{T}{2\pi}}\overline{x}^2&,&k=0 \end{array}\right.\end{equation}
where
\begin{eqnarray}\label{rhat}
\hat{R}(j):=\frac{1}{T}\sum_{t=1}^{T-|j|}x_tx_{t+|j|}
\end{eqnarray}
is the sample autocovariance of a zero-mean stationary process. For $|j|=0,...,T-1$ one obtains `almost' an inverse transformation
\begin{equation}\label{disdv}
\frac{2\pi}{T} \sum_{k=-[T/2]}^{[T/2]} w_k\exp(-ij\omega_k)I_{TX}(\omega_k)=\left\{\begin{array}{ccc}
\hat{R}(j)+\hat{R}(T-j)&,&t\not= 0\\
\hat{R}(0)&,&\textrm{otherwise}
\end{array}\right.
\end{equation}
Note that for large sample sizes $T$ and small $j$, the sample autocovariance $\hat{R}(T-|j|):=\frac{1}{T}\sum_{t=1}^{|j|}x_tx_{t+|j|}$ will
be small compared to $\hat{R}(|j|)$ and therefore \ref{disdv} is `nearly' the inverse of \ref{per3}.
A formal proof of these identities is to be found in proposition \ref{dper3} in the appendix where it is shown, also, that the
additional $\hat{R}(T-j)$ in \ref{disdv} is an expression of the implicit periodicity of the data.\\

The periodogram can be interpreted as a decomposition
of the sample variance into contributions by components of frequency $\omega_k$:
\begin{equation}\label{spec_dec_per}
\hat{R}(0)=\frac{2\pi}{T} \sum_{k=-[T/2]}^{[T/2]} I_{TX}(\omega_k) =\frac{2\pi}{T}I_{TX}(0)+2\frac{2\pi}{T} \sum_{k=1}^{[T/2]} I_{TX}(\omega_k)
\end{equation}
where for simplicity of notation We discarded the practically negligible weights $w_k$. The value $2\frac{2\pi}{T}I_{TX}(\omega_k)$ measures
that part of the sample variance which is attributable to components with frequency $\omega_k$.\\





For a stationary zero-mean process the statistics $\hat{R}(k)$ in \ref{rhat} are the natural sample estimates of the autocovariance function $R(k)$.
Then the periodogram
is a natural estimate of the so-called \emph{spectral density}
\[h(\omega)=\frac{1}{2\pi}\sum_{j=-\infty}^{\infty} {R}(j) \exp(-ij\omega)\]
of the stationary process. The spectral density measures contributions of components with frequency $\omega$ to the \emph{variance} of the
process\footnote{Instead of the discrete sum of the periodogram one would need to integrate the spectral density on a pre-specified frequency-interval
but We here omit practically irrelevant details of this classical theory.}. For a \emph{white noise} sequence ${R}(k)=0$ for $|k|>0$ so that
\[h(\omega)=\frac{\sigma^2}{2\pi}\]
is constant: all components contribute equally to the variance (physical analogy: white light). Unfortunately, the sample estimates do not
vanish: $\hat{R}(k)\not=0$; therefore the periodogram \ref{per3} of a realization of a white noise is not a constant function.
But in the mean, over many realizations,
the sample autocovariances tend to zero and therefore the average periodogram converges to a constant -- a flat spectrum -- too, recall
exercise \ref{z_ma1s_dft_ex}, fig. \ref{z_ma1s_dft} middle graph: the average absolute DFT (the square root of the periodogram)
is concentrated in a narrow band about a constant level. Therefore We could rely on the periodogram statistic for \emph{diagnostic} purposes:
model residuals should have a flat spectrum i.e. the periodogram should be `approximately'
flat\footnote{The periodogram is used as a diagnostic statistic in X-12-ARIMA or X-13-SEATS, for example.}.\\




In the case of a non-stationary integrated process the
autocovariance function is no more time-invariant. However, one can difference the process -- until stationarity is achieved -- and then define
a so-called pseudo spectral density based on the density of the differenced (stationary) process. We here omit details and
refer to later sections where We shall introduce a pseudo-periodogram.\\

Note that \ref{per3} and \ref{disdv} are once again tautological number identities which hold irrespective of model assumptions. Substance
is obtained by interpreting \ref{rhat} as sample estimates of the autocovariance function which requires the process to be stationary. However,
Our usage of the periodogram and the DFT will be `different' than in classical time series analysis
and therefore We can neglect, to some extent, typical model-based arguments.



\subsubsection{Exercises}

\begin{enumerate}
\item Compute the sample Autocovariance function of the AR(2) process in the previous exercise and verify pertinence of \ref{per3}.
<<echo=True>>=
len<-length(y_ar2)
Rk<-1:len
# Compute sample autocovariances
for (j in 1:len)
{
  Rk[j]<-y_ar2[j:len]%*%y_ar2[1:(len-j+1)]/len
}
# Compute periodogram based on sample autocovariances
per<-rep(0,1+len/2)
for (k in 0:as.integer(len/2))  #k<-1
{
  omega_k<-k*pi*2/len
  per[k+1]<-(1/(2*pi))*(Rk[1]+2*Re(Rk[2:len]%*%exp(-1.i*omega_k*1:(len-1))))
}
cbind(abs(DFT_ar2)^2,per)[1:30,]
@
The series are identical, thus confirming \ref{per3}.

\item Write a function which computes the DFT as well as the periodogram of a time series $x_t$. Hint: from now on
We ignore the weights $w_k$.
<<echo=True>>=
per<-function(x,plot_T)
{
  len<-length(x)
  per<-0:(len/2)
  DFT<-per

  for (k in 0:(len/2))
  {
    cexp <- complex(arg=-(1:len)*2*pi*k/len)
    DFT[k+1]<-sum(cexp*x*sqrt(1/(2*pi*len)))
  }
  per<-abs(DFT)^2
  if (plot_T)
  {
    par(mfrow=c(2,1))
    plot(per,type="l",axes=F,xlab="Frequency",ylab="Periodogram",
    main="Periodogram")
    axis(1,at=1+0:6*len/12,labels=c("0","pi/6","2pi/6","3pi/6",
    "4pi/6","5pi/6","pi"))
    axis(2)
    box()
    plot(log(per),type="l",axes=F,xlab="Frequency",ylab="Log-periodogram",
    main="Log-periodogram")
    axis(1,at=1+0:6*len/12,labels=c("0","pi/6","2pi/6","3pi/6",
    "4pi/6","5pi/6","pi"))
    axis(2)
    box()
  }
  return(list(DFT=DFT,per=per))
}
@

\item Verify that the function replicates the periodogram of the above AR(2)-realization.
<<echo=True>>=
plot_T<-F
cbind(per(y_ar2,plot_T)$per,abs(DFT_ar2)^2)[1:30,]
@

\item \label{lo_p_e}Compute the periodogram of the white noise sequence as well as of the lowpassed series in fig.\ref{z_Gamma_n} by relying
on the new periodogram function, see fig.\ref{z_per_n_lpn}. Hint: use
the common time sample $t=100,...,200$ for both the noise $\epsilon_t$ as well as the lowpassed $y_t$.
<<echo=True>>=
len<-300
set.seed(10)
eps<-rnorm(len)
y<-rep(NA,len)
for (k in 100:200)#k<-0
{
	y[k]<-gamma[1:100]%*%eps[k+(0:99)]+gamma[2:100]%*%eps[k-(1:99)]
}
file = paste("z_per_n_lpn.pdf", sep = "")
pdf(file = paste(path.out,file,sep=""), paper = "special", width = 6, height = 6)
plot_T<-F
plot(per(na.exclude(y),plot_T)$per,type="l",axes=F,
main="Periodograms of noise (blue) and lowpassed noise(red)",xlab="",
ylab="",col="red",lwd=2)
lines(per(eps[!is.na(y)],plot_T)$per,col="blue",lwd=2)
K<-length(per(na.exclude(y),plot_T)$per)-1
axis(1, at=c(0,1+1:6*K/6),labels=c("0","pi/6","2pi/6","3pi/6",
"4pi/6","5pi/6","pi"))
axis(2)
box()
dev.off()
@
<<label=z_per_n_lpn.pdf,echo=FALSE,results=tex>>=
  file = paste("z_per_n_lpn.pdf", sep = "")
  cat("\\begin{figure}[H]")
  cat("\\begin{center}")
  cat("\\includegraphics[height=4in, width=4in]{", file, "}\n",sep = "")
  cat("\\caption{Noise (blue) and lowpassed noise (red)", sep = "")
  cat("\\label{z_per_n_lpn}}", sep = "")
  cat("\\end{center}")
  cat("\\end{figure}")
@
We can see that the periodogram of the lowpassed series (red) is close to zero for frequencies in the stopband $\omega_k>\pi/6=\textrm{cutoff}$.
In the passband $\omega_k<\pi/6=\textrm{cutoff}$ both periodograms are similar. Note that We applied a finite approximation of the lowpass
of order 100. By letting the order increase arbitrarily (the time series must be lengthened accordingly) We could verify that the
periodogram of the lowpassed series (red) would converge to zero in the
stopband; in the passband it would converge to the periodogram of the original noise (blue). The term \emph{lowpass} means that the
low frequencies (in the passband) of an arbitrary time series  `go through' whereas the higher frequencies (in the stopband)
are damped or even eliminated. Since We are able to design arbitrary bandpass functions and since any `well behaved' function
can be approximated arbitrary well by a suitable linear combination of bandpasses We could in principle \emph{extract} (highlight) or
eliminate arbitrary components in a series. Note, however, that the above filter cannot be applied towards the sample-end because of
symmetry: it is not designed for \emph{real-time} applications.


\item We here rely on the periodogram for diagnostic purposes. Generate a realization of length 100 of the AR(2)-process
\[x_t=1.4x_{t-1}-0.7x_{t-2}+\epsilon_t\]
with $\sigma^2=1$.
\begin{itemize}
\item Plot the data as well as acf and pacf and determine a plausible model, see fig.\ref{z_acf_ar2}.
<<echo=True>>=
len<-100
set.seed(10)
model<-list(order=c(2,0,0),ar=c(1.4,-0.7))
y<-arima.sim(model,n=len)

file = paste("z_acf_ar2.pdf", sep = "")
pdf(file = paste(path.out,file,sep=""), paper = "special",
width = 6, height = 6)
par(mfrow=c(3,1))
ts.plot(y,main="Data AR(2)")
acf(y)
acf(y,type="partial")
dev.off()
@
<<label=z_acf_ar2.pdf,echo=FALSE,results=tex>>=
  file = paste("z_acf_ar2.pdf", sep = "")
  cat("\\begin{figure}[H]")
  cat("\\begin{center}")
  cat("\\includegraphics[height=4in, width=4in]{", file, "}\n",sep = "")
  cat("\\caption{Data AR(2) and sample acf/pacf", sep = "")
  cat("\\label{z_acf_ar2}}", sep = "")
  cat("\\end{center}")
  cat("\\end{figure}")
@
\item Fit true AR(2)- as well as false AR(1)-,
MA(2)- and MA(5)-models: compute and plot periodograms of the resulting model residuals, see fig.\ref{z_per_ar2_diag}.
<<echo=True>>=
# false models
model_ar1<-list(order=c(1,0,0))
model_ma2<-list(order=c(0,0,2))
model_ma5<-list(order=c(0,0,5))

y.true<-arima(y,order=model$order)
res_true<-y.true$residuals
y.ar1<-arima(y,order=model_ar1$order)
res_ar1<-y.ar1$residuals
y.ma2<-arima(y,order=model_ma2$order)
res_ma2<-y.ma2$residuals
y.ma5<-arima(y,order=model_ma5$order)
res_ma5<-y.ma5$residuals

file = paste("z_per_ar2_diag.pdf", sep = "")
pdf(file = paste(path.out,file,sep=""), paper = "special",
width = 6, height = 6)
par(mfrow=c(2,1))
plot_T<-F
plot(per(res_true,plot_T)$per,type="l",axes=F,
main="Periodograms of residuals : true AR(2)",xlab="",
ylab="",col="black",lwd=2)
K<-length(per(res_true,plot_T)$per)-1
axis(1, at=c(0,1+1:6*K/6),labels=c("0","pi/6","2pi/6","3pi/6",
"4pi/6","5pi/6","pi"))
axis(2)
box()
plot(per(res_ar1,plot_T)$per,type="l",axes=F,
main="Periodograms of residuals: false AR(1), MA(2), MA(5)",xlab="",
ylab="",col="red",lwd=2)
lines(per(res_ma2,plot_T)$per,col="green",lwd=2)
lines(per(res_ma5,plot_T)$per,col="blue",lwd=2)
mtext("AR(1)", side = 3, line = -1,at=len/4,col="red")
mtext("MA(2)", side = 3, line = -2,at=len/4,col="green")
mtext("MA(5)", side = 3, line = -3,at=len/4,col="blue")
K<-length(per(res_true,plot_T)$per)-1
axis(1, at=c(0,1+1:6*K/6),labels=c("0","pi/6","2pi/6","3pi/6",
"4pi/6","5pi/6","pi"))
axis(2)
box()
dev.off()
@
<<label=z_per_ar2_diag.pdf,echo=FALSE,results=tex>>=
  file = paste("z_per_ar2_diag.pdf", sep = "")
  cat("\\begin{figure}[H]")
  cat("\\begin{center}")
  cat("\\includegraphics[height=4in, width=4in]{", file, "}\n",sep = "")
  cat("\\caption{Periodograms of true AR(2) (top) and false AR(1), MA(2), MA(5)", sep = "")
  cat("\\label{z_per_ar2_diag}}", sep = "")
  cat("\\end{center}")
  cat("\\end{figure}")
@
The periodogram of the residuals of the true AR(2)-model is typically `noisy' but otherwise flat: no portions of the frequency band seem
over- or underemphasized. In contrast, the residuals of the misspecified AR(1) and MA(2)-models show evidence of a strong peak in the vicinity
of $\pi/6$. Due to its larger model order, the peak of the false MA(5) is less prominent. By computing the roots of the characteristic
polynomial of the process
<<echo=True>>=
polyroot(c(0.7,-1.4,1))
@
We see that the frequency
<<echo=True>>=
Arg(polyroot(c(0.7,-1.4,1)))
@
of the AR(2)-cycle is $\pi/\Sexpr{round(pi/Arg(polyroot(c(0.7,-1.4,1)))[1],2)}$ which corresponds to the frequency of the
residual peaks in the misspecified models.
\end{itemize}
\end{enumerate}

\section{Filters}

The DFA relies on `filters' rather than `models'. In a model-based approach, `filters' are generally
obtained from `models' (of the DGP). In the DFA We jump over the model-based
bottle-neck\footnote{The classic model-based paradigm relies on rigid one-step ahead forecast
performances. We overcome this issue by proposing a more general estimation paradigm.} and compute
filters `directly', without allusion to a hypothetical DGP. This way, We can derive targeted -- customized --
optimization criteria which allow to control filter characteristics according to important
user-priorities.

\subsection{Introduction}

We here present some ordinary definitions and propose the main filter-classe(s).

\subsubsection{Definitions}\label{def_fil}

A \emph{filter} is a general (linear) transformation, mapping an \emph{input} series/signal $x_t$ to an \emph{output} series/signal $y_t$:
\begin{equation}\label{target_i}
y_t=\sum_{k=-\infty}^{\infty}\gamma_{k} x_{t-k}
\end{equation}
If there does not exist a $k_0>0$ such $\gamma_{|k|}=0$ for $|k|>k_0$ then the filter is called \emph{bi-infinite}:
the filter-coefficients stretch indefinitely on both (positive/negative) `sides'.
If $\gamma_k=\gamma_{-k}$ then
the filter is called \emph{symmetric}: the lowpass in section \ref{lowpass} is a `typical' example. If $\gamma_k=0$ for $k<0$ then the
resulting \emph{one-sided} filter is called \emph{causal}. If $\sum_{k=-\infty}^{\infty}|\gamma_k|^2<\infty$ then the filter is called
\emph{stable}\footnote{In the literature one often finds the classical $L^1$-stability $\sum_{k=-\infty}^{\infty}|\gamma_k|<\infty$ which is more constraining. We
here adopt a slightly more general $L^2$-perspective, relying on the Wold-decomposition theorem (according to which MA-ceofficients must
be square summable, too).}. \\
In applications one has to rely on a \emph{finite} sample  $x_1,...,x_T$ for estimating $y_{T+h}$, where $T$ is the sample size and $h\in \mbox{Z\hspace{-.3em}Z}$ is
any positive or negative integer:
\begin{equation}\label{filter_i}
\hat{y}_{T+h}:=\sum_{k=h}^{L-1+h}b_{kh}x_{T+h-k}
\end{equation}
We distinguish the target filter and the approximating filter in notational terms by using $\gamma_k$ for the former and $b_{kh}$ for the latter.
Note that
\begin{itemize}
\item the estimate $\hat{y}_{T+h}$ can be computed for any $t=T+h$ by setting $h:=t-T$
\item the estimate $\hat{y}_{T+h}$ always relies
on $x_1,...,x_T$, uniquely
\item the time-shift $h$ is obtained by designing filter-coefficients $b_{kh}$ accordingly
\end{itemize}
If $\Gamma(\cdot)$ is \emph{symmetric} (no time-shift) and if $h=0$, then $\hat{y}_{T}$ is called a \emph{nowcast} or
\emph{real-time} estimate (of $y_T$) and $b_{kh}$ are (the coefficients of) a \emph{real-time filter}.
For $h>0$ the estimate $\hat{y}_{T+h}$ is called a \emph{forecast} and $b_{kh}$ are sometimes referred as a forecast filter.
For $h<0$  the expression \ref{filter_i} is called a \emph{smoother} and $\hat{y}_{T+h}$ is sometimes referred to as
a \emph{backcast}.\\


A filter transformation is often
identified with its filter coefficients or (slightly abusively) with its output; therefore the term `filter' refers either to the transformation (\ref{target_i}, \ref{filter_i}) or to
the coefficients ($\gamma_k$, $b_{kh}$) or to the output ($y_t$, $\hat{y}_{T+h}$)\footnote{Note that the output alone (without the
corresponding input) does not determine the filter coefficients. Therefore the last reference is formally abusive but effective and useful in notational terms.}.  \\



\subsubsection{Examples}

Conceptually, a `filter' is a transformation. As an example, an invertible MA(1)-process
\[x_t=\epsilon_t+b_1\epsilon_{t-1}\]
can be interpreted as a one-sided finite stable and causal filter with weights ${\gamma}_0=1,{\gamma}_1=b_1$ whose input series is white noise $\epsilon_t$ and whose
output is the MA(1)-process. The transformation maps the uncorrelated noise to an autocorrelated process. A stationary AR(1)-process
\[x_t=a_1x_{t-1}+\epsilon_t=\sum_{k=0}^{\infty} a_1^k\epsilon_{t-k}\]
is a one-sided stable causal infinite filter with coefficients $\gamma_k=a_1^k$ and input series $\epsilon_t$. More generally,
SARIMA-models can be interpreted as particular one-sided filters whose input series is invariably white noise (if the process
is integrated then the filter is not stable anymore). Generalizing even further, the input series does not have to be white noise:
literally \emph{any} input series is allowed. \\

Filter weights are generally specified/designed in view of performing a particular \emph{task}. As an example, exercise \ref{lo_p_e}
on p.\pageref{lo_p_e} illustrated that the ideal lowpass filter \ref{lwhfsw} eliminates high-frequency noise components, see fig.\ref{z_per_n_lpn}:
the resulting filter output $y_t$ in fig.\ref{z_Gamma_n} is smooth. Alternatively, a (finite) filter can be designed in view of estimating a
particular \emph{target} \ref{target_i} based on data $x_1,...,x_T$. As an example, We shall be interested in deriving an optimal finite-sample filter
\ref{filter_i} whose output approximates the ideal (bi-infinite) lowpass
\[y_{T+h}=\frac{1}{\pi}\sum_{k=-\infty}^\infty\frac{\sin(k\cdot\textrm{cutoff})}{k} x_{T+h-k}\]
in $t=T+h$. We proposed and analyzed simple finite-sample filters approximating the ideal lowpass in section \ref{ilpe}, by truncating filter coefficients. More sophisticated
`user-relevant' estimates will be presented later. Alternatively, We might
be interested in \emph{forecasting} the above MA(1)- or AR(1)-processes such that the target becomes
\[y_t=\sum_{k=-\infty}^\infty\gamma_k x_{t-k}=x_{t+1}\]
i.e. $\gamma_{-1}=1,\gamma_{k}=0, k\not= -1$: the target filter is finite, asymmetric and non-causal.
The best finite sample approximation
of this target in the AR(1)-case is the well-known one-step ahead forecast function
\[\hat{y}_t=\hat{x}_{t+1}=a_1x_t\]
which is of length $L=1$: the coefficients of the approximating filter are $\displaystyle{b_k=\left\{\begin{array}{cc}
a_1&k=0\\0&\textrm{otherwise}\end{array}\right.}$. In the MA(1)-case We obtain
\[\hat{y}_t=\hat{x}_{t+1}=\sum_{k=0}^{\infty} (-1)^{k}b_1^{k+1}x_{t-k}\approx \sum_{k=0}^{T-1} (-1)^{k}b_1^{k+1}x_{t-k}\]
where the finite approximation of length $L=T$ is `good' if the process is invertible ($|b_1|<1$) and the sample size
$T$ is `large'. The coefficients of the approximating filter are $(-1)^{k}b_1^{k+1}, k=0,...,L-1$.


\subsubsection{Filter-Classes: MA-, AR- and ARMA-Filters}


An MA-filter of length $L$ relates output ${y}_{t}$ and input $x_{t+h-j}, j=0,...,L-1$ according to
\[
{y}_{t}:=\sum_{j=0}^{L-1}b_{j-h}x_{t+h-j}
\]
where $h$ allows for arbitrary time-shifts. An AR-filter of length $L'$ links the output $y_t$ to lagged outputs $y_{t-k}, k=1,...,L'$
and the input $x_{t+h}$
\[
{y}_{t}:=\sum_{k=1}^{L'}a_{k} y_{t-k}+b_{-h}x_{t+h}
\]
where $h$ allows for arbitrary time-shifts and $b_{-h}$ is a scaling term. Finally, an ARMA-filter of order
$(L,L')$ relates $y_t$ to lagged $y_{t-k}, k=1,...,L'$ and $x_{t+h-j}, j=0,...,L-1$:
\[
{y}_{t}:=\sum_{k=1}^{L'}a_{k} y_{t-k}+\sum_{j=0}^{L-1}b_{j-h}x_{t+h-j}
\]
The most important class in Our applications, by far, will be the MA-filters. AR and/or ARMA-filters will be used episodically, in exercices.
Both filter types can be inverted into MA($\infty$)-designs: the resulting filter is stable if the AR-part is `stationary' i.e.
if all roots of the characteristic polynomial are strictly smaller than one in absolute value.\\




As illustrated by the above examples, filters allow to address general prospective estimation problems: traditional SARIMA-models and
classical forecast functions are but one possible application. We now propose to
analyze the link between input and output series and to characterize `filters' according to their `effects'. Once completed, We can
proceed to design filters according to desirable properties/effects/characteristics.
Neither of these requirements can be addressed satisfactorily in the time domain.






\subsection{Filter Effects}


\subsubsection{Transfer function: Amplitude, Phase and Time-Shift Functions}

Let $y_t$ be the output of a general filter
\[y_t=\sum_{k=-\infty}^{\infty}\gamma_kx_{t-k}\]
In order to derive the important filter effect We assume a particular (complex-valued)
input series $x_t:=\exp(it\omega )$.
The output signal is thus
\begin{eqnarray}\label{aidehh}
y_{t}&=&\sum_{k=-\infty}^{\infty}{\gamma}_{k}\exp(i\omega(t-k))\\
&=&\exp(i\omega t)\sum_{k=-\infty}^{\infty}{\gamma}_{k}\exp(-ik\omega)\\
&=&\exp(i\omega t){\Gamma}(\omega)
\end{eqnarray}
where the (generally complex) function
\begin{eqnarray}
\Gamma(\omega):=\sum_{k=-\infty}^{\infty}{\gamma}_k\exp(-ik\omega)
\end{eqnarray}
is called the \emph{transfer function} of the filter and $\gamma_k$ are its Fourier coefficients. The lowpass in section
\ref{lowpass} is an example of a (filter) transfer function.
We can represent the complex number $\Gamma(\omega)$ in polar coordinates according to
\begin{eqnarray}
\Gamma(\omega)=A(\omega)\exp(-i\Phi(\omega))
\end{eqnarray}
where $A(\omega)=|\Gamma(\omega)|$ is called the \emph{amplitude} of the filter and $\Phi(\omega)$ is its \emph{phase}. \\

We deduce from \ref{aidehh} that
\(x_t\) is a periodic {eigensignal} of
the filter with eigenvalue \({\Gamma}(\omega)\): the output is again a complex trigonometric signal with the same frequency.
If the filter coefficients are real, then linearity of the filter implies that
real and imaginary parts
of $x_t$ are mapped onto real and imaginary parts of $y_{t}$:
\[F(x_t)=F(\Re(x_t)+i\Im(x_t))=F(\Re(x_t))+iF(\Im(x_t))=\Re(F(x_t))+i\Im(F(x_t))\]
where $F(\Re(x_t)), F(\Im(x_t)), \Re(F(x_t))$ and $\Im(F(x_t))$ are all real.
Therefore the cosine (real-part of the input) is mapped to
\begin{eqnarray}
\cos(t\omega)&\to& \Re(\exp(i\omega t){\Gamma}(\omega))\\
&=&A(\omega)\left[
\cos(t\omega)\cos(-{\Phi}(\omega))-\sin(t\omega)
\sin(-{\Phi}(\omega))\right]\nonumber\\
&=&A(\omega)\cos(t\omega-{\Phi}(\omega))\nonumber\\
&=&A(\omega) \cos(\omega(t-{\Phi}(\omega)/\omega)) \label{costocosphi}
\end{eqnarray}
The amplitude function \(A(\omega)\) can be interpreted as the weight (damping if \(A(\omega)<1\), amplification if
\(A(\omega)>1\)) attributed by the filter to a sinusoidal input signal
with frequency \(\omega\). The function
\begin{eqnarray}\label{tsfunc}
\phi(\omega):=-{\Phi}(\omega)/\omega
\end{eqnarray}
can be interpreted as the \emph{time shift} of the
filter in \(\omega\)\footnote{The singularity in $\omega=0$ is resolved by noting that
$\Phi(0)=0$ for filters satisfying $\Gamma(0)>0$.
As a result $\phi(0):=\dot{\Phi}(0)$, see section \ref{shift_zero}, below.}.
For the lowpass in section \ref{lowpass} We have $\Gamma(\omega)=0$ if $\omega>\textrm{cutoff}$ . Therefore, all high-frequency
components are `cut', as illustrated in fig.\ref{z_per_n_lpn}. Since the lowpass filter is symmetric, the imaginary part must vanish and
since $\Gamma(\omega)\geq 0$ the phase must vanish too (the phase of a positive real number is zero). Therefore the time-shift $\phi(\omega)$
must vanish too (in the passband). \\

\textbf{Remark}\\
The time-shift is defined by $\phi(\omega):=-{\Phi}(\omega)/\omega$. This sign is convention: a positive shift then amounts to a delay (lag)
whereas a negative shift means an anticipation (lead). We note that the transfer function could have been defined by
\[
\Gamma'(\omega):=\sum_{k=-\infty}^{\infty}{\gamma}\exp(ik\omega)
\]
which is just the complex conjugate of Our definition. In this case the phase (the angle of the complex transfer function) would
change sign and We would define the time-shift as $\phi(\omega):={\Phi}(\omega)/\omega$. We feel free to use either definition(s)
in Our R-code. However, time-shifts are always defined such that positive numbers correspond the lags/delays and negative numbers
reflect leads/anticipation.




\subsubsection{Transferfunction of MA- and Stable AR- and ARMA-Filters}

The transferfunction of the MA($L$)-filter
\[
{y}_{t}=\sum_{j=0}^{L-1}b_{j-h}x_{t+h-j}
\]
is
\[\Gamma(\omega)=\sum_{j=0}^{L-1}b_{j-h}\exp(-i(j-h)\omega)\]
The transfer function of the stable AR($L'$)-filter
\[
{y}_{t}=\sum_{k=1}^{L'}a_{k} y_{t-k}+b_{-h}x_{t+h}
\]
can be obtained by inversion of the stable AR-part. Specifically if
\[\left(1-\sum_{k=1}^{L'}a_{k}B^k\right)y_t=b_{-h}x_{t+h}\]
then
\[y_t=\frac{b_{-h}}{(1-\sum_{k=1}^{L'}a_{k}B^k)}x_{t+h}\]
and the transferfunction becomes
\[\Gamma(\omega)=\frac{b_{-h}\exp(ih\omega)}{1-\sum_{k=1}^{L'}a_{k}\exp(-ik\omega)}\]
As an example let
\[
{y}_{t}=a_1y_{t-1}+b_{-h}x_{t+h}=b_{-h}\sum_{k=0}^{\infty}a_1^k x_{t+h-k}
\]
Then
\[\Gamma(\omega)=b_{-h}\sum_{k=0}^{\infty}a_1^k \exp(-i(k-h)\omega)=\frac{b_{-h}\exp(ih\omega)}{1-a_1\exp(-i\omega)}\]
as claimed. \\
The stable ARMA$(L,L')$ filter
\[
{y}_{t}=\sum_{k=1}^{L'}a_{k} y_{t-k}+\sum_{j=0}^{L-1}b_{j-h}x_{t+h-j}
\]
can be inverted into an infinite MA. Specifically, if
\[
\left(1-\sum_{k=1}^{L'}a_{k}B^k\right)y_t=\left(\sum_{j=0}^{L}b_{j-h}B^j\right)x_{t+h}
\]
then
\[
y_t=\frac{\sum_{j=0}^{L}b_{j-h}B^j}{1-\sum_{k=1}^{L'}a_{k}B^k}x_{t+h}
\]
and thus the transferfunction becomes
\[\Gamma(\omega)=\frac{\exp(ih\omega)\sum_{j=0}^{L}b_{j-h}\exp(-ij\omega)}{1-\sum_{k=1}^{L'}a_{k}\exp(-ik\omega)}\]
As an example let
\[
{y}_{t}=a_1y_{t-1}+b_{-h}x_{t+h}+b_{1-h}x_{t+h-1}=b_{-h}\sum_{k=0}^{\infty}a_1^k x_{t+h-k}+b_{1-h}\sum_{k=0}^{\infty}a_1^{k+1} x_{t+h-k}
\]
Then
\begin{eqnarray*}
\Gamma(\omega)&=&b_{-h}\sum_{k=0}^{\infty}a_1^k \exp(i(h-k)\omega)+b_{1-h}\sum_{k=0}^{\infty}a_1^{k+1} \exp(i(h-k)\omega)\\
&=&\exp(ih\omega)\frac{b_{-h}+b_{1-h}\exp(-i\omega)}{1-a_1\exp(-i\omega)}
\end{eqnarray*}
as claimed. \\

We note that the time-domain backshift operator $B^k$ corresponds to the frequency-domain `operator' $\exp(-ik\omega)$ (and conversely):
a time-shift of $\phi(\omega)=k$ time units corresponds to a phase-angle $\Phi(\omega)=-\phi(\omega)\omega=-k\omega$ i.e. the frequency-domain
expression is rotated by an angle of $-k\omega$ or, alternatively, it is multiplied by $\exp(-ik\omega)$.




\subsubsection{The Time-Shift in Frequency $\textrm{Zero}^{*}$}\label{shift_zero}


The time-shift $\hat{\Phi}(\omega)/\omega$ is subject to a singularity in frequency zero: a zero-over-zero quotient.
However, one can apply
first order Taylor approximations to both terms of the quotient (l'H$\hat{\textrm{o}}$pital's rule) which leads to
\begin{eqnarray}
\hat{\phi}(0)&=&\lim_{\omega\to 0}\frac{\hat{\Phi}(\omega)}{\omega}\nonumber\\
&=&\frac{\left.\frac{d}{d\omega}\hat{\Phi}(\omega)\right |_{\omega=0}}{1}\nonumber\\
&=&\frac{\left.\frac{d}{d\omega}\hat{\Gamma}(\omega)\right|_{\omega=0}}{-i\hat{A}(0)}\nonumber\\
&=&\frac{\sum_{j=0}^{L-1}jb_j}{\sum_{j=0}^{L-1}b_j}\label{shift_zero_eq}
\end{eqnarray}
Second and third equalities are obtained by looking at
\begin{eqnarray*}
-i\sum_{j=0}^{L-1}jb_j&=&\left.\frac{d}{d\omega}\hat{\Gamma}(\omega)\right |_{\omega=0}\\
&=&\left.\frac{d}{d\omega}\hat{A}(\omega)\right |_{\omega=0}
\exp(-i\Phi(0))-i\hat{A}(0)\exp(-i\Phi(0))\left.\frac{d}{d\omega}\hat{\Phi}(\omega)\right |_{\omega=0}\\
&=&-i \hat{A}(0)\left.\frac{d}{d\omega}\hat{\Phi}(\omega)\right |_{\omega=0}
\end{eqnarray*}
The derivative of the amplitude vanishes in zero because the amplitude is a continuous even function
i.e. $\hat{A}(-\omega)=\hat{A}(\omega)$.



\subsubsection{Exercises}\label{trigo}

\begin{enumerate}
\item Consider the MA(1)-filter
\[y_t=b_0x_t+b_1x_{t-1}\]
and assume $x_t=cos(t\omega)$ is a trigonometric input signal with frequency $\omega$.
Compute, plot and compare in- and output signals for different $\omega$ for the two filters with coefficients $b_0=b_1=1$ and
$b_0=1, b_1=-1$, see fig.\ref{z_cos_in_out} (We used $\omega=\pi/20$ and $\omega=\pi/5$).
<<echo=True>>=
len<-100
b0<-1
file = paste("z_cos_in_out.pdf", sep = "")
pdf(file = paste(path.out,file,sep=""), paper = "special",
width = 6, height = 6)
par(mfrow=c(2,2))
b1<--1
omega1<-pi/20
y<-rep(0,len)
x<-cos((1:len)*omega1)
y[2:len]<-b0*x[2:len]+b1*x[1:(len-1)]
ts.plot(x,type="l",main=paste("MA(1): b1 = ",b1,
", omega=",round(omega1,3),sep=""),xlab="",
ylab="",col="blue",lwd=1)
lines(y,col="red",lwd=1)
mtext("Input", side = 3, line = -1,at=len/2,col="blue")
mtext("Output", side = 3, line = -2,at=len/2,col="red")
b1<-1
y[2:len]<-b0*x[2:len]+b1*x[1:(len-1)]
ts.plot(y,type="l",main=paste("MA(1): b1 = ",b1,
", omega=",round(omega1,3),sep=""),xlab="",
ylab="",col="red",lwd=1)
lines(x,col="blue",lwd=1)
mtext("Input", side = 3, line = -1,at=len/2,col="blue")
mtext("Output", side = 3, line = -2,at=len/2,col="red")
b1<--1
omega2<-pi/5
y<-rep(0,len)
x<-cos((1:len)*omega2)
y[2:len]<-b0*x[2:len]+b1*x[1:(len-1)]
ts.plot(x,type="l",main=paste("MA(1): b1 = ",b1,
", omega=",round(omega2,3),sep=""),xlab="",
ylab="",col="blue",lwd=1)
lines(y,col="red",lwd=1)
mtext("Input", side = 3, line = -1,at=len/2,col="blue")
mtext("Output", side = 3, line = -2,at=len/2,col="red")
b1<-1
y[2:len]<-b0*x[2:len]+b1*x[1:(len-1)]
ts.plot(y,type="l",main=paste("MA(1):  b1 = ",b1,
", omega=",round(omega2,3),sep=""),xlab="",
ylab="",col="red",lwd=1)
lines(x,col="blue",lwd=1)
mtext("Input", side = 3, line = -1,at=len/2,col="blue")
mtext("Output", side = 3, line = -2,at=len/2,col="red")
dev.off()
@
<<label=z_cos_in_out.pdf,echo=FALSE,results=tex>>=
  file = paste("z_cos_in_out.pdf", sep = "")
  cat("\\begin{figure}[H]")
  cat("\\begin{center}")
  cat("\\includegraphics[height=6in, width=6in]{", file, "}\n",sep = "")
  cat("\\caption{Filter effect of MA(1) on trigonometric input", sep = "")
  cat("\\label{z_cos_in_out}}", sep = "")
  cat("\\end{center}")
  cat("\\end{figure}")
@
\item Compute amplitude and time-shift functions of the two MA(1)-filters and verify the observed filter effects
in the above figure.
<<echo=True>>=
file = paste("z_amp_pha_ma1.pdf", sep = "")
pdf(file = paste(path.out,file,sep=""), paper = "special",
width = 6, height = 6)
par(mfrow=c(2,2))
# Resolution of discrete frequency grid
len1<-1001
omega_k<-(0:len1)*pi/len1
# Compute transfer function, amplitude, phase and time-shift
b1<-1
trffkt1<-1+b1*complex(arg=-omega_k)
amplitude1<-abs(trffkt1)
phase1<-Arg(trffkt1)
shift1<--phase1/omega_k
plot(amplitude1,type="l",main=paste("Amplitude MA(1): b1 = ",b1,sep=""),
axes=F,xlab="Frequency",ylab="Amplitude",col="black")
abline(v=(len1-1)/(pi/omega1),lty=2,col="blue")
abline(v=(len1-1)/(pi/omega2),lty=2,col="orange")
mtext(paste("pi/",pi/omega1,sep=""), side = 3, line = -4,
at=len1/20,col="blue")
mtext(paste("pi/",pi/omega2,sep=""), side = 3, line = -6,
at=len1/5,col="orange")
axis(1,at=c(0,1:6*len1/6+1),labels=c("0","pi/6","2pi/6",
"3pi/6","4pi/6","5pi/6","pi"))
axis(2)
box()
plot(shift1,type="l",main=paste("Time-shift MA(1): b1 = ",b1,sep=""),
axes=F,xlab="Frequency",ylab="Shift",col="green",
ylim=c(min(na.exclude(shift1))-0.5,max(na.exclude(shift1))+0.5))
abline(v=(len1-1)/(pi/omega1),lty=2,col="blue")
abline(v=(len1-1)/(pi/omega2),lty=2,col="orange")
mtext(paste("pi/",pi/omega1,sep=""), side = 3, line = -4,
at=len1/20,col="blue")
mtext(paste("pi/",pi/omega2,sep=""), side = 3, line = -6,
at=len1/5,col="orange")
axis(1,at=c(0,1:6*len1/6+1),labels=c("0","pi/6","2pi/6",
"3pi/6","4pi/6","5pi/6","pi"))
axis(2)
box()
b1<--1
trffkt2<-1+b1*complex(arg=-omega_k)
amplitude2<-abs(trffkt2)
phase2<-Arg(trffkt2)
shift2<--phase2/omega_k
plot(amplitude2,type="l",main=paste("Amplitude MA(1): b1 = ",b1,sep=""),
axes=F,xlab="Frequency",ylab="Amplitude",col="black")
abline(v=(len1-1)/(pi/omega1),lty=2,col="blue")
abline(v=(len1-1)/(pi/omega2),lty=2,col="orange")
mtext(paste("pi/",pi/omega1,sep=""), side = 3, line = -4,
at=len1/20,col="blue")
mtext(paste("pi/",pi/omega2,sep=""), side = 3, line = -6,
at=len1/5,col="orange")
axis(1,at=c(0,1:6*len1/6+1),labels=c("0","pi/6","2pi/6",
"3pi/6","4pi/6","5pi/6","pi"))
axis(2)
box()
plot(shift2,type="l",main=paste("Time-shift MA(1): b1 = ",b1,sep=""),
axes=F,xlab="Frequency",ylab="Shift",col="green")
abline(v=(len1-1)/(pi/omega1),lty=2,col="blue")
abline(v=(len1-1)/(pi/omega2),lty=2,col="orange")
mtext(paste("pi/",pi/omega1,sep=""), side = 3, line = -4,
at=len1/20,col="blue")
mtext(paste("pi/",pi/omega2,sep=""), side = 3, line = -6,
at=len1/5,col="orange")
axis(1,at=c(0,1:6*len1/6+1),labels=c("0","pi/6","2pi/6",
"3pi/6","4pi/6","5pi/6","pi"))
axis(2)
box()
dev.off()
# Compute table with shifts and amplitudes in omega=pi/20 and omega=pi/5
table_amp_shift<-
rbind(c(amplitude1[(len1-1)/(pi/omega1)+1],shift1[(len1-1)/(pi/omega1)+1]),
c(amplitude2[(len1-1)/(pi/omega1)+1],shift2[(len1-1)/(pi/omega1)+1]),
c(amplitude1[(len1-1)/(pi/omega2)+1],shift1[(len1-1)/(pi/omega2)+1]),
c(amplitude2[(len1-1)/(pi/omega2)+1],shift2[(len1-1)/(pi/omega2)+1]))
dimnames(table_amp_shift)[[2]]<-c("Amplitude","Shift")
dimnames(table_amp_shift)[[1]]<-c(paste("pi/",pi/omega1,",b1=1",sep=""),
paste("pi/",pi/omega1,",b1=-1",sep=""),
paste("pi/",pi/omega2,",b1=1",sep=""),paste("pi/",pi/omega2,",b1=-1",sep=""))
@
<<label=z_amp_pha_ma1.pdf,echo=FALSE,results=tex>>=
  file = paste("z_amp_pha_ma1.pdf", sep = "")
  cat("\\begin{figure}[H]")
  cat("\\begin{center}")
  cat("\\includegraphics[height=6in, width=6in]{", file, "}\n",sep = "")
  cat("\\caption{Amplitude and time-shifts of MA(1)-filters", sep = "")
  cat("\\label{z_amp_pha_ma1}}", sep = "")
  cat("\\end{center}")
  cat("\\end{figure}")
@
The amplitude and time shifts at frequencies $\pi/20$ and $\pi/5$ for the two MA-filters are summarized
in table \ref{table_amp_shift}.
<<label=table_amp_shift,echo=FALSE,results=tex>>=
  library(Hmisc)
  require(xtable)
  #latex(cor_vec, dec = 1, , caption = "Example of using latex to create table",
  #center = "centering", file = "", floating = FALSE)
  xtable(table_amp_shift, dec = 1,digits=rep(3,dim(table_amp_shift)[2]+1),
  paste("Amplitudes and time-shifts of MA(1)-filters in pi/20 and pi/5",sep=""),
  label=paste("table_amp_shift",sep=""),
  center = "centering", file = "", floating = FALSE)
@
These numbers coincide with the effects observed in fig.\ref{z_cos_in_out}. The difference filter
\[y_t=x_t-x_{t-1}\]
($b_1=-1$) is important in time series analysis because it is used to transform a non-stationary (trending/integrated) series
into a stationary constant-level series. The lower left panel in fig.\ref{z_amp_pha_ma1} illustrates that the amplitude
function of the difference filter vanishes in frequency zero: thus the filter can eliminate a (zero-frequency) trend in a
non-stationary series. But We see also that the filter amplifies high-frequency noise. Interestingly, the time-shift
of the filter is \emph{negative}\footnote{The divergence towards $-\infty$ in $\omega=0$ is due to $\phi(0)=-\lim_{\omega\to 0}\Phi(\omega)/\omega=-\pi/0$.}:
as can be seen in the left panels of fig.\ref{z_cos_in_out}, the output series (red)
is shifted to the right and turning-points of the input signal are
anticipated\footnote{The slope of a growing (smooth) time series must decrease before the series
can decline: first differences are anticipative. But not every turning-point of the slope
(of first differences) leads to a turning-point of the original series.}.
In contrast, the MA-filter with positive $b_1=1$ damps/eliminates the highest frequencies; its time-shift $\phi(\omega)=0.5$ is positive and
constant everywhere: turning-points are (slightly) delayed in the output series (red) of the right panels in fig.\ref{z_cos_in_out}.
The observed characteristics are typical for one-sided (causal) lowpass filters: the low-frequency components in
the passband tend to be delayed and hence turning-points are delayed\footnote{The positive time-shift has some deep repercussions:
as an example recession down-turns in macro-series cannot be detected timely and trading orders (buy/sells) cannot be executed
at the optimal turning-points of an asset series.}.

\item Shift and scale the input signal according to amplitude and time-shift functions
and verify that the resulting series coincides with the output signal, see fig.\ref{z_out_ssout}
(We compute results for the case $\omega=\pi/20$ and $b_1=1$ only). Hint: the resulting
amplitude and time-shifts in table \ref{table_amp_shift} are stored in the object
\emph{table\textunderscore amp\textunderscore shift}.
<<echo=True>>=
# We verify the claim for omega=pi/20 and b1=1
amp_scaling<-table_amp_shift[1,1]
shift<-table_amp_shift[1,2]
len<-100
b0<-1
b1<-1
omega1<-pi/20
y<-rep(0,len)
x<-cos((1:len)*omega1)
y[2:len]<-b0*x[2:len]+b1*x[1:(len-1)]
# Here We scale and shift the input signal
z<-amp_scaling*cos((1:len-shift)*omega1)
file = paste("z_out_ssout.pdf", sep = "")
pdf(file = paste(path.out,file,sep=""), paper = "special", width = 6, height = 6)
ts.plot(y,type="l",main=paste("Output vs. scaled and shifted input: b0 = b1 = ",
b1,", omega=",round(omega1,3),sep=""),xlab="",
ylab="",col="red",lwd=1)
lines(z,col="green",lwd=1)
mtext("Scaled and shifted input", side = 3, line = -1,at=len/2,col="green")
mtext("Output", side = 3, line = -2,at=len/2,col="red")
dev.off()
@
<<label=z_out_ssout.pdf,echo=FALSE,results=tex>>=
  file = paste("z_out_ssout.pdf", sep = "")
  cat("\\begin{figure}[H]")
  cat("\\begin{center}")
  cat("\\includegraphics[height=4in, width=4in]{", file, "}\n",sep = "")
  cat("\\caption{Output of MA(1) vs. scaled and shifted input", sep = "")
  cat("\\label{z_out_ssout}}", sep = "")
  cat("\\end{center}")
  cat("\\end{figure}")
@
Both series overlap perfectly except at the start where the output is (deliberately
wrongly) initialized with zero: so that We can see that both series are indeed computed
differently!

\item We now look at the AR(1)-filter
\[y_t=a_1y_{t-1}+b_0x_{t}\]
and assume, again, that $x_t=cos(t\omega)$ is a trigonometric input signal with frequency $\omega$.
Compute, plot and compare in- and output signals for different $\omega$ for the two filters with coefficients $b_0=1,a_1=0.9$ and
$b_0=0.1, a_1=0.9$, see fig.\ref{z_cos_in_out_ar1} (We used $\omega=\pi/20$ and $\omega=\pi/5$).
<<echo=True>>=
len<-1000
a1<-0.9
file = paste("z_cos_in_out_ar1.pdf", sep = "")
pdf(file = paste(path.out,file,sep=""), paper = "special", width = 6, height = 6)
len<-1000
par(mfrow=c(2,2))
b0<-1
omega1<-pi/20
y<-rep(0,len)
x<-cos((1:len)*omega1)
for (i in 2:len)
  y[i]<-b0*x[i]+a1*y[i-1]
ts.plot(y[(len-99):len],type="l",main=paste("AR(1): b0 = ",
b0, ", a1 = ",a1,", omega=",round(omega1,3),sep=""),xlab="",
ylab="",col="red",lwd=1)
lines(x[(len-99):len],col="blue",lwd=1)
mtext("Input", side = 3, line = -1,at=50,col="blue")
mtext("Output", side = 3, line = -2,at=50,col="red")
b0<-0.1
y<-rep(0,len)
for (i in 2:len)
  y[i]<-b0*x[i]+a1*y[i-1]
ts.plot(x[(len-99):len],type="l",main=paste("AR(1): b0 = ",
b0, ", a1 = ",a1,", omega=",round(omega1,3),sep=""),xlab="",
ylab="",col="blue",lwd=1)
lines(y[(len-99):len],col="red",lwd=1)
mtext("Input", side = 3, line = -1,at=50,col="blue")
mtext("Output", side = 3, line = -2,at=50,col="red")
b0<-1
omega2<-pi/5
y<-rep(0,len)
x<-cos((1:len)*omega2)
for (i in 2:len)
  y[i]<-b0*x[i]+a1*y[i-1]
ts.plot(y[(len-99):len],type="l",main=paste("AR(1): b0 = ",
b0, ", a1 = ",a1,", omega=",round(omega2,3),sep=""),xlab="",
ylab="",col="red",lwd=1)
lines(x[(len-99):len],col="blue",lwd=1)
mtext("Input", side = 3, line = -1,at=50,col="blue")
mtext("Output", side = 3, line = -2,at=50,col="red")
b0<-0.1
for (i in 2:len)
  y[i]<-b0*x[i]+a1*y[i-1]
ts.plot(x[(len-99):len],type="l",main=paste("AR(1): b0 = ",
b0, ", a1 = ",a1,", omega=",round(omega2,3),sep=""),xlab="",
ylab="",col="blue",lwd=1)
lines(y[(len-99):len],col="red",lwd=1)
mtext("Input", side = 3, line = -1,at=50,col="blue")
mtext("Output", side = 3, line = -2,at=50,col="red")
dev.off()
@
<<label=z_cos_in_out_ar1.pdf,echo=FALSE,results=tex>>=
  file = paste("z_cos_in_out_ar1.pdf", sep = "")
  cat("\\begin{figure}[H]")
  cat("\\begin{center}")
  cat("\\includegraphics[height=6in, width=6in]{", file, "}\n",sep = "")
  cat("\\caption{Filter effect of AR(1) on trigonometric input", sep = "")
  cat("\\label{z_cos_in_out_ar1}}", sep = "")
  cat("\\end{center}")
  cat("\\end{figure}")
@
The coefficient $b_0$ corresponds to a simple scaling of the filter-output.

\item Compute amplitude and time-shift functions of the two AR(1)-filters and verify the observed filter effects
in the above figure.
<<echo=True>>=
file = paste("z_amp_pha_ar1.pdf", sep = "")
pdf(file = paste(path.out,file,sep=""), paper = "special",
width = 6, height = 6)
par(mfrow=c(2,2))
# Resolution of discrete frequency grid
len1<-1001
omega_k<-(0:len1)*pi/len1
# Compute transfer function, amplitude, phase and time-shift
a1<-0.9
b0<-1
trffkt1<-b0/(1-a1*complex(arg=-omega_k))
amplitude1<-abs(trffkt1)
phase1<-Arg(trffkt1)
shift1<--phase1/omega_k
plot(amplitude1,type="l",
main=paste("Amplitude AR(1): b0 = ",b0," ,a1 = ",a1,sep=""),
axes=F,xlab="Frequency",ylab="Amplitude",col="black")
abline(v=(len1-1)/(pi/omega1),lty=2,col="blue")
abline(v=(len1-1)/(pi/omega2),lty=2,col="orange")
mtext(paste("pi/",pi/omega1,sep=""), side = 3,
line = -4,at=len1/20,col="blue")
mtext(paste("pi/",pi/omega2,sep=""), side = 3,
line = -6,at=len1/5,col="orange")
axis(1,at=c(0,1:6*len1/6+1),labels=c("0","pi/6","2pi/6",
"3pi/6","4pi/6","5pi/6","pi"))
axis(2)
box()
plot(shift1,type="l",
main=paste("Time-shift AR(1): b0 = ",b0," ,a1 = ",a1,sep=""),
axes=F,xlab="Frequency",ylab="Shift",col="green")
abline(v=(len1-1)/(pi/omega1),lty=2,col="blue")
abline(v=(len1-1)/(pi/omega2),lty=2,col="orange")
mtext(paste("pi/",pi/omega1,sep=""), side = 3,
line = -4,at=len1/20,col="blue")
mtext(paste("pi/",pi/omega2,sep=""), side = 3,
line = -6,at=len1/5,col="orange")
axis(1,at=c(0,1:6*len1/6+1),labels=c("0","pi/6","2pi/6",
"3pi/6","4pi/6","5pi/6","pi"))
axis(2)
box()
b0<-0.1
trffkt2<-b0/(1-a1*complex(arg=-omega_k))
amplitude2<-abs(trffkt2)
phase2<-Arg(trffkt2)
shift2<--phase2/omega_k
plot(amplitude2,type="l",
main=paste("Amplitude AR(1): b0 = ",b0," ,a1 = ",a1,sep=""),
axes=F,xlab="Frequency",ylab="Amplitude",col="black")
abline(v=(len1-1)/(pi/omega1),lty=2,col="blue")
abline(v=(len1-1)/(pi/omega2),lty=2,col="orange")
mtext(paste("pi/",pi/omega1,sep=""), side = 3,
line = -4,at=len1/20,col="blue")
mtext(paste("pi/",pi/omega2,sep=""), side = 3,
line = -6,at=len1/5,col="orange")
axis(1,at=c(0,1:6*len1/6+1),labels=c("0","pi/6","2pi/6",
"3pi/6","4pi/6","5pi/6","pi"))
axis(2)
box()
plot(shift2,type="l",
main=paste("Time-shift AR(1): b0 = ",b0," ,a1 = ",a1,sep=""),
axes=F,xlab="Frequency",ylab="Shift",col="green")
abline(v=(len1-1)/(pi/omega1),lty=2,col="blue")
abline(v=(len1-1)/(pi/omega2),lty=2,col="orange")
mtext(paste("pi/",pi/omega1,sep=""), side = 3,
line = -4,at=len1/20,col="blue")
mtext(paste("pi/",pi/omega2,sep=""), side = 3,
line = -6,at=len1/5,col="orange")
axis(1,at=c(0,1:6*len1/6+1),labels=c("0","pi/6","2pi/6",
"3pi/6","4pi/6","5pi/6","pi"))
axis(2)
box()
dev.off()
# Compute table with shifts and amplitudes in omega=pi/20 and omega=pi/5
table_amp_shift_ar1<-rbind(c(amplitude1[(len1-1)/(pi/omega1)+1],
shift1[(len1-1)/(pi/omega1)+1]),
c(amplitude2[(len1-1)/(pi/omega1)+1],shift2[(len1-1)/(pi/omega1)+1]),
c(amplitude1[(len1-1)/(pi/omega2)+1],shift1[(len1-1)/(pi/omega2)+1]),
c(amplitude2[(len1-1)/(pi/omega2)+1],shift2[(len1-1)/(pi/omega2)+1]))
dimnames(table_amp_shift_ar1)[[2]]<-c("Amplitude","Shift")
dimnames(table_amp_shift_ar1)[[1]]<-c(paste("pi/",pi/omega1,",b0=1",sep=""),
paste("pi/",pi/omega1,",b0=0.1",sep=""),
paste("pi/",pi/omega2,",b0=1",sep=""),paste("pi/",pi/omega2,",b0=0.1",sep=""))
@
<<label=z_amp_pha_ar1.pdf,echo=FALSE,results=tex>>=
  file = paste("z_amp_pha_ar1.pdf", sep = "")
  cat("\\begin{figure}[H]")
  cat("\\begin{center}")
  cat("\\includegraphics[height=6in, width=6in]{", file, "}\n",sep = "")
  cat("\\caption{Amplitude and time-shifts of AR(1)-filters", sep = "")
  cat("\\label{z_amp_pha_ar1}}", sep = "")
  cat("\\end{center}")
  cat("\\end{figure}")
@
The amplitude and time shifts at frequencies $\pi/20$ and $\pi/5$ for the two MA-filters are summarized
in table \ref{table_amp_shift_ar1}.
<<label=table_amp_shift_ar1,echo=FALSE,results=tex>>=
  library(Hmisc)
  require(xtable)
  #latex(cor_vec, dec = 1, , caption = "Example of using latex to create table",
  #center = "centering", file = "", floating = FALSE)
  xtable(table_amp_shift_ar1, dec = 1,digits=rep(3,dim(table_amp_shift)[2]+1),
  paste("Amplitudes and time-shifts of AR(1)-filters in pi/20 and pi/5",sep=""),
  label=paste("table_amp_shift_ar1",sep=""),
  center = "centering", file = "", floating = FALSE)
@
These numbers coincide with the effects observed in fig.\ref{z_cos_in_out_ar1}. The AR(1)-filter
\[y_t=a_1y_{t-1}+b_0x_{t}\]
is important in time series analysis because of its strong smoothing effect: the amplitude functions in the left panels
of fig.\ref{z_amp_pha_ar1} suggest that the filter with $a_1=0.9$ damps high-frequency components
(relative to low-frequency components) very heavily. The AR(1) with positive $a_1>0$ is a typical example of a lowpass
filter. As such the time-shift is positive: the stronger the smoothing (the larger $a_1<1$) the larger the time-shift in the
passband. The strength of both effects, the smoothing as well as the shift, is a consequence of the (more or less)
slowly decaying filter weights in the MA$(\infty)$ respresentation
\[y_t=\sum_{k=0}^{\infty}a_1^kx_{t-k}\]
If $a_1$ is close to one, then the effective average extends over many lagged $x_{t-k}$'s and therefore smoothing is strong but, unfortunately,
the time-shift will be strong too.
For Our example with $a_1=0.9$, turning-points will be substantially delayed. We note that the effect of $b_0$ is a simple
scaling of the output. For $b_0=0.1$ We have $A(0)=b_0/(1-a_1)=1$ which is often a desirable property of filters: it means that
the important trend component in frequency zero is not altered in scale. The normalization
by $b_0$ has no effect on the time-shift,
at least as long as $b_0>0$.

\item Scale and shift the input signal according to amplitude and time-shift functions and
compare the resulting series with the filter output, see fig.\ref{z_out_ssout_ar1}
(We compute results for the case $\omega=\pi/20$ and $b_0=1$ only). Hint: the resulting
amplitude and time-shifts in table \ref{table_amp_shift_ar1} are stored in the object
\emph{table\textunderscore amp\textunderscore shift\textunderscore ar1}.
<<echo=True>>=
# We verify the claim for omega=pi/20 and b1=1
amp_scaling<-table_amp_shift_ar1[1,1]
shift<-table_amp_shift_ar1[1,2]
len<-200
b0<-1
omega1<-pi/20
y<-rep(0,len)
x<-cos((1:len)*omega1)
for (i in 2:len)
  y[i]<-b0*x[i]+a1*y[i-1]
# Here We scale and shift the input signal
z<-amp_scaling*cos((1:len-shift)*omega1)
file = paste("z_out_ssout_ar1.pdf", sep = "")
pdf(file = paste(path.out,file,sep=""), paper = "special", width = 6, height = 6)
ts.plot(y,type="l",main=paste("Output vs. scaled and shifted input: b0 = ",b0,
", a1 = ", a1,", omega=",round(omega1,3),sep=""),xlab="",
ylab="",col="red",lwd=1)
lines(z,col="green",lwd=1)
mtext("Scaled and shifted input", side = 3, line = -1,at=len/2,col="green")
mtext("Output", side = 3, line = -2,at=len/2,col="red")
dev.off()
@
<<label=z_out_ssout_ar1.pdf,echo=FALSE,results=tex>>=
  file = paste("z_out_ssout_ar1.pdf", sep = "")
  cat("\\begin{figure}[H]")
  cat("\\begin{center}")
  cat("\\includegraphics[height=4in, width=4in]{", file, "}\n",sep = "")
  cat("\\caption{Output of AR(1) vs. scaled and shifted input", sep = "")
  cat("\\label{z_out_ssout_ar1}}", sep = "")
  cat("\\end{center}")
  cat("\\end{figure}")
@
Both series differ at the start and the output series (red) needs some time to
stabilize because it is deliberately
wrongly initialized in zero\footnote{The filter coefficients $b_0a_1^k$ of the
corresponding MA($\infty$)-filter decay slowly towards zero.}. But We can observe that the output converges
to the scaled and shifted input (green) `asymptotically': both series are virtually
indistinguishable for $t>50$.

\item \label{seasonal diff}To conclude We analyze the effects of the classical MA(12) seasonal difference filter
\[y_t=x_t-x_{t-12}\]
with coefficients $b_0=1,b_{12}=-1$ and $b_k=0,k\not= 0,12$. verify that the filter
eliminates trigonometric signals with frequencies $k\pi/6$, $k=0,1,...,6$, multiples of
$\pi/6$.
<<echo=True>>=
len<-120
b0<-1
b12<--1
file = paste("z_cos_in_out_sd.pdf", sep = "")
pdf(file = paste(path.out,file,sep=""), paper = "special", width = 6, height = 6)
par(mfrow=c(2,2))
omega1<-pi/20
y<-rep(0,len)
x<-cos((1:len)*omega1)
y[13:len]<-b0*x[13:len]+b12*x[1:(len-12)]
ts.plot(y,type="l",main=paste("Seasonal difference MA(12): b0 =1, b12 = ",
b12,", omega=",round(omega1,3),sep=""),xlab="",
ylab="",col="red",lwd=1)
lines(x,col="blue",lwd=1)
mtext("Input", side = 3, line = -1,at=len/2,col="blue")
mtext("Output", side = 3, line = -2,at=len/2,col="red")
omega2<-pi/5
y<-rep(0,len)
x<-cos((1:len)*omega2)
y[13:len]<-b0*x[13:len]+b12*x[1:(len-12)]
ts.plot(y,type="l",main=paste("Seasonal difference MA(12): b0 =1, b12 = ",
b12,", omega=",round(omega1,3),sep=""),xlab="",
ylab="",col="red",lwd=1)
lines(x,col="blue",lwd=1)
mtext("Input", side = 3, line = -1,at=len/2,col="blue")
mtext("Output", side = 3, line = -2,at=len/2,col="red")
omega3<-pi/6
y<-rep(0,len)
x<-cos((1:len)*omega3)
y[13:len]<-b0*x[13:len]+b12*x[1:(len-12)]
ts.plot(x,type="l",main=paste("Seasonal difference MA(12): b0 =1, b12 = ",
b12,", omega=",round(omega1,3),sep=""),xlab="",
ylab="",col="blue",lwd=1)
lines(y,col="red",lwd=1)
mtext("Input", side = 3, line = -1,at=len/2,col="blue")
mtext("Output", side = 3, line = -2,at=len/2,col="red")
omega4<-4*pi/6
y<-rep(0,len)
x<-cos((1:len)*omega4)
y[13:len]<-b0*x[13:len]+b12*x[1:(len-12)]
ts.plot(x,type="l",main=paste("Seas. diff. MA(12), omega=",
round(omega1,3),sep=""),xlab="",
ylab="",col="blue",lwd=1)
lines(y,col="red",lwd=1)
mtext("Input", side = 3, line = -1,at=len/2,col="blue")
mtext("Output", side = 3, line = -2,at=len/2,col="red")
dev.off()
@
<<label=z_cos_in_out_sd.pdf,echo=FALSE,results=tex>>=
  file = paste("z_cos_in_out_sd.pdf", sep = "")
  cat("\\begin{figure}[H]")
  cat("\\begin{center}")
  cat("\\includegraphics[height=6in, width=6in]{", file, "}\n",sep = "")
  cat("\\caption{Filter effect of seasonal difference MA(12) on trigonometric inputs", sep = "")
  cat("\\label{z_cos_in_out_sd}}", sep = "")
  cat("\\end{center}")
  cat("\\end{figure}")
@
The first two signals with frequencies $\pi/20$ (upper left panel) and $\pi/5$ (upper
right panel) are amplified by the filter. The signals with frequencies $\pi/6$ (lower left
panel) and $4\pi/6$ (lower right panel) are eliminated.

\item Compute amplitude and time-shift functions of the filter.
<<echo=True>>=
file = paste("z_amp_pha_ma12.pdf", sep = "")
pdf(file = paste(path.out,file,sep=""), paper = "special", width = 6, height = 6)
# Resolution of discrete frequency grid
len1<-1200
omega_k<-(0:len1)*pi/len1
# Compute transfer function, amplitude, phase and time-shift
b1<-1
b12<--1
trffkt<-1+b12*complex(arg=-12*omega_k)
amplitude<-abs(trffkt)
phase<-Arg(trffkt)
shift<--phase/omega_k
par(mfrow=c(2,1))
plot(amplitude,type="l",main="Amplitude seasonal difference MA(12)",
axes=F,xlab="Frequency",ylab="Amplitude",col="black")
abline(v=(len1-1)/(pi/omega1),lty=2,col="blue")
abline(v=(len1-1)/(pi/omega2),lty=2,col="orange")
mtext(paste("pi/",pi/omega1,sep=""), side = 3, line = -2,
at=len1/20,col="blue")
mtext(paste("pi/",pi/omega2,sep=""), side = 3, line = -4,
at=len1/5,col="orange")
axis(1,at=c(0,1:6*len1/6+1),labels=c("0","pi/6","2pi/6","3pi/6",
"4pi/6","5pi/6","pi"))
axis(2)
box()
plot(shift,type="l",main="Time shift seasonal difference MA(12)",
axes=F,xlab="Frequency",ylab="Amplitude",col="black")
abline(v=(len1-1)/(pi/omega1),lty=2,col="blue")
abline(v=(len1-1)/(pi/omega2),lty=2,col="orange")
mtext(paste("pi/",pi/omega1,sep=""), side = 3, line = -2,
at=len1/20,col="blue")
mtext(paste("pi/",pi/omega2,sep=""), side = 3, line = -4,
at=len1/5,col="orange")
axis(1,at=c(0,1:6*len1/6+1),labels=c("0","pi/6","2pi/6","3pi/6",
"4pi/6","5pi/6","pi"))
axis(2)
box()
dev.off()
@
<<label=z_amp_pha_ma12.pdf,echo=FALSE,results=tex>>=
  file = paste("z_amp_pha_ma12.pdf", sep = "")
  cat("\\begin{figure}[H]")
  cat("\\begin{center}")
  cat("\\includegraphics[height=6in, width=6in]{", file, "}\n",sep = "")
  cat("\\caption{Amplitude and time-shifts of seasonal difference MA(12)-filter", sep = "")
  cat("\\label{z_amp_pha_ma12}}", sep = "")
  cat("\\end{center}")
  cat("\\end{figure}")
@
The amplitude function of the seasonal difference filter vanishes in multiples $j\pi/6$,
$j=1,...,6$ of the seasonal fundamental $\pi/6$. Note, however, that the trend in frequency
zero is eliminated too (which is undesirable) and that the amplitude exceeds one in frequency-bands
between the seasonal harmonics. In particular, the amplitude is larger than one in
$\pi/20$ and $\pi/5$ which explains that the outputs (red) in the upper panels of fig.\ref{z_cos_in_out_sd}
are magnified. The amplitude function suggests heavy distortions by this filter.
\end{enumerate}



\subsection{Finite Sample Discrete Convolution Theorem and Serial Linkage of Filters}

\subsubsection{Stationary Processes}


The transferfunction or, alternatively, the amplitude and the phase (or time-shift) functions,
describe comprehensively  the effect of a filter as
applied to an elementary (periodic and deterministic) trigonometric signal $x_t=\exp(it\omega)$:
\[
y_t=\sum_{j=-\infty}^{\infty}\gamma_jx_{t-j}={\Gamma}(\omega)x_t
\]
An arbitrary sequence $x_1,...,x_T$, neither periodic nor deterministic, can be decomposed into a weighted sum of trigonometric  sinusoids
\begin{equation}\label{dft_r}
x_t=\frac{\sqrt{2\pi}}{\sqrt{ T}}\sum_{k=-[T/2]}^{[T/2]} w_k\Xi_{TX}(\omega_k)\exp(it\omega_k )
\end{equation}
and similarly for $y_t$
\begin{equation}\label{dft_ry}
y_t=\frac{\sqrt{2\pi}}{\sqrt{ T}}\sum_{k=-[T/2]}^{[T/2]} w_k\Xi_{TY}(\omega_k)\exp(it\omega_k )
\end{equation}
recall \ref{idft}. Therefore, when applying the filter to
a general sequence $x_1,...,x_T$ We might proceed as follows
\begin{eqnarray}
y_t&=&\sum_{j=-\infty}^{\infty}\gamma_jx_{t-j}\nonumber\\
&\approx&\sum_{j=-\infty}^{\infty}\gamma_j\left(\frac{\sqrt{2\pi}}{\sqrt{ T}}\sum_{k=-[T/2]}^{[T/2]} w_k\Xi_{TX}(\omega_k)\exp(i(t-j)\omega_k )\right)\label{conv_app}\\
&=&\frac{\sqrt{2\pi}}{\sqrt{ T}}\sum_{k=-[T/2]}^{[T/2]} w_k\Xi_{TX}(\omega_k)\left(\sum_{j=-\infty}^{\infty}\gamma_j\exp(i(t-j)\omega_k )\right)\nonumber\\
&=&\frac{\sqrt{2\pi}}{\sqrt{ T}}\sum_{k=-[T/2]}^{[T/2]} w_k\Xi_{TX}(\omega_k)\Gamma(\omega_k)\exp(it\omega_k )\label{conv_o}
\end{eqnarray}
Comparing \ref{dft_ry} and \ref{conv_o} suggests that the DFT $\Xi_{TY}(\omega_k)$ of the output signal is linked to the DFT $\Xi_{TX}(\omega_k)$
of the input signal via
\begin{equation}\label{convolution_dft}
\Xi_{TY}(\omega)\approx\Gamma(\omega)\Xi_{TX}(\omega)
\end{equation}
This result is not a strict equality (because \ref{conv_app} is an approximation) but from a practical perspective We can
ignore the error which is small if some elementary precaution-principles apply\footnote{The problem is that the DFT of $x_{t-j}$ on the left hand-side
is $\frac{1}{\sqrt{2\pi T}}\sum_{t=1}^Tx_{t-j}\exp(i-t\omega)$
which is not strictly the same as $\Xi_{TX}(\omega):=\frac{1}{\sqrt{2\pi T}}\sum_{t=1}^Tx_t\exp(i-t\omega)$ used on the right-hand side
because the data is shifted by $j$ time-units. But one can show that the approximation error in \ref{conv_app}
is small, at least if the data
is stationary}.\\

Equation \ref{convolution_dft} is of crucial importance because it describes the filter effect as applied to a non-periodic/non deterministic
sequence of numbers $x_1,...,x_T$. So for example the periodograms of output and input are linked via
\begin{equation}\label{convolution_per}
I_{TY}(\omega)=\left|\Xi_{TY}(\omega)\right|^2\approx\left|\Gamma(\omega)\right|^2I_{TX}(\omega)
\end{equation}
This equation is a finite sample discrete analogon of the so-called \emph{spectral convolution theorem} which states that the
spectral densities $h_y(\omega),h_x(\omega)$
of (stationary\footnote{The autocovariances should be absolutely summable which can
be assumed in typical economic applications (at least for stationary data).})
input and output signals are linked by
\begin{equation}\label{spectral_conv}
h_y(\omega)=\left|\Gamma(\omega)\right|^2h_x(\omega)
\end{equation}
Recall that We here assume a stationary framework. For non-stationary integrated series one can derive similar (discrete finite sample)
results based on the pseudo-DFT
or the pseudo-periodogram, to be defined later.\\


Assume now that We apply two filters $\gamma_{j1}$ and $\gamma_{j2}$, $j=-\infty,...,0,...\infty$, with transferfuctions $\Gamma_1(\omega)$
and $\Gamma_2(\omega)$, in series:
\begin{eqnarray*}
y_t&=&\sum_{j=-\infty}^{\infty}\gamma_{j1}x_{t-j}\\
z_t&=&\sum_{j=-\infty}^{\infty}\gamma_{j2}y_{t-j}
\end{eqnarray*}
where $z_t$ is the output of the series linkage. To see what happens We can rely on
\begin{eqnarray}
z_t&=&\sum_{j=-\infty}^{\infty}\gamma_{j2}y_{t-j}\nonumber\\
&\approx&\frac{\sqrt{2\pi}}{\sqrt{ T}}\sum_{k=-[T/2]}^{[T/2]} w_k\Xi_{TY}(\omega_k)\Gamma_2(\omega_k)\exp(it\omega_k )\label{convoloi}\\
&\approx&\frac{\sqrt{2\pi}}{\sqrt{ T}}\sum_{k=-[T/2]}^{[T/2]} w_k\Xi_{TX}(\omega_k)\Gamma_1(\omega_k)\Gamma_2(\omega_k)\exp(it\omega_k )\label{convolo}
\end{eqnarray}
where We inserted \ref{convolution_dft} into \ref{convoloi} to obtain \ref{convolo}. We infer that the DFT $\Xi_{TZ}(\omega_k)$ of the output signal is linked to the
DFT $\Xi_{TX}(\omega_k)$ of the input signal via
\begin{equation}\label{linkage_dft}
\Xi_{TZ}(\omega)\approx\Gamma_1(\omega)\Gamma_2(\omega)\Xi_{TX}(\omega)
\end{equation}
Amplitude- and phase-functions of the convolution are obtained from
\begin{eqnarray*}
\Gamma_1(\omega)\Gamma_2(\omega)&=&|\Gamma_1(\omega)||\Gamma_1(\omega)|\exp(-i\Phi_1(\omega))\exp(-i\Phi_2(\omega))\\
&=&A_1(\omega)A_2(\omega)\exp(-i(\Phi_1(\omega)+\Phi_2(\omega)))
\end{eqnarray*}
i.e. the amplitude $A_{\Gamma_1\Gamma_2}(\omega)$ of the convolution is the product of the
individual amplitude functions and the phase $\Phi_{\Gamma_1\Gamma_2}(\omega)$ is the sum of the individual
phase functions:
\begin{eqnarray*}
A_{\Gamma_1\Gamma_2}(\omega)&=&A_1(\omega)A_2(\omega)\\
\Phi_{\Gamma_1\Gamma_2}(\omega)&=&\Phi_1(\omega)+\Phi_2(\omega)
\end{eqnarray*}
The periodogram of the output signal becomes
\begin{equation}\label{linkage_per}
I_{TZ}(\omega)=\left|\Xi_{TZ}(\omega)\right|^2\approx\left|\Gamma_1(\omega)\right|^2\left|\Gamma_2(\omega)\right|^2I_{TX}(\omega)
\end{equation}
accordingly. Once again, the approximation error is generally small (at least for stationary data)
which means that We can often ignore its effect in practice. Putting
several (more than two) filters in series just prolonges the multiplicative chain in the above expressions.










\subsubsection{Exercises}

We here analyze the effects of classical filters on particular processes and we
check pertinence of the discrete finite sample convolution theorem for the DFT and the
Periodogram. In contrast to the previous exercises We here consider `complex' stochastic input series (not deterministic
trigonometric signals).

\begin{enumerate}

\item We here analyze the effect of the ordinary difference filter
\[y_t=x_t-x_{t-1}\]
where $b_0=1,b_1=-1$, on white noise $x_t=\epsilon_t$ and verify the quality of the approximations \ref{convolution_dft}
and \ref{convolution_per}. Generate a realization of length $T=100$ of $\epsilon_t$ and compare graphically
\begin{itemize}
\item The periodogram $I_{TX}(\omega_k)$ of the input signal (white noise)
\item the periodogram $I_{TY}(\omega_k)$ of the output signal
\item the convolution $|\Gamma(\omega_k)|^2I_{TX}(\omega_k)$ of input signal and filter.
\end{itemize}
see fig.\ref{z_convo_diff}.
<<echo=True>>=
set.seed(10)
len<-100
x<-rnorm(len)
y<-c(0,diff(x))
# Resolution of discrete frequency grid
len1<-len/2
omega_k<-(0:len1)*pi/len1
# Compute transfer function, amplitude, phase and time-shift
b1<--1
trffkt<-1+b1*complex(arg=-omega_k)
amplitude<-abs(trffkt)
phase<-Arg(trffkt)
shift<-phase/omega_k
plot_T<-F

file = paste("z_convo_diff.pdf", sep = "")
pdf(file = paste(path.out,file,sep=""), paper = "special", width = 6, height = 6)
par(mfrow=c(2,2))
plot(amplitude,type="l",main="Convolution: ordinary difference filter",
axes=F,xlab="Frequency",ylab="periodograms",col="black",ylim=c(0,2))
lines(amplitude^2*per(x,plot_T)$per,col="green")
lines(per(y,plot_T)$per,col="red",lty=2)
lines(per(x,plot_T)$per,col="blue")
mtext("Amplitude", side = 3, line = -1,at=len1/2,col="black")
mtext("Convolution", side = 3, line = -2,at=len1/2,col="green")
mtext("Periodogram output", side = 3, line = -3,at=len1/2,col="red")
mtext("Periodogram input", side = 3, line = -4,at=len1/2,col="blue")
axis(1,at=c(0,1:6*len1/6+1),labels=c("0","pi/6","2pi/6","3pi/6",
"4pi/6","5pi/6","pi"))
axis(2)
box()
ts.plot(x,col="blue",main="Input and output series",ylab="")
lines(y,col="red")
mtext("Input series", side = 3, line = -1,at=len/2,col="blue")
mtext("Output series", side = 3, line = -2,at=len/2,col="red")
plot(Re(trffkt),type="l",main="Convolution: real-parts of DFT",
axes=F,xlab="Frequency",ylab="Real parts of DFT",col="black",ylim=c(-1,2))
lines(Re(trffkt*per(x,plot_T)$DFT),col="green")
lines(Re(per(y,plot_T)$DFT),col="red",lty=2)
lines(Re(per(x,plot_T)$DFT),col="blue")
mtext("Re(transfer function)", side = 3, line = -1,at=len1/2,col="black")
mtext("Re(convolution of DFT)", side = 3, line = -2,at=len1/2,col="green")
mtext("Re(DFT output)", side = 3, line = -3,at=len1/2,col="red")
mtext("Re(DFT input)", side = 3, line = -4,at=len1/2,col="blue")
axis(1,at=c(0,1:6*len1/6+1),labels=c("0","pi/6","2pi/6","3pi/6",
"4pi/6","5pi/6","pi"))
axis(2)
box()
plot(Im(trffkt),type="l",main="Convolution: imaginary-parts of DFT",
axes=F,xlab="Frequency",ylab="Imaginary parts of DFT",col="black",ylim=c(-1,1))
lines(Im(trffkt*per(x,plot_T)$DFT),col="green")
lines(Im(per(y,plot_T)$DFT),col="red",lty=2)
lines(Im(per(x,plot_T)$DFT),col="blue")
mtext("Im(transfer function)", side = 3, line = -1,at=len1/2,col="black")
mtext("Im(convolution of DFT)", side = 3, line = -2,at=len1/2,col="green")
mtext("Im(DFT output)", side = 3, line = -3,at=len1/2,col="red")
mtext("Im(DFT input)", side = 3, line = -4,at=len1/2,col="blue")
axis(1,at=c(0,1:6*len1/6+1),labels=c("0","pi/6","2pi/6","3pi/6",
"4pi/6","5pi/6","pi"))
axis(2)
box()
dev.off()
@
<<label=z_convo_diff.pdf,echo=FALSE,results=tex>>=
  file = paste("z_convo_diff.pdf", sep = "")
  cat("\\begin{figure}[H]")
  cat("\\begin{center}")
  cat("\\includegraphics[height=6in, width=6in]{", file, "}\n",sep = "")
  cat("\\caption{Convolution of difference filter (green), periodogram of input (blue) and output (red)
  signals and amplitude function of ordinary differences (black)", sep = "")
  cat("\\label{z_convo_diff}}", sep = "")
  cat("\\end{center}")
  cat("\\end{figure}")
@
We observe that the convolution $|\Gamma(\omega_k)|^2I_{TX}(\omega_k)$ (green) and the periodogram
of the output series $I_{TY}(\omega_k)$ (red) in the top-left panel are almost indistinguishable: the convolution error
in the discrete spectral convolution \ref{convolution_per} is very small (negligible by all practical means). As expected, low-frequency components
are damped and high-frequency components are magnified and the output series (red line in top right panel) is `more volatile'.
The bottom panels confirm that the convolution of the DFT \ref{convolution_dft} applies for real and imaginary parts.
Note that these results are much more general than the exercises in
section \ref{trigo} because the new input series $x_t=\epsilon_t$ is not a deterministic trigonometric
line spectrum: all frequencies are `excited' and contribute equally to the process (to its variance).

\item We now analyze the AR(1)-filter
\[y_t=0.95y_{t-1}+0.05x_{t}\]
with $a_1=0.95, b_0=1-a_1=0.05$\footnote{The amplitude in frequency zero is $A(0)=|b_0/(1-a_1)|$
so that $A(0)=1$ if $b_0=1-a_1$.} and $x_t=\epsilon_t$ is again a white noise sequence. Compute
and compare periodograms of input and output signals as well as the convolution, see fig.\ref{z_convo_ar1}.
<<echo=True>>=
set.seed(10)
len2<-1000
a1<-0.95
b0<-1-a1
omega1<-pi/20
y1<-rep(0,len2)
x1<-rnorm(len2)
for (i in 2:len2)
  y1[i]<-b0*x1[i]+a1*y1[i-1]
# The first 900 observations are discarded (initialization)
len<-100
y<-y1[(len2-len+1):len2]
x<-x1[(len2-len+1):len2]
# Resolution of discrete frequency grid
len1<-len/2
omega_k<-(0:len1)*pi/len1
# Compute transfer function, amplitude, phase and time-shift
trffkt<-b0/(1-a1*complex(arg=-omega_k))
amplitude<-abs(trffkt)
phase<-Arg(trffkt)
shift<-phase/omega_k
plot_T<-F

file = paste("z_convo_ar1.pdf", sep = "")
pdf(file = paste(path.out,file,sep=""), paper = "special",
width = 6, height = 6)
par(mfrow=c(2,2))
plot(amplitude,type="l",main=paste("Convolution AR(1)-filter: b0 = ",
b0," a1 = ",a1,sep=""),
axes=F,xlab="Frequency",ylab="periodograms",col="black",ylim=c(0,1))
lines(amplitude^2*per(x,plot_T)$per,col="green")
lines(per(y,plot_T)$per,col="red",lty=2)
lines(per(x,plot_T)$per,col="blue")
mtext("Amplitude", side = 3, line = -1,at=len1/2,col="black")
mtext("Convolution", side = 3, line = -2,at=len1/2,col="green")
mtext("Periodogram output", side = 3, line = -3,at=len1/2,col="red")
mtext("Periodogram input", side = 3, line = -4,at=len1/2,col="blue")
axis(1,at=c(0,1:6*len1/6+1),labels=c("0","pi/6","2pi/6","3pi/6",
"4pi/6","5pi/6","pi"))
axis(2)
box()
ts.plot(x,col="blue",main="Input and output series",ylab="")
lines(y,col="red")
mtext("Input series", side = 3, line = -1,at=len/2,col="blue")
mtext("Output series", side = 3, line = -2,at=len/2,col="red")
plot(Re(trffkt),type="l",main="Convolution: real-parts of DFT",
axes=F,xlab="Frequency",ylab="Real parts of DFT",col="black",ylim=c(-1,1))
lines(Re(trffkt*per(x,plot_T)$DFT),col="green")
lines(Re(per(y,plot_T)$DFT),col="red",lty=2)
lines(Re(per(x,plot_T)$DFT),col="blue")
mtext("Re(transfer function)", side = 3, line = -1,at=len1/2,col="black")
mtext("Re(convolution of DFT)", side = 3, line = -2,at=len1/2,col="green")
mtext("Re(DFT output)", side = 3, line = -3,at=len1/2,col="red")
mtext("Re(DFT input)", side = 3, line = -4,at=len1/2,col="blue")
axis(1,at=c(0,1:6*len1/6+1),labels=c("0","pi/6","2pi/6","3pi/6",
"4pi/6","5pi/6","pi"))
axis(2)
box()
plot(Im(trffkt),type="l",main="Convolution: imaginary-parts of DFT",
axes=F,xlab="Frequency",ylab="Imaginary parts of DFT",col="black",ylim=c(-1,1))
lines(Im(trffkt*per(x,plot_T)$DFT),col="green")
lines(Im(per(y,plot_T)$DFT),col="red",lty=2)
lines(Im(per(x,plot_T)$DFT),col="blue")
mtext("Im(transfer function)", side = 3, line = -1,at=len1/2,col="black")
mtext("Im(convolution of DFT)", side = 3, line = -2,at=len1/2,col="green")
mtext("Im(DFT output)", side = 3, line = -3,at=len1/2,col="red")
mtext("Im(DFT input)", side = 3, line = -4,at=len1/2,col="blue")
axis(1,at=c(0,1:6*len1/6+1),labels=c("0","pi/6","2pi/6","3pi/6",
"4pi/6","5pi/6","pi"))
axis(2)
box()
dev.off()
@
<<label=z_convo_ar1.pdf,echo=FALSE,results=tex>>=
  file = paste("z_convo_ar1.pdf", sep = "")
  cat("\\begin{figure}[H]")
  cat("\\begin{center}")
  cat("\\includegraphics[height=6in, width=6in]{", file, "}\n",sep = "")
  cat("\\caption{Convolution of AR(1)-filter (green), periodogram of input (blue) and output (red)
  signals and amplitude function of AR(1) filter (black)", sep = "")
  cat("\\label{z_convo_ar1}}", sep = "")
  cat("\\end{center}")
  cat("\\end{figure}")
@
Convolution (green) and periodogram of output (red) in the top-left panel are almost indistinguishable, which
confirms \ref{convolution_per}, at least up to
frequency zero: the latter error is still small and its impact (on optimized filters in the DFA)
will be negligible when compared to the natural sampling error. The strong smoothing obtained by the large AR(1)-coefficient
$a_1=0.95$ implies that high-frequency components are heavily damped by the filter as can
be seen in the top-right panel of fig.\ref{z_convo_ar1}. The bottom panel confirms \ref{convolution_dft} for
real (left) and imaginary (right) parts of the DFT's involved.


\item \label{ar12}We here analyze the inverse of the seasonal MA(12) analyzed in exercise \ref{seasonal diff}, p.\pageref{seasonal diff},
namely the AR(12) filter
\[y_t=y_{t-12}+x_t\]
where $x_t=\epsilon_t$ is again white noise and where $a_k=0,k\not= 12, a_{12}=1$. This filter is particular because it is \emph{unstable}:
the roots of the characteristic polynomial
<<echo=True>>=
polyroot(c(1,rep(0,11),-1))
@
all lie on the unit circle i.e. their absolute value is one
<<echo=True>>=
abs(polyroot(c(1,rep(0,11),-1)))
@
In such a case the classical spectral convolution theorem \ref{spectral_conv} cannot be applied because the output $y_t$ of the
filter is not stationary. In contrast, Our frequency-domain number-identities apply irrespective of model assumptions. It is therefore
interesting to check if the discrete finite sample spectral convolution \ref{convolution_per} applies or, more precisely, if the approximation
error is still `small'. We now generate a realization of length 100 of the above process and compare periodograms and DFT's of
input and output signals with spectral estimates obtained by convolution.
<<echo=True>>=
set.seed(10)
len2<-1000
a12<-1
b0<-1
omega1<-pi/20
y1<-rep(0,len2)
x1<-rnorm(len2)
for (i in 13:len2)
  y1[i]<-b0*x1[i]+a12*y1[i-12]
# The first 900 observations are discarded (initialization)
len<-120
y<-y1[(len2-len+1):len2]
x<-x1[(len2-len+1):len2]
# Resolution of discrete frequency grid
len1<-len/2
omega_k<-(0:len1)*pi/len1
# Compute transfer function, amplitude, phase and time-shift
trffkt<-b0/(1-a12*complex(arg=-12*omega_k))
# The transfer function is infinite at the seasonal frequencies
trffkt[c(1,1+1:6*(len1/6))]<-Inf
amplitude<-abs(trffkt)
phase<-Arg(trffkt)
shift<-phase/omega_k
plot_T<-F

file = paste("z_convo_ar12.pdf", sep = "")
pdf(file = paste(path.out,file,sep=""), paper = "special",
width = 6, height = 6)
par(mfrow=c(2,2))
plot(amplitude,type="l",main="Convolution seasonal AR(12)",
axes=F,xlab="Frequency",ylab="periodograms",col="black",ylim=c(0,5))
lines(amplitude^2*per(x,plot_T)$per,col="green")
lines(per(y,plot_T)$per,col="red",lty=2)
lines(per(x,plot_T)$per,col="blue")
mtext("Amplitude", side = 3, line = -1,at=len1/2,col="black")
mtext("Convolution", side = 3, line = -2,at=len1/2,col="green")
mtext("Periodogram output", side = 3, line = -3,at=len1/2,col="red")
mtext("Periodogram input", side = 3, line = -4,at=len1/2,col="blue")
axis(1,at=c(0,1:6*len1/6+1),labels=c("0","pi/6","2pi/6","3pi/6",
"4pi/6","5pi/6","pi"))
axis(2)
box()
ts.plot(x,col="blue",main="Input and output series",ylab="",
ylim=c(min(y),max(y)))
lines(y,col="red")
mtext("Input series", side = 3, line = -1,at=len/2,col="blue")
mtext("Output series", side = 3, line = -2,at=len/2,col="red")
plot(Re(trffkt),type="l",main="Convolution: real-parts of DFT",
axes=F,xlab="Frequency",ylab="Real parts of DFT",col="black",ylim=c(-5,5))
lines(Re(trffkt*per(x,plot_T)$DFT),col="green")
lines(Re(per(y,plot_T)$DFT),col="red",lty=2)
lines(Re(per(x,plot_T)$DFT),col="blue")
mtext("Re(transfer function)", side = 3, line = -1,at=len1/2,col="black")
mtext("Re(convolution of DFT)", side = 3, line = -2,at=len1/2,col="green")
mtext("Re(DFT output)", side = 3, line = -3,at=len1/2,col="red")
mtext("Re(DFT input)", side = 3, line = -4,at=len1/2,col="blue")
axis(1,at=c(0,1:6*len1/6+1),labels=c("0","pi/6","2pi/6","3pi/6",
"4pi/6","5pi/6","pi"))
axis(2)
box()
plot(Im(trffkt),type="l",main="Convolution: imaginary-parts of DFT",
axes=F,xlab="Frequency",ylab="Imaginary parts of DFT",col="black",ylim=c(-5,5))
lines(Im(trffkt*per(x,plot_T)$DFT),col="green")
lines(Im(per(y,plot_T)$DFT),col="red",lty=2)
lines(Im(per(x,plot_T)$DFT),col="blue")
mtext("Im(transfer function)", side = 3, line = -1,at=len1/2,col="black")
mtext("Im(convolution of DFT)", side = 3, line = -2,at=len1/2,col="green")
mtext("Im(DFT output)", side = 3, line = -3,at=len1/2,col="red")
mtext("Im(DFT input)", side = 3, line = -4,at=len1/2,col="blue")
axis(1,at=c(0,1:6*len1/6+1),labels=c("0","pi/6","2pi/6","3pi/6",
"4pi/6","5pi/6","pi"))
axis(2)
box()
dev.off()
@
<<label=z_convo_ar12.pdf,echo=FALSE,results=tex>>=
  file = paste("z_convo_ar12.pdf", sep = "")
  cat("\\begin{figure}[H]")
  cat("\\begin{center}")
  cat("\\includegraphics[height=6in, width=6in]{", file, "}\n",sep = "")
  cat("\\caption{Convolution of unstable seasonal AR(12)-filter (green), periodogram of input (blue) and output (red)
  signals and amplitude function of seasonal AR(12) (black)", sep = "")
  cat("\\label{z_convo_ar12}}", sep = "")
  cat("\\end{center}")
  cat("\\end{figure}")
@
Please note that green and black lines in the above graphs are not defined at the singularities $j\pi/6$, $j=0,1,...,6$ because
the transferfunction
\[\Gamma(\omega)=\frac{1}{1-\exp(-i12\omega)}\]
is infinite whenever $\exp(-i12\omega)=1$ i.e. whenever $\omega=j\pi/6$. Therefore, the convolution
$|\Gamma(\omega_k)|^2I_{TX}(\omega_k)$ (green) is infinite. In contrast, the periodogram
of the output series $I_{TY}(\omega_k)$ (red, top-left panel) must be finite since $y_t$ is a well-defined (finite) time series. But We can see that
$I_{TY}(\omega_k)$ (red line, top-left) peaks at $\omega=j\pi/6$, as desired\footnote{These peaks would diverge asymptotically if $T\to\infty$:
you can convince yourself by increasing len2 and len in the R-code.}. At
all other frequencies the correspondence between green and red lines is good thus confirming pertinence of Our finite-sample discrete
convolution results \ref{convolution_dft} and \ref{convolution_per}. The infinite peaks of the amplitude function in the seasonal frequencies
means that the corresponding frequency components are magnified `very heavily': as can be seen in the upper right panel, red line,
the periodicity is strongly marked. Letting the data-size grow unboundedly would lead to asymptotically unbounded seasonal
patterns i.e. the difference between consecutive/different months would diverge asymptotically. This typical non-stationary
feature is reflected by the infinite peaks of the amplitude function. Of course, any finite sample of the non-stationary
seasonal process $y_t$ must be finite and therefore its DFT or periodogram must be finite too.
\end{enumerate}
The above exercises confirme that Our discrete finite sample convolution results \ref{convolution_dft} and \ref{convolution_per}
hold remarkably well over a broad range of filters -- the approximation error is small/negligible -- even in situations where the standard theory
(equation \ref{spectral_conv}) fails, because input and/or output signals are integrated. However, We must warn against uncautious application because the
approximation error may become `large' -- sufficiently large to interfer with Our later optimization -- if the data is
trending: recall that a trend strongly contradicts the implicit periodicity assumption of the DFT (see exercise
\ref{period_e}, p.\pageref{period_e}, and fig.\ref{z_nsf}). In the following section We propose an extension of the convolution
results to non-stationary \emph{integrated} processes.



\subsubsection{Non-stationary Integrated I(1)-Processes} \label{pseudo_pseudo}

We mainly discuss the practically relevant case of a non-stationary integrated \emph{input} process $x_t\in I(1)$ which is stationary after
ordinary differences
\[x_t-x_{t-1}=u_t\in I(0)\]
In principle $x_t$ can be viewed as the output of an unstable AR(1)-filter with input $u_t$:
\[x_t=x_{t-1}+u_t\]
whose transfer function
\[\Gamma(\omega)=\frac{1}{1-\exp(-i\omega)}\]
has a singularity in frequency zero: low-frequency components are heavily magnified by the (unstable) filter or, stated otherwise,
$x_t$ is trending, see section 9\footnote{???} in the SARIMA-script. Our discrete convolution results suggest that
\begin{eqnarray}
\Xi_{TX}(\omega_k)&\approx&\frac{\Xi_{TU}(\omega_k)}{1-\exp(-i\omega_k)}\label{approx1}\\
I_{TX}(\omega_k)&\approx&\frac{I_{TU}(\omega_k)}{|1-\exp(-i\omega_k)|^2}\label{approx2}
\end{eqnarray}
Unfortunately, the singularity of the transferfunction leads to an unreconciliable mismatch in frequency $\omega_0=0$ since
the left-hand side is finite (the data $x_1,...,x_T$ is finite) whereas the right-hand side is generally infinite in frequency
zero. A similar mismatch was observed in exercise \ref{ar12}, p.\pageref{ar12}, see fig.\ref{z_convo_ar12}: the periodogram of the
output signal (red line) was finite in the seasonal frequencies vs. an infinite convolution (green line). Besides this obvious mismatch
in the unit-root frequency zero,
one can show that $I_{TX}(\omega_k)$ is also biased for all $k>0$ whereby the amount of distortion depends on the input process see the exercise
below and Wildi (2008) for the theory. The approximation errors in \ref{approx1} and \ref{approx2} are no more negligible,
in general, though they are often found to be negligible for typical economic data, see the exercise below.
In order to eliminate the bias We recommend usage of the pseudo-DFT and of the pseudo-periodogram statistics
\begin{eqnarray}
\Xi_{TX}^{\textrm{pseudo}}(\omega_k)&:=&\frac{\Xi_{TU}(\omega_k)}{1-\exp(-i\omega_k)}\label{pseudo_dft}\\
I_{TX}^{\textrm{pseudo}}(\omega_k)&:=&\frac{I_{TU}(\omega_k)}{|1-\exp(-i\omega_k)|^2}\label{pseudo_per}
\end{eqnarray}
in lieu of the `direct' sample estimates $\Xi_{TX}(\omega_k)$ and $I_{TX}(\omega_k)$.

\subsubsection{Exercise}

We here illustrate that the periodogram of an integrated process is generally biased. Since the magnitude of the bias
depends on the Data Generating Process (DGP) \footnote{The bias is larger if the differenced stationary process is positively autocorrelated, see
Wildi (2008) for details.} We also attempt to convey an intuitive feeling of the problem by analyzing
three simple processes which reflect characteristics of typical economic data. Specifically, let $y_t$ be the output of
the MA(1) ordinary difference filter
\begin{eqnarray}\label{y_diff_bias}
y_t&=&x_t-x_{t-1}
\end{eqnarray}
and assume the input is a non-stationary process
\begin{eqnarray*}
x_t&=&x_{t-1}+u_t
\end{eqnarray*}
where the stationary differences are an AR(1)-process
\begin{eqnarray}\label{ar1_bias}
u_t&=&a_1u_{t-1}+\epsilon_t
\end{eqnarray}
Note that $y_t=x_t-x_{t-1}=u_t$. Thus the periodogram of $y_t$ should coincide with the periodogram of $u_t$:
\[I_{TY}(\omega_k)=I_{TU}(\omega_k)\]
We now check
this trivial requirement by computing the periodogram $I_{TY}(\omega_k)$ of $y_t$ in two different ways:
\begin{eqnarray}
I_{TY}^{\textrm{pseudo}}(\omega_k)&:=&|1-\exp(-i\omega_k)|^2\frac{I_{TU}(\omega_k)}{|1-\exp(-i\omega_k)|^2}=I_{TU}(\omega_k)\label{pseudo_est}\\
I_{TY}^{\textrm{direct}}(\omega_k)&:=&|1-\exp(-i\omega_k)|^2I_{TX}(\omega_k)\approx I_{TU}(\omega_k)\label{direct_conv}
\end{eqnarray}
where $1-\exp(-i\omega_k)$ is the transfer function of the difference filter in \ref{y_diff_bias}. The first periodogram estimate
$I_{TY}^{\textrm{pseudo}}(\omega_k)$ relies on \ref{convolution_per} where We insert the pseudo-periodogram \ref{pseudo_per}
of the non-stationary $x_t$. This equates trivially to $I_{TU}(\omega_k)$, as desired, which happens to be $I_{TY}(\omega_k)$
also\footnote{In practice, applying the difference operator to $x_t$ means that one looses one observation i.e. the
sample becomes $y_2,...,y_T$ whereas the periodogram of $u_t$ could rely on $u_1,...,u_T$: We here ignore this
artifact and assume that $y_1=u_1$ is known.}.
The second periodogram estimate \ref{direct_conv} relies on
a direct computation of the periodogram $I_{TX}(\omega_k)$. We now analyze the quality of the approximation of
$I_{TU}(\omega_k)$ by $I_{TY}^{\textrm{direct}}(\omega_k)$ in \ref{direct_conv}. As We shall see, the magnitude of the approximation error
depends on $a_1$ in \ref{ar1_bias}: We therefore analyze three settings -- data samples -- corresponding to $a_1=-0.9,0$ and $0.9$.



\begin{enumerate}
\item Consider the above filter \ref{y_diff_bias} where $x_t=x_{t-1}+u_t$ is based on \ref{ar1_bias}. Generate three
different realizations of length 1000 of $y_t, x_t$ based on $a_1=-0.9, a_1=0$ and $a_1=0.9$ in \ref{ar1_bias} and analyze the
quality of the approximation
\ref{direct_conv} by comparing $I_{TY}^{\textrm{direct}}(\omega_k)$ and $I_{TU}(\omega_k)$ graphically, see fig.\ref{z_convo_bias}. Hint:
use the same setseed argument for all three realizations.

<<echo=True>>=
len<-1000
set.seed(10)
u1<-arima.sim(list(ar=-0.9),n=len)
set.seed(10)
u2<-arima.sim(list(ar=0.),n=len)
set.seed(10)
u3<-arima.sim(list(ar=0.9),n=len)
# Compute transferfunction of difference filter
len1<-length(per(u1,plot_T)$per)-1
trffkt_diff<-(1-exp(1.i*(0:len1)*pi/len1))
# Amplitude function of difference filter
amp_diff<-abs(trffkt_diff)

file = paste("z_convo_bias.pdf", sep = "")
pdf(file = paste(path.out,file,sep=""), paper = "special", width = 6, height = 6)
plot_T<-F
par(mfrow=c(3,1))
per_pseudo<-per(u1,plot_T)$per
# compute non-stationary x
x1<-cumsum(u1)
per_direct<-amp_diff^2*per(x1,plot_T)$per
plot(per_pseudo,type="l",main="Bias effect: a1=-0.9",
axes=F,xlab="Frequency",ylab="periodograms",col="black",ylim=c(0,max(per_direct)))
lines(per_direct,col="blue")
mtext("Direct (biased)", side = 3, line = -1,at=len1/2,col="blue")
mtext("Pseudo (unbiased)", side = 3, line = -2,at=len1/2,col="black")
axis(1,at=c(0,1:6*len1/6+1),labels=c("0","pi/6","2pi/6","3pi/6","4pi/6","5pi/6","pi"))
axis(2)
box()
per_pseudo<-per(u2,plot_T)$per
x2<-cumsum(u2)
per_direct<-amp_diff^2*per(x2,plot_T)$per
plot(per_pseudo,type="l",main="Bias effect: a1=0",
axes=F,xlab="Frequency",ylab="periodograms",col="black",ylim=c(0,max(per_direct)))
lines(per_direct,col="blue")
mtext("Direct (biased)", side = 3, line = -1,at=len1/2,col="blue")
mtext("Pseudo (unbiased)", side = 3, line = -2,at=len1/2,col="black")
axis(1,at=c(0,1:6*len1/6+1),labels=c("0","pi/6","2pi/6","3pi/6","4pi/6","5pi/6","pi"))
axis(2)
box()
per_pseudo<-per(u3,plot_T)$per
x3<-cumsum(u3)
per_direct<-amp_diff^2*per(x3,plot_T)$per
plot(per_pseudo,type="l",main="Bias effect: a1=0.9",
axes=F,xlab="Frequency",ylab="periodograms",col="black",ylim=c(0,max(per_direct)))
lines(per_direct,col="blue")
mtext("Direct (biased)", side = 3, line = -1,at=len1/2,col="blue")
mtext("Pseudo (unbiased)", side = 3, line = -2,at=len1/2,col="black")
axis(1,at=c(0,1:6*len1/6+1),labels=c("0","pi/6","2pi/6","3pi/6","4pi/6","5pi/6","pi"))
axis(2)
box()
dev.off()
@
<<label=z_convo_bias.pdf,echo=FALSE,results=tex>>=
  file = paste("z_convo_bias.pdf", sep = "")
  cat("\\begin{figure}[H]")
  cat("\\begin{center}")
  cat("\\includegraphics[height=6in, width=6in]{", file, "}\n",sep = "")
  cat("\\caption{Bias effect depending on DGP: a1=-0.9 (top), a1=0 (middle) and a1=0.9 (bottom)", sep = "")
  cat("\\label{z_convo_bias}}", sep = "")
  cat("\\end{center}")
  cat("\\end{figure}")
@
We can see that `direct' and `pseudo' periodograms are almost indistinguishable for $a_1=-0.9$ (because the strong negative autocorrelation of $u_t$
nearly `cancels' the unit-root of $x_t$); for $a_1=0$ We can observe differences but nothing `systematic'
yet\footnote{One can show that the periodogram of $x_t$ retains all relevant information in this case (random-walk), see
Wildi (2008).}; for $a_1=0.9$, however,
the difference between both periodograms is substantial and the bias of $I_{TY}^{\textrm{direct}}(\omega_k)$ is marked for
the higher frequencies whose contributions to the sample variance are `massively' exagerated: We observe a strong positive bias of
$I_{TY}^{\textrm{direct}}(\omega_k)$. Note that $a_1=0.9$ means that a typical path of $x_t$ ressembles a realization of an I(2)-process:
We here deliberately magnify the unit-root distortion in order to make the problem palpable.

\end{enumerate}
We infer from the above exercise that a direct computation of the periodogram $I_{TX}(\omega_k)$ of a non-stationary series $x_t$
is nearly unbiased if the (stationary) first differences $u_t=x_t-x_{t-1}$ do not show evidence of a strong positive
autocorrelation ($a_1$ `small') which applies to typical economic data\footnote{Differences or log-returns of important macro-series (GDP, IPI,
employment,...) or of financial data (stock prices, futures,...) are generally weakly autocorrelated or even close to white noise.}. In such a case the difference between the
pseudo-periodogram and the periodogram is negligible.

\subsubsection{Non-Stationary Processes: Additional Unit-Roots}

If $x_t\in I(d)$ is of higher integration order $d\geq 2$ or if the unit-roots are seasonal (not in frequency zero) then $x_t$ could be differenced
accordingly and the pseudo-DFT or the pseudo-periodogram could be obtained. As an example
\begin{eqnarray*}
\Xi_{TX}^{\textrm{pseudo}}(\omega_k)&:=&\frac{\Xi_{TU}(\omega_k)}{(1-\exp(-i\omega_k))^2}\\
I_{TX}^{\textrm{pseudo}}(\omega_k)&:=&\frac{I_{TU}(\omega_k)}{|1-\exp(-i\omega_k)|^4}
\end{eqnarray*}
for an I(2)-process
\[(1-B)^2x_t\in I(0)\]
or
\begin{eqnarray*}
\Xi_{TX}^{\textrm{pseudo}}(\omega_k)&:=&\frac{\Xi_{TU}(\omega_k)}{1-\exp(-i12\omega_k)}\\
I_{TX}^{\textrm{pseudo}}(\omega_k)&:=&\frac{I_{TU}(\omega_k)}{|1-\exp(-i12\omega_k)|^2}
\end{eqnarray*}
for a process with seasonal roots
\[(1-B^{12})x_t\in I(0)\]
However We did not find the above extensions to be useful
for `typical' economic data and therefore We here restrict ourselves to the the simple I(1)-case with ordinary (first) differences.
We recommend usage of the pseudo-DFT/pseudo-periodogram if first differences of the data are strongly positively
autocorrelated.
It should be emphasized that log-returns are often more interesting than the original levels because the former emphasize the
relevant growth-dynamics: in such a case
one can apply ordinary DFT and periodogram statistics to the (non-trending) log-transformed data.



\section{Direct Filter Approach (DFA)}

In the previous section We analyzed extensively filters (scaling/shift/convolution) and We proposed tools
for quantifying their effects (transfer function, amplitude, phase). We now attempt to optimize filter coefficients
in view of a particular task: forecasting, nowcasting or backcasting the output of a (possibly bi-infinite) target filter \ref{target_i} based
on a finite sample $x_1,...,x_T$ of $x_t$.


\subsection{Mean-Square Paradigm}

According to section \ref{def_fil} We assume a general target specification
\begin{equation}\label{target}
y_t=\sum_{k=-\infty}^{\infty}\gamma_{k} x_{t-k}
\end{equation}
For practical relevance as well as for simplicity We assume stability and symmetry of the target filter $\gamma_k=\gamma_{-k}$.  We now
seek filter coefficients $b_{kh}$, $k=h,...,L-1+h$ such that the finite sample
estimate
\begin{equation}\label{filter}
\hat{y}_{t}^{h}:=\sum_{k=h}^{L-1+h}b_{kh}x_{t-k}
\end{equation}
is closest possible to $y_{t}$, $h\in \mbox{Z\hspace{-.3em}Z}$, in mean-square
\begin{eqnarray}\label{mso}
E\left[(y_{t}-\hat{y}_{t}^h)^2\right]\to\min_{\mathbf{b}_h}
\end{eqnarray}
where $\mathbf{b}_h=(b_{hh},...,b_{L-1+h,h})$.
\begin{itemize}
\item If $h=0$ We use data $x_t,...,x_{t-(L-1)}$ for estimating $y_t$: this is a nowcast and $b_{k0}$, $k=0,...,L-1$ is a real-time filter.
\item If $h=1$ We use data $x_{t-1},...,x_{t-L}$ for estimating $y_t$: this is a forecast and $b_{k1}$, $k=1,...,L$ is a forecast filter.
\item If $h=-1$ We use data $x_{t+1},...,x_{t-(L-2)}$ for estimating $y_t$: this is a backcast and $b_{k,-1}$, $k=-1,...,L-2$ is a smoother.
\end{itemize}
For simplicity of notation (as well as practical relevance) We now fix attention
to the real-time estimate (nowcast) $\hat{y}_T$ of $y_T$ and drop the index $h=0$ in sub- and superscripts.\\

As usual, in applications, the expectation is unknown and therefore We could try to replace \ref{mso} by its sample estimate
\begin{equation}\label{s_dfa}
\frac{1}{T}\sum_{t=1}^T (y_{t}-\hat{y}_{t})^2=\frac{2\pi}{T}\sum_{k=-[T/2]}^{[T/2]}I_{T\Delta Y}(\omega_k)
\end{equation}
where $I_{T\Delta Y}(\omega_k)$ is the periodogram of $\Delta y_t:=y_{t}-\hat{y}_{t}$ and where the identity follows from \ref{spec_dec_per}.
Unfortunately, the output $y_t$ of the generally bi-infinite filter isn't observed and therefore $I_{T\Delta Y}(\omega_k)$ is unknown too.
But We could try to approximate the periodogram by relying on the finite-sample discrete convolution \ref{convolution_per}:
\begin{eqnarray*}
I_{T\Delta Y}(\omega_k)&\approx& \left|\Delta \Gamma(\omega_k) \right|^2I_{TX}(\omega_k)
\end{eqnarray*}
where $\Delta \Gamma(\omega_k)=\Gamma(\omega_k)-\hat{\Gamma}(\omega_k)=\sum_{j=-\infty}^{\infty}\Delta\gamma_j\exp(-ij\omega_k)$
is the difference of target and real-time transfer functions (recall that We assume $h=0$, for simplicity). Then
\begin{eqnarray}
\frac{1}{T}\sum_{t=1}^T (y_{t}-\hat{y}_{t})^2&=&\frac{2\pi}{T}\sum_{k=-[T/2]}^{[T/2]}I_{T\Delta Y}(\omega_k)\label{s_dfa_e}\\
&\approx&\frac{2\pi}{T}\sum_{k=-[T/2]}^{[T/2]}\left|\Delta\Gamma(\omega_k) \right|^2 I_{TX}(\omega_k)\to\min_{\mathbf{b}} \label{dfa_ms}
\end{eqnarray}
In contrast to \ref{s_dfa} or \ref{mso}, the frequency-domain criterion \ref{dfa_ms} can be computed: since filter coefficients $b_0,...,b_{L-1}$
are optimized `directly' We named this method \emph{Direct Filter Approach} (DFA). We can see that the periodogram $I_{TX}(\omega_k)$ weights
the approximation error $\Delta \Gamma(\omega_k)$ between target and real-time filters: the latter must be close to the former
if $I_{TX}(\omega_k)$ is large,
in relative terms. Formally, one can show that the approximation error in \ref{dfa_ms} is `unusually' small\footnote{The term `unusual' here means that
the error is hidden by ordinary sample fluctuations whose magnitude is typically of order $\textrm{O}(1/\sqrt{T})$: the error
in \ref{dfa_ms} is of smaller magnitude asymptotically: this property is commonly referred to as `superconsistency'. The interested reader is
referred to Wildi (2008) for formal results.}. Furthermore, the time-domain MSE (left-hand side of \ref{s_dfa_e}) is itself an asymptotically
efficient estimate of \ref{mso}, see Wildi (2008). Therefore, minimizing \ref{dfa_ms} is indeed pertinent.
If $x_t$ is integrated (trending) then one could plug the \emph{pseudo-periodogram} \ref{pseudo_per}
of $x_t$ as an alternative.\\

From a computational perspective the above criterion \ref{dfa_ms} is convenient because it is a squared function of the unknown coefficients.
Therefore a unique solution exists and can be computed analytically, in closed-form (interested readers are referred to
Wildi (2013)\footnote{\url{http://blog.zhaw.ch/idp/sefblog/index.php?/archives/259-Elements-of-Forecasting-and-Signal-Extraction.html}}).



\subsubsection{Exercises}\label{ex_lag_amp}

We here compute real-time estimates of the ideal lowpass
\begin{eqnarray*}
\Gamma(\omega,\textrm{cutoff})=\left\{\begin{array}{cc}1& |\omega|<\textrm{cutoff}\\0&\textrm{otherwise} \end{array} \right.
\end{eqnarray*}
with filter weights
\[
\Gamma(\omega)=\frac{\textrm{cutoff}}{\pi}+\frac{2}{\pi}\sum_{k=1}^\infty\frac{\sin(k\cdot\textrm{cutoff})}{k} \cos(k\omega)
\]
see \ref{lwhfsw}. The following DFA-code can perform typical mean-square optimization based on criterion \ref{dfa_ms}:
<<echo=True>>=
# This function computes mean-square DFA-solutions
# L is the length of the MA filter,
# periodogram is the frequency weighting function in the DFA
# Gamma is the transferfunction of the symmetric filter (target) and
# Lag is the lag-parameter: Lag=0 implies real-time filtering, Lag=L/2
#     implies symmetric filter
# The function returns optimal coefficients as well as the transfer function of the
#     optimized real-time filter
dfa_ms<-function(L,periodogram,Lag,Gamma)
{

  K<-length(periodogram)-1
  X<-exp(-1.i*Lag*pi*(0:(K))/(K))*rep(1,K+1)*sqrt(periodogram)
  X_y<-exp(-1.i*Lag*pi*(0:(K))/(K))*rep(1,K+1)
  for (l in 2:L)          #l<-L<-21
  {
    X<-cbind(X,(cos((l-1-Lag)*pi*(0:(K))/(K))+
    1.i*sin((l-1-Lag)*pi*(0:(K))/(K)))*sqrt(periodogram))
    X_y<-cbind(X_y,(cos((l-1-Lag)*pi*(0:(K))/(K))+
    1.i*sin((l-1-Lag)*pi*(0:(K))/(K))))
  }
  xtx<-t(Re(X))%*%Re(X)+t(Im(X))%*%Im(X)
# MA-Filtercoefficients
  b<-as.vector(solve(xtx)%*%(t(Re(X_y))%*%(Gamma*periodogram)))
# Transferfunction
  trffkt<-1:(K+1)
  trffkt[1]<-sum(b)
  for (k in 1:(K))#k<-1
  {
    trffkt[k+1]<-(b%*%exp(1.i*k*(0:(length(b)-1))*pi/(K)))
  }
  return(list(b=b,trffkt=trffkt))
}
@
The following inputs are required in the head of the function-call
\begin{itemize}
\item $L$: the filter length. Larger $L$ require more coefficients to be estimated and the resulting filter
will be more flexible (which is welcome) but potentially prone to \emph{overfitting} (which is unwelcome).
\item weight\textunderscore func: the frequency weighting-function. We will use mainly
the periodogram (experienced users know why). But We will also plug-in model-based spectra
in section \ref{replica} (in order to replicate and to customize model-based approaches).
\item Lag: corresponds to $-h$ i.e. it allows for estimation of $y_{T-Lag}$: A forecast, nowcast (real-time filter) or backcast (smoother)
is obtained depending on $Lag<0$, $Lag=0$ or $Lag>0$. A finite symmetric filter is obtained if $L$ is an odd
integer and $Lag=(L-1)/2$.
\item Gamma: the target symmetric filter.
\end{itemize}



\begin{enumerate}
\item We estimate optimal real-time filters ($h=Lag=0$) of length $L=12$ for three different stationary processes
\begin{eqnarray}
\left.\begin{array}{ccc}x_t&=&0.9x_{t-1}+\epsilon_t\\
x_t&=&\epsilon_t\\
x_t&=&-0.9x_{t-1}+\epsilon_t
\end{array}\right\}\label{ar1_processes}
\end{eqnarray}
by applying the above DFA-function to approximate the ideal lowpass
with cutoff $\pi/6$. Since the processes are different We expect the solutions of \ref{dfa_ms} to be different too.
\begin{itemize}

\item Generate realizations of length $T=120$ for the above three processes and compute real-time estimates $\hat{y}_t$
of $y_t$.  Plot the data and
the filtered series see fig.\ref{z_dfa_ar1_output}.
<<echo=True>>=
# Generate series
set.seed(10)
len<-120
a_vec<-c(0.9,0,-0.9)
x<-matrix(nrow=len,ncol=3)
plot_T<-F
yhat<-x
periodogram<-matrix(ncol=3,nrow=len/2+1)
trffkt<-periodogram
# Generate series
for (i in 1:3)
{
  set.seed(10)
  x[,i]<-arima.sim(list(ar=a_vec[i]),n=len)
}
# Specify filter settings
L<-12
Lag<-0
Gamma<-c(1,(1:(len/2))<len/12)
b<-matrix(nrow=L,ncol=3)
# Compute real-time filters
for (i in 1:3)
{
  periodogram[,i]<-per(x[,i],plot_T)$per
# Optimize filters
  filt<-dfa_ms(L,periodogram[,i],Lag,Gamma)
  trffkt[,i]<-filt$trffkt
  b[,i]<-filt$b
# Compute outputs
  for (j in L:len)
    yhat[j,i]<-filt$b%*%x[j:(j-L+1),i]
}
file = paste("z_dfa_ar1_output.pdf", sep = "")
pdf(file = paste(path.out,file,sep=""), paper = "special", width = 6, height = 6)
par(mfrow=c(3,1))
for (i in 1:3)
{
  ts.plot(x[,i],main=paste("a1 = ",a_vec[i],sep=""),col="blue")
  lines(yhat[,i],col="red")
}
dev.off()
@
<<label=z_dfa_ar1_output.pdf,echo=FALSE,results=tex>>=
  file = paste("z_dfa_ar1_output", sep = "")
  cat("\\begin{figure}[H]")
  cat("\\begin{center}")
  cat("\\includegraphics[height=6in, width=6in]{", file, "}\n",sep = "")
  cat("\\caption{Inputs (blue) and real-time outputs (red) for a1=0.9 (top), a1=0 (middle) and a1=-0.9 (bottom)", sep = "")
  cat("\\label{z_dfa_ar1_output}}", sep = "")
  cat("\\end{center}")
  cat("\\end{figure}")
@
We can see that the filter effect is weakest for the top-DGP ($a_1=0.9$) and strongest for the bottom process ($a_1=-0.9$).
\item Compare amplitude and time-shift functions of the three real-time filters graphically, see fig.\ref{z_dfa_ar1_amp_shift}.

<<echo=True>>=
omega_k<-pi*0:(len/2)/(len/2)
file = paste("z_dfa_ar1_amp_shift.pdf", sep = "")
pdf(file = paste(path.out,file,sep=""), paper = "special", width = 6, height = 6)
par(mfrow=c(2,2))
amp<-abs(trffkt)
shift<-Arg(trffkt)/omega_k
plot(amp[,1],type="l",main="Amplitude functions",
axes=F,xlab="Frequency",ylab="Amplitude",col="black",ylim=c(0,1))
lines(amp[,2],col="orange")
lines(amp[,3],col="green")
lines(Gamma,col="violet")
mtext("Amplitude a1=0.9", side = 3, line = -1,at=len/4,col="black")
mtext("Amplitude a1=0", side = 3, line = -2,at=len/4,col="orange")
mtext("Amplitude a1=-0.9", side = 3, line = -3,at=len/4,col="green")
mtext("Target", side = 3, line = -4,at=len/4,col="violet")
axis(1,at=c(0,1:6*len/12+1),labels=c("0","pi/6","2pi/6","3pi/6",
"4pi/6","5pi/6","pi"))
axis(2)
box()
plot(shift[,1],type="l",main="Time-shifts",
axes=F,xlab="Frequency",ylab="Shift",col="black",
ylim=c(0,max(na.exclude(shift[,3]))))
lines(shift[,2],col="orange")
lines(shift[,3],col="green")
lines(rep(0,len/2+1),col="violet")
mtext("Shift a1=0.9", side = 3, line = -1,at=len/4,col="black")
mtext("Shift a1=0", side = 3, line = -2,at=len/4,col="orange")
mtext("Shift a1=-0.9", side = 3, line = -3,at=len/4,col="green")
mtext("Target", side = 3, line = -4,at=len/4,col="violet")
axis(1,at=c(0,1:6*len/12+1),labels=c("0","pi/6","2pi/6","3pi/6",
"4pi/6","5pi/6","pi"))
axis(2)
box()
plot(periodogram[,1],type="l",main="Periodograms",
axes=F,xlab="Frequency",ylab="Periodogram",col="black",
ylim=c(0,max(periodogram[,3])/6))
lines(periodogram[,2],col="orange")
lines(periodogram[,3],col="green")
mtext("Periodogram a1=0.9", side = 3, line = -1,at=len/4,col="black")
mtext("Periodogram a1=0", side = 3, line = -2,at=len/4,col="orange")
mtext("Periodogram a1=-0.9", side = 3, line = -3,at=len/4,col="green")
axis(1,at=c(0,1:6*len/12+1),labels=c("0","pi/6","2pi/6","3pi/6",
"4pi/6","5pi/6","pi"))
axis(2)
box()
dev.off()
@
<<label=z_dfa_ar1_amp_shift.pdf,echo=FALSE,results=tex>>=
  file = paste("z_dfa_ar1_amp_shift.pdf", sep = "")
  cat("\\begin{figure}[H]")
  cat("\\begin{center}")
  cat("\\includegraphics[height=6in, width=6in]{", file, "}\n",sep = "")
  cat("\\caption{Amplitude (top left), time-shifts (top-right) and periodograms (bottom left) for
  a1=0.9 (black), a1=0 (orange) and a1=-0.9 (green)", sep = "")
  cat("\\label{z_dfa_ar1_amp_shift}}", sep = "")
  cat("\\end{center}")
  cat("\\end{figure}")
@
\end{itemize}
The amplitude function (top left panel) of the black filter (corresponding to the positively autocorrelated series $a_1=0.9$)
is the farest away from the target in the stop-band -- the filter damps high-frequency components the least -- but it is closest
to the target in in the passband $\omega<\pi/6$: it sacrifices to some extent high-frequency
damping against better passband properties. The converse applies to the green filter, corresponding to $a_1=-0.9$ and the orange
filter (white noise $a_1=0$) lies in between. The very strong (relative) damping of the green filter shows up in a larger time-shift
(top right panel). This strong damping towards the highest frequencies is due to a large green periodogram (bottom left panel)
towards $\omega=\pi$ (We decided to truncate black and green periodograms for ease of visual inspection). In contrast,
the large black periodogram towards the lowest frequencies implies that the fit of of $\Gamma(\omega_k)$ (violet) by
$\hat{\Gamma}(\omega_k)$ (black) in the top panels must be improved: both the amplitude (left) as well as the shift (right)
are closest to the target in the passband. These observations match very well the structure of the optimization criterion
\ref{dfa_ms}, as expected.
\item We now briefly verify that the optimized coefficients are indeed optimal
\begin{itemize}
\item Plug $\Gamma(\omega_k)$ and $\hat{\Gamma}(\omega_k)$ into \ref{dfa_ms} and compute the criterion values for all
three processes.
\item Exchange $\hat{\Gamma}(\omega_k)$ and periodograms corresponding to the three processes in \ref{dfa_ms} and verify that any process-specific
solution is suboptimal for the other two processes (as an example: the white-noise solution should not be optimal for the
two autocorrelated processes).
<<echo=True>>=
# Compute criterion values for all (3*3) combinations of filters and periodograms
perf_mat<-matrix(nrow=3,ncol=3)
for (i in 1:3)
{
  for (j in 1:3)
  {
# Criterion value
    perf_mat[j,i]<-(2*pi/length(Gamma))*
    abs(Gamma-trffkt[,i])^2%*%periodogram[,j]
  }
}
dimnames(perf_mat)[[2]]<-c("filter: a1=0.9","filter: a1=0","filter: a1=-0.9")
dimnames(perf_mat)[[1]]<-c("periodogram: a1=0.9",
" periodogram: a1=0","periodogram: a1=-0.9")
@
The criterion values for all combinations of periodograms and filters are summarized in table \ref{perf_mat}.
<<label=table_perf_mat,echo=FALSE,results=tex>>=
  library(Hmisc)
  require(xtable)
  #latex(cor_vec, dec = 1, , caption = "Example of using latex to create table",
  #center = "centering", file = "", floating = FALSE)
  xtable(perf_mat, dec = 1,digits=rep(3,dim(perf_mat)[2]+1),
  paste("Criterion values for all 3*3 combinations of periodograms and filters",sep=""),
  label=paste("perf_mat",sep=""),
  center = "centering", file = "", floating = FALSE)
@
For a fixed periodogram (row $i$) the smallest criterion value is achieved for the corresponding filter in column $j=i$, as expected.


\item \label{tdc_e}Compare frequency-domain criterion values \ref{dfa_ms} and time-domain mean-square errors \ref{s_dfa}.
Hint: in order to perform time-domain MSE's We must
observe $y_1,...,y_{120}$. For this purpose We generate a long series $x_{-1000},...,x_{1000}$ of length $T=2001$ and apply a
truncated symmetric lowpass of length $2001-120$ for estimating $y_{1},...,y_{120}$\footnote{Fig.\ref{z_Gamma_a} in section
\ref{ilpe}
suggests that an approximation of order 2000 should be good.}.
<<echo=True>>=
# Define all relevant variables
set.seed(10)
lenh<-2000
len<-120
a_vec<-c(0.9,0,-0.9)
xh<-matrix(nrow=lenh,ncol=length(a_vec))
x<-matrix(nrow=len,ncol=length(a_vec))
plot_T<-F
yhat<-x
y<-x
periodogram<-matrix(ncol=3,nrow=len/2+1)
trffkt<-periodogram
# Generate series for each process
for (i in 1:length(a_vec))
{
  set.seed(10)
  xh[,i]<-arima.sim(list(ar=a_vec[i]),n=lenh)
}
# We extract 120 observations in the midddle of xh: these will be used
# for real-time filtering
x<-xh[lenh/2+(-len/2):((len/2)-1),]
# Compute the coefficients of the symmetric target filter
cutoff<-pi/6
# Order of approximation
ord<-1000
gamma<-c(cutoff/pi,(1/pi)*sin(cutoff*1:ord)/(1:ord))
# Compute the outputs yt of the symmetric target filter
for (i in 1:length(a_vec))
{
  for (j in 1:120)
  {
    y[j,i]<-gamma[1:900]%*%xh[lenh/2+(-len/2)-1+(j:(j-899)),i]+
    gamma[2:900]%*%xh[lenh/2+(-len/2)+(j:(j+898)),i]
  }
}
# Specify real-time filter settings
L<-12
Lag<-0
Gamma<-c(1,(1:(len/2))<len/12)
b<-matrix(nrow=L,ncol=3)
# Compute real-time filters
for (i in 1:3)
{
  periodogram[,i]<-per(x[,i],plot_T)$per
# Optimize filters
  filt<-dfa_ms(L,periodogram[,i],Lag,Gamma)
  trffkt[,i]<-filt$trffkt
  b[,i]<-filt$b
# Compute outputs: We can use the longer series in order to obtain
# outputs for j=1,...,len
  for (j in 1:len)
    yhat[j,i]<-filt$b%*%xh[lenh/2+(-len/2)-1+j:(j-L+1),i]
  perf_mat[i,i]<-(2*pi/length(Gamma))*abs(Gamma-trffkt[,i])^2%*%periodogram[,i]
}
# Compute time-domain MSE
mse<-apply(na.exclude((yhat-y))^2,2,mean)
file = paste("z_dfa_ar1_sym_output.pdf", sep = "")
pdf(file = paste(path.out,file,sep=""), paper = "special", width = 6, height = 6)
par(mfrow=c(3,1))
for (i in 1:3)
{
  ymin<-min(min(y[,i]),min(na.exclude(yhat)[,i]))
  ymax<-max(max(y[,i]),max(na.exclude(yhat)[,i]))
  ts.plot(yhat[,i],main=paste("Time-domain MSE = ",
  round(mse[i],3)," , Frequency-domain MSE = ",
  round(perf_mat[i,i],3),", a1 = ",a_vec[i],sep=""),col="blue",ylim=c(ymin,ymax))
  lines(y[,i],col="red")
  mtext("Real-time", side = 3, line = -1,at=len/2,col="blue")
  mtext("target", side = 3, line = -2,at=len/2,col="red")
}
dev.off()
@
<<label=z_dfa_ar1_sym_output.pdf,echo=FALSE,results=tex>>=
  file = paste("z_dfa_ar1_sym_output", sep = "")
  cat("\\begin{figure}[H]")
  cat("\\begin{center}")
  cat("\\includegraphics[height=6in, width=6in]{", file, "}\n",sep = "")
  cat("\\caption{Real-time filter output (blue) vs. targets (red) for a1=0.9 (top), a1=0 (middle) and a1=-0.9 (bottom)", sep = "")
  cat("\\label{z_dfa_ar1_sym_output}}", sep = "")
  cat("\\end{center}")
  cat("\\end{figure}")
@
We can see that the real-time estimates (blue) are substantially noisier than the target (red) and that they lag. The remaining
(undesirable) stop-band noise is due to the leaking amplitude functions in fig.\ref{z_dfa_ar1_amp_shift}, top-left panel; the lag corresponds to the
positive time-shifts in fig.\ref{z_dfa_ar1_amp_shift}, top-right panel. The time-domain MSE's
\[\frac{1}{T}\sum_{t=1}^T(y_t-\hat{y}_t)^2\]
are very close to the criterion values \ref{dfa_ms}, as can be seen in the headers of the above figure: minimizing
the \emph{observable} frequency-domain MSE is virtually the same as minimizing the (generally) \emph{unobservable} time-domain MSE.

\end{itemize}

\item \label{yhat_Lag}We now analyze the effect of $h$ ($h=-Lag$) on the real-time filters. Compute optimal finite sample filters for $Lag=0,...,L/2$ for
the above three processes and plot the resulting amplitude and time-shift functions, see fig.\ref{z_dfa_ar1_amp_shift_Lag}. Hint: We now use $L=13$ in order to obtain
a symmetric filter in $Lag=6$
<<echo=True>>=
L<-13
yhat_Lag<-array(dim=c(len,3,L/2+2))
trffkt<-array(dim=c(len/2+1,3,L/2+2))
b<-array(dim=c(L,3,L/2+2))
# Compute real-time filters for Lag=,...,L/2 and for the above three AR-processes
for (i in 1:3)
{
  periodogram[,i]<-per(x[,i],plot_T)$per
  for (Lag in 0:((L/2)+1))
  {
# Optimize filters
    filt<-dfa_ms(L,periodogram[,i],Lag,Gamma)
    trffkt[,i,Lag+1]<-filt$trffkt
    b[,i,Lag+1]<-filt$b
# Compute outputs
    for (j in L:len)
      yhat_Lag[j,i,Lag+1]<-filt$b%*%x[j:(j-L+1),i]
  }
}
omega_k<-pi*0:(len/2)/(len/2)
colo<-rainbow(L/2+2)
file = paste("z_dfa_ar1_amp_shift_Lag.pdf", sep = "")
pdf(file = paste(path.out,file,sep=""), paper = "special", width = 6, height = 6)
par(mfrow=c(3,4))
amp<-abs(trffkt)
shift<-Arg(trffkt)/omega_k
for (i in 1:3)
{
  ymin<-min(amp[,i,],na.rm=T)
  ymax<-max(amp[,i,],na.rm=T)
  plot(amp[,i,1],type="l",main=paste("Amplitude functions, a1 = ",a_vec[i],sep=""),
  axes=F,xlab="Frequency",ylab="Amplitude",col=colo[1],ylim=c(ymin,ymax))
  mtext("Lag=0", side = 3, line = -1,at=len/4,col=colo[1])
  for (j in 2:(L/2+2))
  {
    lines(amp[,i,j],col=colo[j])
    mtext(paste("Lag=",j-1,sep=""), side = 3, line = -j,at=len/4,col=colo[j])
  }
  axis(1,at=c(0,1:6*len/12+1),labels=c("0","pi/6","2pi/6","3pi/6","4pi/6","5pi/6","pi"))
  axis(2)
  box()
  ymin<-min(shift[,i,],na.rm=T)
  ymax<-max(shift[,i,],na.rm=T)
  plot(shift[,i,1],type="l",main=paste("Time-Shifts, a1 = ",a_vec[i],sep=""),
  axes=F,xlab="Frequency",ylab="Shift",col=colo[1],ylim=c(ymin,ymax))
  mtext("Lag=0", side = 3, line = -1,at=len/4,col=colo[1])
  for (j in 2:(L/2+2))
  {
    lines(shift[,i,j],col=colo[j])
    mtext(paste("Lag=",j-1,sep=""), side = 3, line = -j,at=len/4,col=colo[j])
  }
  axis(1,at=c(0,1:6*len/12+1),labels=c("0","pi/6","2pi/6","3pi/6","4pi/6","5pi/6","pi"))
  axis(2)
  box()
  ymin<-min(b[,i,],na.rm=T)
  ymax<-max(b[,i,],na.rm=T)
  plot(b[,i,1],col=colo[1],ylim=c(ymin,ymax),main=paste("Filter coefficients"),
  ylab="Output",xlab="lag",axes=F,typ="l")
  for (j in 2:(L/2+2))
  {
    lines(b[,i,j],col=colo[j],type="l")
    mtext(paste("Lag=",j-1,sep=""), side = 3, line = -j,at=L/2,col=colo[j])
  }
  axis(1,at=1:L,labels=-1+1:L)
  axis(2)
  box()
  ymin<-min(yhat_Lag[,i,],na.rm=T)
  ymax<-max(yhat_Lag[,i,],na.rm=T)
  ts.plot(yhat_Lag[,i,1],col=colo[1],ylim=c(ymin,ymax),main=paste("Output series"),
  ylab="Output")
  mtext("Lag=0", side = 3, line = -1,at=len/4,col=colo[1])
  for (j in 2:(L/2+2))
  {
    lines(yhat_Lag[,i,j],col=colo[j])
    mtext(paste("Lag=",j-1,sep=""), side = 3, line = -j,at=len/4,col=colo[j])
  }

}
dev.off()
@
<<label=z_dfa_ar1_amp_shift_Lag.pdf,echo=FALSE,results=tex>>=
  file = paste("z_dfa_ar1_amp_shift_Lag.pdf", sep = "")
  cat("\\begin{figure}[H]")
  cat("\\begin{center}")
  cat("\\includegraphics[height=6in, width=6in]{", file, "}\n",sep = "")
  cat("\\caption{Amplitude (left) and time-shift (right) functions as a function of Lag (rainbow) for
  a1=0.9 (top), a1=0 (middle) and a1=-0.9 (bottom)", sep = "")
  cat("\\label{z_dfa_ar1_amp_shift_Lag}}", sep = "")
  cat("\\end{center}")
  cat("\\end{figure}")
@
For better visual inspection We here focus attention on the `white noise' filter (DGP with $a_1=0$), see fig.\ref{z_dfa_ar1_amp_shift_Lag_0}.
<<echo=True>>=
file = paste("z_dfa_ar1_amp_shift_Lag_0.pdf", sep = "")
pdf(file = paste(path.out,file,sep=""), paper = "special", width = 6, height = 6)
par(mfrow=c(2,2))
amp<-abs(trffkt)
shift<-Arg(trffkt)/omega_k
for (i in 2:2)
{
  ymin<-min(amp[,i,],na.rm=T)
  ymax<-max(amp[,i,],na.rm=T)
  plot(amp[,i,1],type="l",main=paste("Amplitude functions, a1 = ",a_vec[i],sep=""),
  axes=F,xlab="Frequency",ylab="Amplitude",col=colo[1],ylim=c(ymin,ymax))
  mtext("Lag=0", side = 3, line = -1,at=len/4,col=colo[1])
  for (j in 2:(L/2+2))
  {
    lines(amp[,i,j],col=colo[j])
    mtext(paste("Lag=",j-1,sep=""), side = 3, line = -j,at=len/4,col=colo[j])
  }
  axis(1,at=c(0,1:6*len/12+1),labels=c("0","pi/6","2pi/6","3pi/6",
  "4pi/6","5pi/6","pi"))
  axis(2)
  box()
  ymin<-min(shift[,i,],na.rm=T)
  ymax<-max(shift[,i,],na.rm=T)
  plot(shift[,i,1],type="l",main=paste("Time-Shifts, a1 = ",a_vec[i],sep=""),
  axes=F,xlab="Frequency",ylab="Shift",col=colo[1],ylim=c(ymin,ymax))
  mtext("Lag=0", side = 3, line = -1,at=len/4,col=colo[1])
  for (j in 2:(L/2+2))
  {
    lines(shift[,i,j],col=colo[j])
    mtext(paste("Lag=",j-1,sep=""), side = 3, line = -j,at=len/4,col=colo[j])
  }
  axis(1,at=c(0,1:6*len/12+1),labels=c("0","pi/6","2pi/6","3pi/6",
  "4pi/6","5pi/6","pi"))
  axis(2)
  box()
  ymin<-min(b[,i,],na.rm=T)
  ymax<-max(b[,i,],na.rm=T)
  plot(b[,i,1],col=colo[1],ylim=c(ymin,ymax),main=paste("Filter coefficients"),
  ylab="Output",xlab="lag",axes=F,typ="l")
  for (j in 2:(L/2+2))
  {
    lines(b[,i,j],col=colo[j],type="l")
    mtext(paste("Lag=",j-1,sep=""), side = 3, line = -j,at=L/2,col=colo[j])
  }
  axis(1,at=1:L,labels=-1+1:L)
  axis(2)
  box()

  ymin<-min(yhat_Lag[,i,],na.rm=T)
  ymax<-max(yhat_Lag[,i,],na.rm=T)
  ts.plot(yhat_Lag[,i,1],col=colo[1],ylim=c(ymin,ymax),
  main=paste("Output series"),ylab="Output")
  for (j in 2:(L/2+2))
  {
    lines(yhat_Lag[,i,j],col=colo[j])
    mtext(paste("Lag=",j-1,sep=""), side = 3, line = -j,at=len/4,col=colo[j])
  }

}
dev.off()
@
<<label=z_dfa_ar1_amp_shift_Lag_0.pdf,echo=FALSE,results=tex>>=
  file = paste("z_dfa_ar1_amp_shift_Lag_0.pdf", sep = "")
  cat("\\begin{figure}[H]")
  cat("\\begin{center}")
  cat("\\includegraphics[height=6in, width=6in]{", file, "}\n",sep = "")
  cat("\\caption{Amplitude (left) and time-shift (right) functions as a function of Lag (rainbow colors) for
  the white noise process (a1=0)", sep = "")
  cat("\\label{z_dfa_ar1_amp_shift_Lag_0}}", sep = "")
  cat("\\end{center}")
  cat("\\end{figure}")
@
As expected, the time-shift (top-right) increases with increasing Lag (decreasing $h$).
For $Lag=(L-1)/2=6$ the filter is symmetric and therefore
the corresponding time-shift (violet line) is constant\footnote{The shift is constant (flat line) in the passband. The variable shift
in the stopband is an artifact of the Arg-function: the physical shift must be constant
since the filter weights (violet line bottom left
panel) are symmetric.}. The shift does not vanish because the filter is causal: coefficients are symmetric about Lag 6 (not zero), see
the violet line in the bottom-left panel. We can see that the output series
(bottom-right panel) are shifted accordingly: the larger the shift, the smoother the series. Indeed, We can observe, also, that
the amplitude functions get closer to zero in the stop-band, as
$Lag=-h$ increases. The stronger noise suppression is obtained by assigning more weight to past realizations as can be seen in
the bottom left panel: whereas the
real-time filter (red line bottom left panel) emphasizes the last observation, the $Lag=6$ symmetric filter assigns a more regular symmetric
weighting scheme centered around the middle coefficient $b_{(L-1)/2}=b_6$. The effect is that the real-time filter will be much faster
(much smaller time shift, see top-right panel) but noisier (the amplitude is leaking in the stop-band, top-left panel).

\item To conclude this series of exercises We compute finite-sample distributions of filter coefficients, of amplitude and of time-shift
functions and We analyze overfitting issues by comparing in-sample and out-of-sample characteristics
of the optimized filter as a function of the filter length $L$. For this purpose We focus on properties of the practically relevant
real-time filters $Lag=-h=0$ and We rely on the white noise process. Specifically We fit filters of lengths $L=3,6,12,24$ and $L=48$ and
analyze MSE's both in-sample as well as out-of-sample.
\begin{itemize}
\item Generate a simulation sample of 100 realizations of length 2000+2*120
\item For each realization: apply a symmetric filter of length  1999 to the data for $t=1000:1239$
\item Compute the periodogram on the sample  $t=1000:1119$ (in-sample length $T=120$)
\item Estimate real-time filters of lengths $L=3,6,12,24$ and $L=48$
\item Compute real-time filter outputs for $t=1000:1239$ (total sample-length 240: the last 120 observations are out-of-sample)
\item Determine in-sample mean-square errors $\frac{1}{120}\sum_{t=1000}^{1119}(\hat{y}_t^i-y_t)^2$ as well as
out-of-sample mean-square errors $\frac{1}{120}\sum_{t=1120}^{1239}(\hat{y}_t^i-y_t)^2$ where $y_t$ is the finite symmetric
filter and $\hat{y}_t^i$, $i=1,...,5$ are the outputs of the real-time filters of length $L=3,6,12,24$ and $L=48$
\item Store filter-coefficients and transfer functions in order to compute empirical distributions.
<<echo=True>>=
# Compute real-time filters of length 3,6,12,24 and 48
Lag<-0
L_vec<-c(3,6,12,24,48)
# Compute the coefficients of the symmetric target filter
cutoff<-pi/6
# Order of approximation
ord<-999
gamma<-c(cutoff/pi,(1/pi)*sin(cutoff*1:ord)/(1:ord))
# Determine full-length and in-sample length
len<-120
lenh<-2240
set.seed(10)
anzsim<-100
perf_outsample<-matrix(nrow=anzsim,ncol=length(L_vec))
perf_insample<-perf_outsample
b_sim<-as.list(1:anzsim)
b<-as.list(1:length(L_vec))
trffkt<-array(dim=c(len/2+1,length(L_vec),anzsim))
ymin<-1:length(L_vec)
ymax<-1:length(L_vec)
# Target in frequency-domain
Gamma<-c(1,(1:(len/2))<len/12)
#
# Start simulation
for (k in 1:anzsim)
{
# Full length data
  xh<-rnorm(lenh)
# in-sample data-set: used for estimation of periodogram
  x<-xh[(lenh/2-len):(lenh/2-1)]
  y<-x
# compute output of symmetric filter
  for (j in 1:(2*len))
  {
    y[j]<-gamma%*%xh[lenh/2-len-1+(j:(j-(ord)))]+
    gamma[2:ord]%*%xh[lenh/2-len+(j:(j+ord-2))]
  }
  yhat_insample<-array(dim=c(2*len,length(L_vec)))
# Compute Periodogram based on in-sample data
  per_wn<-per(x,plot_T)$per
  for (i in 1:length(L_vec))
  {
# Optimize filters in sample
    filt<-dfa_ms(L_vec[i],per_wn,Lag,Gamma)
    trffkt[,i,k]<-filt$trffkt
    b[[i]]<-filt$b
    if (k==1)
    {
      ymin[i]<-min(b[[i]])
      ymax[i]<-max(b[[i]])
    } else
    {
      ymin[i]<-min(ymin[i],min(b[[i]]))
      ymax[i]<-max(ymax[i],max(b[[i]]))
    }
# Compute the outputs in and out-of-sample
    for (j in 1:(2*len))#j<-1
      yhat_insample[j,i]<-b[[i]]%*%xh[lenh/2-len-1+(j:(j-(L_vec[i])+1))]
  }
# Store filter coefficients
  b_sim[[k]]<-b
# MSE performances: in and out-of-sample
  perf_insample[k,]<-apply(na.exclude(y[1:len]-yhat_insample[1:len,])^2,2,mean)
  perf_outsample[k,]<-
  apply(na.exclude(y[len+1:len]-yhat_insample[len+1:len,])^2,2,mean)
}

perf_mat<-rbind(apply(perf_insample,2,mean),apply(perf_outsample,2,mean))
dimnames(perf_mat)[[2]]<-paste("L=",L_vec,sep="")
dimnames(perf_mat)[[1]]<-c("In-sample","Out-of-sample")
@
<<label=perf_mat_dfa_i_o,echo=FALSE,results=tex>>=
  library(Hmisc)
  require(xtable)
  #latex(cor_vec, dec = 1, , caption = "Example of using latex to create table",
  #center = "centering", file = "", floating = FALSE)
  xtable(perf_mat, dec = 1,digits=rep(3,dim(perf_mat)[2]+1),
  paste("In- and out-of-sample MSE-performances of real-time trend extraction filters of length 3-48",sep=""),
  label=paste("perf_mat_dfa_i_o",sep=""),
  center = "centering", file = "", floating = FALSE)
@
MSE-performances of the simulation-study as summarized in table \ref{perf_mat_dfa_i_o} suggest that the optimal filter length
is $L=12$: for larger lengths overfitting affects (out-of-sample) performances. As expected, in-sample MSE's decrease
with increasing $L$\footnote{In fact this statement is not trivial since the DFA-optimization criterion is implemented in
the frequency-domain, whereas
Our performance measure is implemented in the time-domain. We just confirmed, once again, the close correspondence
between \ref{s_dfa} and \ref{dfa_ms}.}. As a result, the gap between in-sample and out-of-sample performances
opens for increasing $L$. However, out-of-sample performances of the filter with length $L=24$ are very close to the
best filter $L=12$ and therefore the design
seems quite robust since We have only 120 observations (10 years of monthly data) available.

\item Plot the empirical distribution of the filter coefficients, see fig.\ref{z_dfa_wn_b_dist}.
<<echo=True>>=
file = paste("z_dfa_wn_b_dist.pdf", sep = "")
pdf(file = paste(path.out,file,sep=""), paper = "special",
width = 6, height = 6)
par(mfrow=c(2,2))
for (i in 2:length(L_vec))
{
  b_opt<-dfa_ms(L_vec[i],rep(1,length(Gamma)),Lag,Gamma)$b
  ts.plot(b_sim[[1]][[i]],ylim=c(ymin[i],ymax[i]),
  main=paste("Filter coefficients L = ",L_vec[i],sep=""),xlab="lag",
  ylab="coefficients")
  for (j in 2:anzsim)
    lines(b_sim[[j]][[i]])
  lines(b_opt,col="red",lwd=2)
}
dev.off()
@
<<label=z_dfa_wn_b_dist.pdf,echo=FALSE,results=tex>>=
  file = paste("z_dfa_wn_b_dist.pdf", sep = "")
  cat("\\begin{figure}[H]")
  cat("\\begin{center}")
  cat("\\includegraphics[height=6in, width=6in]{", file, "}\n",sep = "")
  cat("\\caption{Empirical distribution of filter coefficients as a function of the filter length L: DGP is white noise (a1=0)", sep = "")
  cat("\\label{z_dfa_wn_b_dist}}", sep = "")
  cat("\\end{center}")
  cat("\\end{figure}")
@
The above figure illustrates the empirical distributions of the coefficients around the theoretical best real-time estimates (red lines)
assuming knowledge of the GDP (white noise). We observe that the estimates are unbiased. Also, coefficients
are not significantly different from zero for lags larger
than six (zero lies in the $\pm 2\sigma$ band for higher lags): this result is in conformity with the above
out-of-sample results (increased discrepancy between in-sample and out-of-sample performances for $L>6$: overfitting).
Interestingly, a damped periodic `structure' is built into the estimates as $L$
increases\footnote{The coefficients of the symmetric target filter are also a damped sinusoid
of frequency $\pi/6$ which coincides quite well with the above figure.}.

\item Plot the empirical distribution of amplitude and time-shift functions, see figs.\ref{z_dfa_wn_amp_dist} and
\ref{z_dfa_wn_shift_dist}.
<<echo=True>>=
file = paste("z_dfa_wn_amp_dist.pdf", sep = "")
pdf(file = paste(path.out,file,sep=""), paper = "special",
width = 6, height = 6)
par(mfrow=c(2,2))
for (i in 2:length(L_vec)) #i<-2
{
  trffkt_opt<-dfa_ms(L_vec[i],rep(1,length(Gamma)),Lag,Gamma)$trffkt
  ts.plot(abs(trffkt[,i,1]),ylim=c(min(abs(trffkt[,i,])),max(abs(trffkt[,i,]))),
  main=paste("Amplitude:  L = ",
  L_vec[i],sep=""),xlab="lag",ylab="coefficients")
  for (j in 2:anzsim)
    lines(abs(trffkt[,i,j]))
    lines(abs(trffkt_opt),col="red",lwd=2)
}
dev.off()
@
<<label=z_dfa_wn_amp_dist.pdf,echo=FALSE,results=tex>>=
  file = paste("z_dfa_wn_amp_dist.pdf", sep = "")
  cat("\\begin{figure}[H]")
  cat("\\begin{center}")
  cat("\\includegraphics[height=6in, width=6in]{", file, "}\n",sep = "")
  cat("\\caption{Empirical distribution of amplitude functions as a function of the filter length L: DGP is white noise (a1=0)", sep = "")
  cat("\\label{z_dfa_wn_amp_dist}}", sep = "")
  cat("\\end{center}")
  cat("\\end{figure}")
@
We observe that the variance of the distribution of the empirical amplitude-function increases substantially
with increasing $L$. This result certainly explains poorer out-of-sample properties of filters whose length equals
or exceeds $L=24$.

<<echo=True>>=
file = paste("z_dfa_wn_shift_dist.pdf", sep = "")
pdf(file = paste(path.out,file,sep=""), paper = "special",
width = 6, height = 6)
omega_k<-pi*0:(len/2)/(len/2)
par(mfrow=c(2,2))
for (i in 2:length(L_vec)) #i<-2
{
  trffkt_opt<-dfa_ms(L_vec[i],rep(1,length(Gamma)),Lag,Gamma)$trffkt
  ts.plot(Arg(trffkt[,i,1])/omega_k,ylim=c(min(na.exclude(Arg(trffkt[,i,])/omega_k)),max(na.exclude(Arg(trffkt[,i,])/omega_k))),
  main=paste("Shift:  L = ",L_vec[i],sep=""),xlab="lag",
  ylab="coefficients")
  for (j in 2:anzsim)
    lines(Arg(trffkt[,i,j])/omega_k)
    lines(Arg(trffkt_opt)/omega_k,col="red",lwd=2)
}
dev.off()
@
<<label=z_dfa_wn_shift_dist.pdf,echo=FALSE,results=tex>>=
  file = paste("z_dfa_wn_shift_dist.pdf", sep = "")
  cat("\\begin{figure}[H]")
  cat("\\begin{center}")
  cat("\\includegraphics[height=6in, width=6in]{", file, "}\n",sep = "")
  cat("\\caption{Empirical distribution of time-shift functions as a function of the filter length L: DGP is white noise (a1=0)", sep = "")
  cat("\\label{z_dfa_wn_shift_dist}}", sep = "")
  cat("\\end{center}")
  cat("\\end{figure}")
@
The empirical distribution of the time-shift confirms previous findings: the variance increases
for $L\geq 12$.


\end{itemize}

\end{enumerate}
In general, neither the theoretical MSE-criterion \label{ms0} nor the time-domain sample criterion \ref{s_dfa} are
observed\footnote{In order to approximate the time-domain criterion We had to extended the sample on both sides, recall exercise
\ref{tdc_e}.}, but the frequency-domain MSE \ref{dfa_ms} can be computed and We verified that the latter is very close to the
time-domain criterion \ref{s_dfa} (superconsistency). Also, We could observe the rich interaction between filter-characteristics and $-h$ (Lag parameter in the R-code)
which addresses the problem of smoothing and
\emph{revisions}, see next section. Finally, We emphasized and quantified the well-known overfitting problem and We analyzed
the empirical distribution of filter coefficients as well as of amplitude and time-shift functions, depending on the filter length $L$.
The MSE-filter is sensitive to overfitting for $L\geq 12$.




\subsection{Revisions: Releases, Vintage Triangle, Revision Error}\label{vintages_triangle_revision}

\subsubsection{Introduction}

Readers mainly interested in financial applications may skip this section without loss of information. The topic to be developped here
addresses publications of economic macro-indicators because the corresponding time series -- vintages, see below -- are subject to a `particular' non-stationarity
which is likely to mask `poor' real-time performances of such designs. Let's start then!\\

GDP-growth rates as released by the BEA\footnote{Bureau of Economic Analysis. The real-time data can be accessed
via the Philadelphia FED: \url{http://www.philadelphiafed.org/research-and-data/real-time-center/real-time-data/data-files/ROUTPUT/} (we
computed growth-rates based on the original level-data).}
are summarized in table \ref{US_GDP}:
<<echo=True>>=
US_GDP<-read.csv(paste(path.dat,"US_GDP.csv",sep=""),header=T)
US_GDP_wp<-read.csv(paste(path.dat,"US_GDP_wp.csv",sep=""),header=T,sep=";")
@
<<label=US_GDP,echo=FALSE,results=tex>>=
  library(Hmisc)
  require(xtable)
  #latex(cor_vec, dec = 1, , caption = "Example of using latex to create table",
  #center = "centering", file = "", floating = FALSE)
  xtable(US_GDP,
  paste("US-GDP: yearly vintages starting in Q1 2009 and ending in Q1 2013",sep=""),
  label=paste("US_GDP",sep=""),
  center = "centering", file = "", floating = FALSE)
@
Columns correspond to publication dates: the first column was published in the first quarter 2009 and the last column in the first quarter
2013. Rows correspond to historical time. The fifth row (2008:Q4) addresses GDP in the last quarter 2008: We see how the
initial estimate or \emph{first release} $\Sexpr{US_GDP_wp[5,2]}\%$ is \emph{revised} as new information in subsequent years flows in,
ending in a substantial negative growth of $\Sexpr{US_GDP_wp[5,6]}\%$ according to data up to the first quarter 2013. We learn from the above
example that
\begin{itemize}
\item Revisions of typical economic data may be substantial. In general We observe that the level of aggregation affects the magnitude of
revisions: GDP (strongly aggregated) is ordinarily less heavily revised than IPI (industrial production index), for example.
\item Revisions can extend over long time spans: in the above example We see that revisions are still `substantial' between
year `two' and year `three' after the initial release.
\end{itemize}
The origins of revisions are multiple. One of them is related to `filters': GDP-data is seasonally adjusted and early estimates rely on
`forecasts' of components which are not available at time of publishing. We here focus specifically on filter-induced revisions.

\subsubsection{Filter Revisions}

In analogy to the above GDP-table We want to estimate a target $y_{t}$ (say `true' GDP) based on data $x_1,...,x_{t-h}$: the index $t-h$
corresponds to the publication-date (the columns in the above table) and $t$ is historical time to which the target refers: $t$
shifts along the \emph{rows}. To be clear, hopefully, let us emphasize that $t$ and $h$ allow to `move' along rows and columns (diagonals)
in the table:
\begin{itemize}
\item For fixed $t$, a changing $h$ allows to move along the $t$-th row:
\begin{itemize}
\item for $h=0$ the \emph{initial release} (real-time estimate, nowcast)
of $y_t$ is obtained based on data $x_1,...,x_{t}$ up to $t$;
\item for $h=-1$ the \emph{second release} of $y_t$ is obtained based on data $x_1,...,x_{t+1}$ up to $t+1$: one future
observation $x_{t+1}$ is used and We shift from column $t$ to column $t+1$ but We stay in row $t$.
\item for arbitrary $h$ the $|h|$-th release of $y_t$ is obtained based on data $x_1,...,x_{t+|h|}$ up to $t+|h|$: $|h|$
future observations $x_{t+1},...,x_{t+|h|}$ are used. We stay in row $t$ but We shifted to column $t+|h|$.
\end{itemize}
\item For fixed $h$, a changing $t$ allows to move along the $|h|$-\emph{diagonal} of the matrix:
\begin{itemize}
\item For $h=0$ fixed, the target $y_t$, $t=1,...,T$ is estimated based on data $x_1,...,x_t$: We obtain the time series of first releases
(real-time estimates) along the first diagonal of the matrix.
\item For $h=-1$ fixed, the target $y_t$, $t=1,...,T-1$ is estimated based on data $x_1,...,x_{t+1}$: We obtain the time series of
second releases along the second diagonal of the matrix.
\item For arbitrary but fixed $h$, the target $y_t$, $t=1,...,T-|h|$ is estimated based on data $x_1,...,x_{t+|h|}$:
We obtain the time series of
$|h|$-releases along the $|h|$-th diagonal of the matrix.
\end{itemize}
\item Moving along \emph{columns} is also possible by changing $t$ \emph{and} $h$ simultaneously such that $t-h$ is fixed:
\begin{itemize}
\item For $t-h=1$ fixed, the target $y_t$ is restricted to $t=1$: $y_1$ then relies on $x_1$. Note that We do not compute forecasts
$y_2,y_3,...$ based on $x_1$ and therefore all corresponding column-elements are NA's.
\item For $t-h=2$ fixed, the target $y_t$ is restricted to $t=1,2$: $y_1$ and $y_2$ both rely on $x_1,x_2$. The estimate
for $y_1$ can rely on a future $x_2$: it is therefore the second release of $y_1$. The estimate of $y_2$ is the first (real-time)
release. All other elements in the second column are NA's.
\item For arbitrary but fixed $t-h=k$, the target $y_t$ is restricted to $t=1,2,...,k$: $y_1,...,y_k$ rely on data $x_1,x_2,...,x_k$.
The estimate for $y_1$ can rely on future $x_2,...,x_k$: it is therefore the $k$-th release of $y_1$. The estimate of $y_k$ is the first
(real-time) release. All other elements in the $k$-th column are NA's.
\end{itemize}
The column time series are called \emph{vintages}. Vintages are important because
many important economic series are revised: the users observes vintages of GDP or IPI or employment, for example.
\item The triangular shape of the table is due to the fact that We do not perform forecasts of the target.
\end{itemize}


Let's now explain how We obtain these estimates in Our filter framework i.e. how We access rows and columns in the table.\\

\textbf{The Initial Release}\\
For $h=0$ We want to estimate $y_t$ based on $x_1,...,x_t$ (nowcast, real-time estimate). For this purpose We optimize
coefficients $b_{k0}$, $k=0,...,L-1$ of the real-time filter
\[
\hat{y}_{t}^0:=\sum_{k=0}^{L-1}b_{k0}x_{t-k}
\]
The case $h=0$ corresponds to $Lag=0$ in fig.\ref{z_dfa_ar1_amp_shift_Lag_0}. The resulting time series
$\hat{y}_{t}^0$ corresponds to the first diagonal in table \ref{US_GDP}. \\


\textbf{The Second Release}\\
For $h=-1$ ($Lag=1$ in Our R-code) We want to estimate $y_{t}$ based on data $x_1,...,x_{t+1}$ up to $t+1$: We use one
future observation $x_{t+1}$. For this purpose We optimize
coefficients $b_{k,-1}$, $k=-1,...,L-2$ of the real-time filter
\[
\hat{y}_{t}^{-1}:=\sum_{k=-1}^{L-2}b_{k,-1}x_{t-k}
\]
The case $h=-1$ corresponds to $Lag=1$ in fig.\ref{z_dfa_ar1_amp_shift_Lag_0}. The resulting time series
$\hat{y}_{t}^{-1}$ corresponds to the second diagonal in table \ref{US_GDP}. \\

\textbf{The $|h|$-th Release}\\
For arbitrary $h<0$ ($Lag=-h>0$ in Our code) We want to estimate $y_{t}$ based on data $x_1,...,x_{t+|h|}$ up to $t+|h|$: We use
future observations $x_{t+1},...,x_{t+|h|}$. For this purpose We optimize
coefficients $b_{kh}$, $k=h,...,L-1+h$ of the real-time filter
\[
\hat{y}_{t}^{h}:=\sum_{k=h}^{L-1+h}b_{kh}x_{t-k}
\]
This corresponds to the case $Lag=-h$ in fig.\ref{z_dfa_ar1_amp_shift_Lag_0}, whereby the maximal considered
Lag was $(L+1)/2=7$. The resulting time series
$\hat{y}_{t}^{h}$ corresponds to the $|h|$-th diagonal in table \ref{US_GDP}. \\


\textbf{The Final Release}\\
The filter (revision-) sequence $Lag=0,...,7$ could have been stopped in $Lag=(L-1)/2=6$
because the resulting
filter is symmetric, see fig.\ref{z_dfa_ar1_amp_shift_Lag_0}, violet line, bottom-left panel.
The output $\hat{y}_{t}^{-(L-1)/2}$ of the
symmetric filter $Lag=(L-1)/2$ is called `final' release\footnote{The terminology `final' is borrowed from the fact that characteristics
of filters whose Lag-parameter exceeds $L/2$ are generally suboptimal
designs and therefore We refrain from increasing Lag or $-h$ beyond $L/2$.}.
In the above GDP-table \ref{US_GDP},
estimates are stabilized after
a revision-span of three years: the resulting estimates are `final'; note however that `true'
GDP (whatever this theoretical construct might be) does not have to coincide with this
final number. Accordingly,
the final estimate $\hat{y}_{t}^{-(L-1)/2}$ is not identical
with the target $y_t$ because the latter is the (unobservable) output of a \emph{bi-infinite} filter.\\

\textbf{Vintages}\\
The $j$-th vintage (the $j$-th column in table \ref{US_GDP}) is
obtained by estimating $y_t$, $t=1,...,j$ by
\begin{equation}\label{vintage}
\hat{y}_{t}^{t-j}:=\sum_{k=t-j}^{L-1+t-j}b_{k,t-j}x_{t-k}
\end{equation}
The resulting time series, the $j$-th vintage, is a \emph{non-stationary} time series because the filters $b_{k,t-j}$
depend on $t$: as an example, the last observation $\hat{y}_{j}^{0}$ is generally noisier than any previous observation $\hat{y}_{t}^{t-j}$
because $b_{k0}$ is the real-time filter (the amplitude of the real-time filter damps high-frequency noise less effectively
than filters with $Lag>0$, see fig.\ref{z_dfa_ar1_amp_shift_Lag_0}, top-left panel).\\


\subsubsection{Examples/Exercices} \label{tentacle_plot}


In analogy to the US-GDP table \ref{US_GDP}, vintage-data can be arranged in the so-called \emph{vintage triangle}. For illustration
We here compute the vintage-triangle for the three AR(1)-processes \ref{ar1_processes} based on expression \ref{vintage}, whereby we
us the filters for $Lag=0,...,6$ as computed in exercise \ref{yhat_Lag}, p.\pageref{yhat_Lag}, and plotted
in figs.\ref{z_dfa_ar1_amp_shift_Lag} and \ref{z_dfa_ar1_amp_shift_Lag_0}.
<<echo=True>>=
vintage<-array(dim=c(len,3,len))
dim(vintage)
# For each of the three AR(1)-processes We compute the vintage series
for (i in 1:3)
{
  for (j in L:len)#j<-L
  {
    vintage[(j-as.integer(L/2)):j,i,j]<-yhat_Lag[j,i,(as.integer(L/2)+1):1]
    vintage[1:(j-as.integer(L/2)-1),i,j]<-
    yhat_Lag[(as.integer(L/2)+1):(j-1),i,as.integer(L/2)+1]
  }
}
# We select the third DGP with a1=-0.9
i<-3
vintage_triangle<-vintage[,i,]
dimnames(vintage_triangle)[[2]]<-paste("Publ. ",1:len,sep="")
dimnames(vintage_triangle)[[1]]<-paste("Target ",1:len,sep="")
@
Table \ref{vintage_triangle}
shows the last six vintages (last six columns and rows) for $t=115,...,120$ for the third process ($a_1=-0.9$).
<<label=vintage_triangle,echo=FALSE,results=tex>>=
  library(Hmisc)
  require(xtable)
  #latex(cor_vec, dec = 1, , caption = "Example of using latex to create table",
  #center = "centering", file = "", floating = FALSE)
  xtable(vintage_triangle[(len-5):len,(len-5):len], dec = 1,digits=rep(3,dim(vintage_triangle[(len-5):len,(len-5):len])[2]+1),
  paste("Last 5 vintages of finite sample filter for the AR(1)-process with a1=-0.9: columns correspond to vintages and are indexed
  by corresponding publication dates; rows correspond to revisions of estimates for a fixed historical target date",sep=""),
  label=paste("vintage_triangle",sep=""),
  center = "centering", file = "", floating = FALSE)
@
In the last column on the right We see the last vintage $y_{120+h}^{h}$ for $h=0,-1,-2,-3,-4$: $y_{120}^{0}$
is the real-time estimate (first release)
of the target $y_{120}$ based on data $x_1,...,x_{120}$; $y_{119}^{-1}$ is the second release of the target $y_{119}$ based on data $x_1,...,x_{120}$
and so
on.  In analogy to table \ref{US_GDP}, the column-date in table
\ref{vintage_triangle} refers to the \emph{publication date} and the row-date refers to the
\emph{target time} i.e. the index $t$ of $y_t$. The initial release
in the first column (publication date 115) is $\Sexpr{round((vintage_triangle[(len-5):len,(len-5):len])[1,1],3)}$;  the second release
of the same target value $y_{115}$ in the second column is $\Sexpr{round((vintage_triangle[(len-5):len,(len-5):len])[1,2],3)}$;
the third release in the third column is $\Sexpr{round((vintage_triangle[(len-5):len,(len-5):len])[1,3],3)}$, and so on. The differences
between the various releases are due to revisions: previous estimates of  $y_{115}$ are up-dated when new information becomes available. We can
observe that the initial release $\Sexpr{round((vintage_triangle[(len-5):len,(len-5):len])[1,1],3)}$ is first revised upwards and then
downwards, ending in the slightly smaller value $\Sexpr{round((vintage_triangle[(len-5):len,(len-5):len])[1,6],3)}$ in the last column.




\textbf{Tentacle Plot}\\

We now plot the vintages obtained for the three AR(1)-processes \ref{ar1_processes}, see fig.\ref{z_vintages}: the
resulting graph is called a \emph{tentacle plot}.
<<echo=True>>=
colo<-rainbow(len)
file = paste("z_vintages.pdf", sep = "")
pdf(file = paste(path.out,file,sep=""), paper = "special", width = 6, height = 6)
par(mfrow=c(3,1))
for (i in 1:3)
{
  ymin<-min(vintage[,i,],na.rm=T)
  ymax<-max(vintage[,i,],na.rm=T)
  ts.plot(vintage[,i,L],col=colo[1],ylim=c(ymin,ymax),
  main=paste("Tentacle plot: vintages and full revision sequence,
  a1 = ",a_vec[i],sep=""),ylab="Vintages")
  for (j in (L+1):len)
  {
    lines(vintage[,i,j],col=colo[j])
  }
  lines(vintage[,i,len],col="red",lwd=2)
}
dev.off()
@
<<label=z_vintages.pdf,echo=FALSE,results=tex>>=
  file = paste("z_vintages.pdf", sep = "")
  cat("\\begin{figure}[H]")
  cat("\\begin{center}")
  cat("\\includegraphics[height=6in, width=6in]{", file, "}\n",sep = "")
  cat("\\caption{Tentacle plot: full historical revision sequence for
  a1=0.9 (top), a1=0 (middle) and a1=-0.9 (bottom). Final release is emphasized in bold red", sep = "")
  cat("\\label{z_vintages}}", sep = "")
  cat("\\end{center}")
  cat("\\end{figure}")
@
By illustrating the stepwise convergence of initial to final releases, the main revision effects are summarized in a handy and very effective way.
Ideally, the revision error would vanish resulting in a single line in the tentacle plot. Obviously this is not the case for
the three AR(1)-processes in the above plot: the larger the revisions -- the more difficult the
real-time problem -- the wider apart the tentacles.
We first observe that the magnitude of the revision error seems to depend on the DGP: it is smallest for the positively
autocorrelated process ($a_1=0.9$) and largest
for the negatively autocorrelated DGP ($a_1=-0.9$) although We should emphasize that We here effectively address a \emph{relative} effect, namely
the magnitude of the revisions relative to the strength of the signal. This result should not come as a surprise since We know that
the filter for the first process does not damp high-frequency noise as strongly as the filter for the third process, see fig.\ref{z_dfa_ar1_amp_shift_Lag}:
the signal-extraction task is less demanding when the process is positively autocorrelated, see fig.\ref{z_dfa_ar1_output}.
In order to discuss the topic more thoroughly We focus on the second (white noise) process, see fig.\ref{z_vintages_2} whose specification
is close to typical finance data.
<<echo=True>>=
file = paste("z_vintages_2.pdf", sep = "")
pdf(file = paste(path.out,file,sep=""), paper = "special", width = 6, height = 6)
par(mfrow=c(2,1))
i<-2
ymin<-min(vintage[,i,],na.rm=T)
ymax<-max(vintage[,i,],na.rm=T)
ts.plot(vintage[,i,L],col=colo[1],ylim=c(ymin,ymax),
main="Vintages: full revision sequence and final release (black)",ylab="Vintages")
for (j in (L+1):len)
{
  lines(vintage[,i,j],col=colo[j])
}
lines(vintage[,i,len],col="black",lwd=2)
i<-2
ymin<-min(vintage[,i,],na.rm=T)
ymax<-max(vintage[,i,],na.rm=T)
ts.plot(vintage[,i,L],col=colo[1],ylim=c(ymin,ymax),
main="Vintages: full revision sequence and real-time initial release (black)",
ylab="Vintages")
for (j in (L+1):len)
{
  lines(vintage[,i,j],col=colo[j])
}
lines(yhat_Lag[,i,1],col="black",lty=1)
dev.off()
@
<<label=z_vintages_2.pdf,echo=FALSE,results=tex>>=
  file = paste("z_vintages_2.pdf", sep = "")
  cat("\\begin{figure}[H]")
  cat("\\begin{center}")
  cat("\\includegraphics[height=6in, width=6in]{", file, "}\n",sep = "")
  cat("\\caption{Vintages: full historical revision sequence in the case of the white noise process", sep = "")
  cat("\\label{z_vintages_2}}", sep = "")
  cat("\\end{center}")
  cat("\\end{figure}")
@
The two graphs differ in the sense that We emphasize in bold (black) either the last vintage (top graph) or the first release (bottom graph).
Otherwise both plots are identical. Convergence to the final release is achieved after $(L-1)/2=6$ time steps in Our setting.
For typical macro-data the revision span is generally  longer: as an example US-GDP in table \ref{US_GDP} is still
substantial revised after after two years or eight quarters.
The real-time \emph{initial} release emphasized in the bottom graph (black) is obtained by linking the end-points of the
tentacles. The second release would be obtained by linking the next-to-last endpoints, and so on.
Typically, early releases are \emph{noisier}
(weaker noise suppression: leaking amplitude function) and \emph{delayed}
(positive time-shift), as confirmed and quantified by Our earlier
amplitude and time-shift plots figs.\ref{z_dfa_ar1_amp_shift_Lag} and \ref{z_dfa_ar1_amp_shift_Lag_0}.
Therefore \textbf{turning-points cannot be detected accurately in real-time}:
one has to wait several time-units (days, weeks, months, quarters) until a trend reversal can
be assessed with sufficient confidence. As an example,
the \emph{revised} history of most economic indicators dates the great recession (Dec07-June09) timely and accurately; in \textbf{real-time}, however,
indicators were substantially delayed, some designs by more than  a year, and early warnings were heavily revised
i.e. pretty unreliable.





\subsection{Customization}\label{custo}


The above examples and in particular fig.\ref{z_dfa_ar1_amp_shift_Lag_0} illustrated a fundamental tradeoff
between the ability of suppressing noise and the magnitude of the time-shift of a filter\footnote{One can
derive an analytic link between phase and amplitude functions which shows that either function can be derived explicitly
from knowing the other.
We here omit discussion presentation and discussion of this result because it would take us too far away from the
main topic: forecasting and real-time signal extraction.}. In fact We used this interaction between amplitude and time-shift
functions when implementing revision-sequences since higher-lag filters are able to suppress noise better: therefore the
history of a filtered time series could be `cosmetically enhanced' (smoothed). \\

The ordinary
mean-square paradigm -- the optimization criterion \ref{dfa_ms} -- proposes a particular option for the tradeoff between
`speed' and `smoothness', as illustrated in Our previous examples.
However, in practice users may assign priorities differently: a manager of a pension-fund
may prefer  `reliability' (strong noise suppression) whereas a high-frequency trader might favour
`speed' (small time-shift), instead. In terms of revisions-sequences: the former user might be willing to wait until a clear `revised'
signal emerges (loosing on early but potentially risky opportunities) whereas the latter user
might jump on `early releases' in order to take profit of potential rebounds (at higher-volatility risk levels). Instead
of the somehow rigid (mean-square) revision framework We here propose novel estimation algorithms which can account specifically
for a broad range of practically relevant user priorities by emphasizing specifically the observed `speed-noise' or
timeliness-smoothness dilemma. In fact, We shall propose a much richer three dimensional tradeoff, the so-called
\emph{Accuracy-Timeliness-Smootness
trilemma} (ATS for short) which will allow to improve on any single or any pair of ATS-components, at costs of the remaining one(s):
as an example We will show how to improve on Timeliness \emph{and} Smoothness simultaneously,
when benchmarked against the ordinary MSE-design.\\

In short: Customization will allow to implement user-preferences directly into the optimization criterion, through
a suitable interface, in order to match \emph{performances} and \emph{research priorities}, better than by the
ordinary and somehow rigid MSE-paradigm.




\subsubsection{Decomposition of the MS-Criterion}


The following identity holds for general transfer functions $\Gamma$ and $\hat{\Gamma}$:
\begin{eqnarray}
|\Gamma(\omega)-\hat{\Gamma}(\omega)|^2&=&A(\omega)^2+\hat{A}(\omega)^2-2A(\omega)
\hat{A}(\omega)\cos\left(\hat{\Phi}(\omega)-\Phi(\omega)\right)\nonumber\\
&=&
(A(\omega)-\hat{A}(\omega))^2\nonumber\\
&&+2A(\omega)\hat{A}(\omega)\left[1-\cos\left(\hat{\Phi}(\omega)-\Phi(\omega)\right)\right]\label{etrigid}
\end{eqnarray}
If We assume that $\Gamma$ is symmetric and positive, then \(\Phi(\omega)\equiv
0\). Inserting \ref{etrigid} into \ref{dfa_ms} and using
$1-\cos(\hat{\Phi}(\omega))=2\sin(\hat{\Phi}(\omega)/2)^2$ then leads to
\begin{eqnarray} &&\frac{2\pi}{T}\sum_{k=-[T/2]}^{[T/2]}\left|\Gamma(\omega_k)-\hat{\Gamma}(\omega_k) \right|^2 I_{TX}(\omega_k)\nonumber\\
&=&\frac{2\pi}{ T} \sum_{k=-T/2}^{T/2}
(A(\omega_k)-\hat{A}(\omega_k))^2 I_{TX}(\omega_k)\label{unboptidioe}\\
&&+\frac{2\pi}{ T} \sum_{k=-T/2}^{T/2}
4A(\omega_k)\hat{A}(\omega_k)\sin(\hat{\Phi}(\omega_k)/2)^2
I_{TX}(\omega_k)\label{unboptidio}
\end{eqnarray}
The first summand \ref{unboptidioe} is the distinctive part of the total mean-square filter error
which is attributable to the amplitude function of the real-time filter (the MS-amplitude error).
The second summand \ref{unboptidio} measures the distinctive contribution of the phase or time-shift to
the total mean-square error (the MS-time-shift error). The term
$A(\omega_k)\hat{A}(\omega_k)$ in \ref{unboptidio} is a `scaling' factor which accounts for the fact that
the phase function is dimensionless i.e. it does not convey level information.

\subsubsection{Accuracy, Timeliness and Smoothness (ATS-) Error-Components}  \label{ats_section}



The above decomposition into amplitude and time-shift error contributions can be refined further by splitting
each term into contributions from pass and stopbands. For ease of exposition We assume an ideal lowpass target with
a specified cutoff-frequency delimiting both frequency-bands\footnote{Pass- and stopbands can be defined similarly for
ideal bandpass filters or for any interesting target such as, for example, HP- or CF-filters or more general model-based
designs.}: $|\omega_k|<\textrm{cutoff}$ then corresponds to the passband and its complement is the stop- or rejection-band.

\begin{eqnarray}
\left.\begin{array}{ccc}
\textrm{A(ccuracy)}&:=&\frac{2\pi}{ T} \sum_{\textrm{Passband}} (A(\omega_k)-\hat{A}(\omega_k))^2 I_{TX}(\omega_k)\\
\textrm{S(moothness)}&:=&\frac{2\pi}{ T} \sum_{\textrm{Stopband}} (A(\omega_k)-\hat{A}(\omega_k))^2 I_{TX}(\omega_k)\\
\textrm{T(imeliness)}&:=&\frac{2\pi}{ T}  \sum_{\textrm{Passband}} 4A(\omega_k)\hat{A}(\omega_k)\sin(\hat{\Phi}(\omega_k)/2)^2
I_{TX}(\omega_k)                                     \\
\textrm{R(esidual)}&:=&\frac{2\pi}{ T}  \sum_{\textrm{Stopband}} 4A(\omega_k)\hat{A}(\omega_k)\sin(\hat{\Phi}(\omega_k)/2)^2
I_{TX}(\omega_k)
\end{array}\right\}\label{mse_dec_ats}
\end{eqnarray}
\begin{itemize}
\item Accuracy measures the contribution to the MSE-norm if the time-shift (in the passband) and the noise suppression (in the stop-band)
are ignored. This would correspond to the performance of a symmetric filter (no time-shift) with perfect noise suppression ($\hat{A}(\cdot)=0$ in
the stopband) and with the same amplitude as the considered real-time filter in the passband. Such a design could be easily constructed by
the techniques presented in the first section (apply the inverse Fourier-transform to the specified function).
\item Smoothness measures the contribution to total MSE of the undesirable high-frequency noise, leaking
through the imperfect amplitude function (convolution theorem) in the stopband of
the real-time filter. As We shall see, this statistic is closely linked to a well-known `curvature' or smoothness measure in
the time-domain (mean-square second order differences), see section \ref{tdpm}.
\item Timeliness measures the MSE-contribution generated by the time-shift. As We shall see, this `measure' is closely linked
to a well-known time-domain concept referred to as `peak-correlation', see section \ref{tdpm}.
\item The Residual is that part of the MSE which is not attributable to any of the above error-components: it corresponds to
a time-shift contribution from the stopband which is generally irrelevant (it
cannot be identified with a well-known user-priority) and negligible. In Our case -- ideal lowpass target -- this error term
vanishes because $\Gamma(\omega_k)=A(\omega_k)=0$ in the stopband.
\end{itemize}
From now on We ignore the Residual and consider the ATS-decomposition of the MSE-norm.


\subsubsection{General Customized Criterion}


Based on the above ATS-decomposition
\begin{equation}\label{ats_mse}
\textrm{MSE=Accuracy+Timeliness+Smoothness}
\end{equation}
We are in a position to generalize the mean-square optimization criterion \ref{dfa_ms}
by assigning weights to the ATS-components:
\begin{equation}\label{ats_cust}
\textrm{MSE-Cust}(\lambda_1,\lambda_2)=\textrm{Accuracy}+(1+\lambda_1) \textrm{Timeliness}+(1+\lambda_2) \textrm{Smoothness}\to \min_{\mathbf{b}}
\end{equation}
Selecting $\lambda_1>0$ would emphasize the contribution by the Timeliness component: the resulting
optimal filter would have a smaller Timeliness error contribution; selecting
$\lambda_2>0$ would magnify Smoothness issues and the resulting filter would have a smaller
Smoothness error; finally, selecting $\lambda_1>0,\lambda_2>0$ would
emphasize both dimensions simultaneously: depending on the relative magnitudes of
$\lambda_1,\lambda_2$ either or \emph{both} error contributions could be reduced; at
costs of the Accuracy-component whose loss must overcompensate potential gains of
Timeliness and/or Smoothness.\\
To be clear, let us emphasize the latter point: the MSE-filter minimizes
\ref{ats_mse}; therefore any other design must lead to a higher
criterion value (uniqueness of the minimum of a quadratic function); as a result, the Accuracy component
of an alternative filter with smaller Timeliness and Smoothness error-contributions must necessarily
\emph{over}compensate the latter gains. This disproportionate contribution might put the Accuracy-term
`under stress' and therefore arbitrary performance-gains -- by immoderate disequilibria (large $\lambda_1,\lambda_2$) -- are in the realm of
fiction and desire; nonetheless, effective true \emph{out-of-sample} gains are permissible
in terms of Smoothness \emph{and} Timeliness \emph{simultaneously}, in the frequency-domain, as well as in terms of
improved peak-correlation and curvature
performances, as measured in the time-domain. \\


We now briefly generalize the above schematic criterion \ref{ats_cust} by introducing weighting functions $W_1(\omega_k)\geq 0$ and
$W_2(\omega_k)\geq 0$ in \ref{ats_mse}:
\begin{eqnarray*}
&&\frac{2\pi}{ T} \sum_{\textrm{Passband}} (A(\omega_k)-\hat{A}(\omega_k))^2 I_{TX}(\omega_k)\nonumber\\
&&+\frac{2\pi}{ T} \sum_{\textrm{Stopband}} (A(\omega_k)-\hat{A}(\omega_k))^2 W_2(\omega_k)I_{TX}(\omega_k)\nonumber\\
&&+\frac{2\pi}{ T}  \sum_{\textrm{Passband}} 4A(\omega_k)\hat{A}(\omega_k)\sin(\hat{\Phi}(\omega_k)/2)^2
W_1(\omega_k)I_{TX}(\omega_k)\to\min_{\mathbf{b}}
\end{eqnarray*}
where We omitted the (vanishing or negligible) Residual-component. The schematic criterion \ref{ats_cust} could be replicated
by setting
\begin{eqnarray*}
W_1(\omega_k,\lambda_1)&=&1+\lambda_1\\
W_2(\omega_k,\lambda_2)&=&1+\lambda_2
\end{eqnarray*}
Note, however, that $W_2$ as defined above introduces an undesirable discontinuity in the cutoff-frequency since
frequency-components on the left (in the passband) are unweighted (weight `one') whereas components on the right (stopband) are magnified
by $1+\lambda_2$ (this effect does not apply to $W_1$ because We neglected the Residual).
We now depart from the schematic simplification \ref{ats_cust} and propose the following refined weighting
scheme
\begin{eqnarray}
\left.\begin{array}{ccc}W_1(\omega_k,\lambda)&=&1+\lambda\\
W_2(\omega_k,\eta)&=&(1+|\omega_k|-\textrm{cutoff})^{\eta}
\end{array}\right\}\label{w_func_cust}
\end{eqnarray}
The new function $W_2$ proposes a continuous weighting scheme whose importance increases monotonically as a function
of $\omega_k$: higher frequencies are affected more heavily than lower frequencies in the stopband if $\eta>0$: this is
a welcome asset in many real-time applications\footnote{The derivative of a trigonometric signal is proportional to frequency. Therefore
the impact of undesirable high-frequency components (noise) on `smoothness' is dependent upon frequency. The proposed
weighting scheme specifically accounts for corresponding user-requirements; better than  \ref{ats_cust}.}.
Note that We re-labelled
$\lambda_1,\lambda_2$ by $\lambda,\eta$ in order to distinguish symbolically both effects: $\lambda>0$ emphasizes the
Timeliness-component whereas $\eta>0$ magnifies Smoothness-issues. The above criterion can be re-written in its original
(`historical') more compact form as
\begin{eqnarray}
&&\frac{2\pi}{ T} \sum_{\textrm{All~Frequencies}} (A(\omega_k)-\hat{A}(\omega_k))^2 W(\omega_k,\eta) I_{TX}(\omega_k)\nonumber\\
&&+(1+\lambda)\frac{2\pi}{ T}  \sum_{\textrm{All~Frequencies}} 4A(\omega_k)\hat{A}(\omega_k)\sin(\hat{\Phi}(\omega_k)/2)^2
W(\omega_k)I_{TX}(\omega_k)\to\min_{\mathbf{b}}\label{dfatp}
\end{eqnarray}
where
\begin{equation}\label{w}
W(\omega_k,\eta,\textrm{cutoff})=\left\{\begin{array}{cc}
1~,~\textrm{if~} |\omega_k|<\textrm{cutoff}\\
(1+|\omega_k|-\textrm{cutoff})^{\eta}~,~\textrm{otherwise}
\end{array}\right.
\end{equation}
Since $\Gamma(\cdot)=0$, the stopband contributions of the phase-error in the second sum (Residual) vanishes;
also, We could skip $W(\omega_k)$ in this
expression since the weighting function is one in the passband. The above expression \ref{dfatp} was first proposed in Wildi (2005); we
here wanted to establish a link to the ATS-decomposition \ref{ats_mse} of the MSE-norm, as proposed in McElroy and Wildi (2012).



\begin{itemize}
\item Classical mean-square optimization is obtained for $\lambda=\eta=0$:
the revision error is addressed. This setting is ideal when implementing a revision-sequence (filling a vintage triangle).
\item For $\lambda>0$ the user can emphasize the
contribution of the time-shift error. As a result, the customized filter output will gain in `speed' (improved
peak-correlation performances).
\item The parameter $\eta$ emphasizes noise-suppression in the stopband. As a result, the customized filter
output  will gain in smoothness (smaller curvature).
\end{itemize}
It incumbes upon the user to address specific research priorities, in terms of ATS-components, by suitable choices
of $\lambda$ and $\eta$. Mean-square performances are obtained by selecting $\lambda=\eta=0$. The general optimization
principle \ref{dfatp} is called a \textbf{Customized Criterion}. In Our applications We invariably rely on weighting functions
as defined by \ref{w} though experienced users might shape the weighting-scheme according to more sophisticated
or more specific priorities.




\subsubsection{I-DFA: Numerically Tractable $\textrm{Customization}^{*}$}\label{idfas}

The mean-square error criterion is a quadratic function of the filter parameters and therefore
the solution can be obtained analytically. Unfortunately, \ref{dfatp} is a non-linear function of the coefficients if
$\lambda>0$. Therefore, We here propose an alternative customized criterion which is tractable analytically.
Consider the following expression:
\begin{equation}\label{idfa}
\frac{2\pi}{T} \sum_{k=-[T/2]}^{[T/2]}
 \left|\Gamma(\omega_k)-\left\{\Re\left(\hat{\Gamma}(\omega_k)\right)+i\sqrt{1+4\lambda\Gamma(\omega_k)}
 \Im\left(\hat{\Gamma}(\omega_k)\right)\right\}\right|^2 W(\omega_k,\eta)I_{TX}(\omega_k)\to\min
\end{equation}
where $\Re(\cdot)$ and $\Im(\cdot)$ denote real and imaginary parts and $i^2=-1$ is the imaginary unit.
We call the resulting approach I-DFA\footnote{The capital I in the acronym stands for the imaginary part which
is emphasized by $\lambda$.}. Obviously, the above expression is quadratic in the filter coefficients.
In analogy to \ref{dfatp}, the weighting function $W(\omega_k,\eta)$ emphasizes the fit  in the
stop band. The term $\lambda\Gamma(\omega_k)$ emphasizes the imaginary part of the real-time filter
in the pass band: for $\lambda>0$ the imaginary part is artificially inflated and therefore the phase
is affected. The following development allows for a direct comparison of \ref{dfatp} and \ref{idfa}:
\begin{eqnarray}
&&\frac{2\pi}{T} \sum_{k=-[T/2]}^{[T/2]}
 \left|\Gamma(\omega_k)-\Big(\Re\left(\hat{\Gamma}(\omega_k)\right)+i\sqrt{1+4\lambda\Gamma(\omega_k)}\Im\left(\hat{\Gamma}(\omega_k)\right)\Big)\right|^2 W(\omega_k,\eta)I_{TX}(\omega_k)\label{idfa_t}\\
&=&\frac{2\pi}{T} \sum_{k=-[T/2]}^{[T/2]}
  \left\{\left(\Gamma(\omega_k)-\Re\left(\hat{\Gamma}(\omega_k)\right)\right)^2+\Im\left(\hat{\Gamma}(\omega_k)\right)^2\right\}W(\omega_k,\eta)I_{TX}(\omega_k)\nonumber\\
&&+4\lambda\frac{2\pi}{T} \sum_{k=-[T/2]}^{[T/2]}
  \Gamma(\omega_k)\Im\left(\hat{\Gamma}(\omega_k)\right)^2W(\omega_k,\eta)I_{TX}(\omega_k) \nonumber\\
&=&\frac{2\pi}{T} \sum_{k=-[T/2]}^{[T/2]}
 |\Gamma(\omega_k)-\hat{\Gamma}(\omega_k)|^2 W(\omega_k,\eta)I_{TX}(\omega_k)\nonumber\\
&&+4\lambda\frac{2\pi}{T} \sum_{k=-[T/2]}^{[T/2]}
  A(\omega_k)\hat{A}(\omega_k)^2\sin(\hat{\Phi}(\omega_k))^2W(\omega_k,\eta)I_{TX}(\omega_k)\label{idfatp}
\end{eqnarray}
A direct comparison of \ref{dfatp} and \ref{idfatp} reveals that $\hat{\Phi}(\omega_k)/2$ is replaced
by $\hat{\Phi}(\omega_k)$ and a supernumerary weighting-term $\hat{A}(\omega_k)$ appears in the latter
expression. Expression \ref{idfatp} can be solved analytically for arbitrary $\lambda$ and/or weighting
functions $W(\omega_k,\eta)$ because $\hat{A}(\omega_k)^2\sin(\hat{\Phi}(\omega_k))^2$ is simply the
squared imaginary part of the real-time filter (i.e. it is a quadratic function of the filter coefficients).
For $\lambda=\eta=0$ the original (DFA) mean-square
criterion \ref{dfa_ms} is obtained. Overemphasizing the imaginary part of the real-time filter
in the pass-band by augmenting $\lambda>0$ results in filters with smaller time-shifts, as desired.\\

\textbf{Summary}\\

The I-DFA criterion \ref{idfa} replicates perfectly the ordinary MSE-criterion
\ref{dfa_ms} when $\lambda=\eta=0$. In contrast to \ref{dfatp},  it can be solved analytically, in closed form, for any $(\lambda,\eta)$-specification:
the resulting algorithm is \emph{fast} and solutions are \emph{unique}. It sligthly
differs from \ref{dfatp} but the effects of $\lambda$ and $\eta$ on Timeliness and Smoothness are similar.
Therefore, from a practical perspective We assign preference
to the customized I-DFA criterion \ref{idfa} or \ref{idfatp}.




\subsubsection{R-code: I-DFA}


We implement the generalized criterion \ref{idfa} or, equivalently, \ref{idfatp} in R and define a corresponding new
function called $dfa\textunderscore analytic()$:

<<echo=True>>=
# This function computes analytic DFA-solutions
# L is the length of the MA-filter,
# weight_func is the periodogram,
# lambda emphasizes phase artifacts in the customized criterion,
# eta emphasizes noise-suppression/smoothness
# Gamma is the transferfunction of the symmetric filter (target) and
# Lag is the lag-parameter: Lag=0 implies real-time filtering, Lag=L/2
#     implies symmetric filter
# i1 and i2 allow for filter restrictions in frequency zero
# The function returns the weights of the MA-Filter as well as its transferfunction
#
#
dfa_analytic<-function(L,lambda,weight_func,Lag,Gamma,eta,cutoff,i1,i2)
{
  K<-length(weight_func)-1
# Define the amplitude weighting function weight_h (W(omega_k))
  omega_Gamma<-as.integer(cutoff*K/pi)
  if ((K-omega_Gamma+1)>0)
  {
    weight_h<-weight_func*(c(rep(1,omega_Gamma),(1:(K-omega_Gamma+1))^(eta)))
  } else
  {
    weight_h<-weight_func*rep(1,K+1)
  }
# First order filter restriction: assigning a `large' weight to frequency zero
  if (i1)
    weight_h[1]<-max(1.e+10,weight_h[1])

  X<-exp(-1.i*Lag*pi*(0:(K))/(K))*rep(1,K+1)*sqrt(weight_h)
  X_y<-exp(-1.i*Lag*pi*(0:(K))/(K))*rep(1,K+1)
  if (i2)
  {
# Second order restriction: time shift in frequency zero vanishes
    for (l in 2:(L-1))
    {
      X<-cbind(X,(cos((l-1-Lag)*pi*(0:(K))/(K))-((l-1)/(L-1))*
      cos((L-1-Lag)*pi*(0:(K))/(K))+
      sqrt(1+Gamma*lambda)*1.i*(sin((l-1-Lag)*pi*(0:(K))/(K))-((l-1)/(L-1))*
      sin((L-1-Lag)*pi*(0:(K))/(K))))*sqrt(weight_h))
      X_y<-cbind(X_y,(cos((l-1-Lag)*pi*(0:(K))/(K))-((l-1)/(L-1))*
      cos((L-1-Lag)*pi*(0:(K))/(K))+
      1.i*(sin((l-1-Lag)*pi*(0:(K))/(K))-((l-1)/(L-1))*sin((L-1-Lag)*pi*(0:(K))/(K)))))
    }
    xtx<-t(Re(X))%*%Re(X)+t(Im(X))%*%Im(X)
  # MA-Filterweights
    b<-as.vector(solve(xtx)%*%(t(Re(X_y))%*%(Gamma*weight_h)))
# the last weight is a function of the previous ones through the second order restriction
    b<-c(b,-sum(b*(0:(length(b)-1)))/(length(b)))
  } else
  {
    for (l in 2:L)
    {
      X<-cbind(X,(cos((l-1-Lag)*pi*(0:(K))/(K))+
      sqrt(1+Gamma*lambda)*1.i*sin((l-1-Lag)*pi*(0:(K))/(K)))*sqrt(weight_h))
      X_y<-cbind(X_y,(cos((l-1-Lag)*pi*(0:(K))/(K))+
      1.i*sin((l-1-Lag)*pi*(0:(K))/(K))))
    }
    xtx<-t(Re(X))%*%Re(X)+t(Im(X))%*%Im(X)
  # MA-Filterweights
    b<-as.vector(solve(xtx)%*%(t(Re(X_y))%*%(Gamma*weight_h)))
  }
  # Transferfunction
  trffkt<-1:(K+1)
  trffkt[1]<-sum(b)
  for (k in 1:(K))#k<-1
  {
    trffkt[k+1]<-(b%*%exp(1.i*k*(0:(length(b)-1))*pi/(K)))
  }
  return(list(b=b,trffkt=trffkt))
}
@




\section{ATS-Trilemma}

When addressing complex decision problems, people often face bipolar choices based on fundamental (`natural')
tradeoffs. One cannot expect to progress on two desirable but antagonistic dimensions at the same time: the
`debt' vs. `economic-growth' debate stigmatizes such a dilemma in politics (I did not say `economics').
In sciences in general and in statistics specifically such fundamental tradeoffs (uncertainty-principle, bias variance dilemma)
offer exciting structural and philosophical insights into the considered subject matter and beyond. Time series analysis brings its own
`home-brew' tradeoff, the timeliness-smoothness dilemma encountered in previous examples.
Interestingly, all precited dilemma are linked: the
same universal `human incapacity' is addressed though in different contexts and by different
technical slangs. So let's try to
go beyond this cramped bipolar perspective and try to address more sophisticated priorities.\\

Adding dimensions is a well-known strategy to circumvent apparently impossible tasks (Klein's bottel). We do not depart from this receipt and
consider adding dimension(s) to the original mean-square paradigm. The ATS(R)-decomposition proposed in section \ref{ats_section} and concretized
in \ref{ats_mse} and in the customized criterion \ref{dfatp} or in the numerically (much) more attractive variant \ref{idfatp}, adds
an Accuracy-component as well as a (generally negligible) Residual. Therefore, the bulck of the mean-square mass (the energy of the filter error)
can be shifted among three (neglecting the Residual) instead of two `recipients'. We can relieve any single or any pair of components by
assigning the additional burden to the remaining one(s). If We want to improve Timeliness and Smoothness, We must be willing to afflict
Accuracy: the affliction might be `heavy' because as We depart from the optimal MSE-solution,
the MSE-mass inflates. The ATS-Trilemma adds flexibility
in order to match research priorities by optimization criteria.\\




We now apply the DFA-apparatus to two `standard' tasks, namely \emph{seasonal adjustment} and \emph{trend extraction}
and take profit of the additional opportunities unfolded by customization and the ATS-trilemma\footnote{We do not explicitly emphasize
\emph{cycle extraction} (bandpass filters) but since Our proceeding is generic the former could be straightforwardly implemented.}.





\section{Seasonal Adjustment}

We now apply the above function to real-time seasonal adjustment. We analyze the structure of the estimation problem,
derive filter characteristics and apply a simple customization scheme. We augment the filter length and analyze overfitting
issues by relying on out-of-sample performances. We compute vintages, analyze revisions and obtain  tentacle plots.
Finally, We compute finite sample distributions of filter coefficients as well as finite sample distributions
of real-time amplitude and time-shift functions (as far as We know the latter are a novelty). In order to illustrate potentially interesting
issues We first analyze a very simple periodic (trigonometric) seasonal component. After that We propose SA for
a series with a complex mix of components, including deterministic, stationary stochastic and non-stationary stochastic seasonal
patterns.

\subsection{Narrow Seasonal Peak: Seasonal Line Spectrum}\label{sa_ls}

We here analyze a particular monthly `seasonal' series of large sample length $T=600$ and a simple deterministic (trigonometric)
seasonal of frequency $\pi/6$: the experimental design
is conceived such as to reveal some of the salient features of seasonal adjustment as well as to illustrate the flexibility
of the DFA as compared to the ubiquitous and rigid seasonal difference filter $1-B^{12}$.

\subsubsection{The DGP: AR(1) plus Single Line Spectrum}


We generate a realization of length $T=600$ of an AR(1)-process and add a determinsistic trigonometric signal of
frequency $\pi/6$
\begin{eqnarray*}
z_t&=&0.9z_{t-1}+\epsilon_t\\
x_t&=&z_t+cos(t\pi/6)
\end{eqnarray*}
with $\sigma^2=1$. The AR(1)-process $z_t$ is the signal and $cos(t\pi/6)$ is the (undesirable) seasonal component.
The aggregate $x_t$, its acf and its periodogram are plotted in fig.\ref{z_seas_a}.

<<echo=True>>=
# Simulation of the AR(1)+cos series
len<-600
set.seed(10)
eps<-rnorm(len)
ar1<-0.9
z<-eps
for (i in 2:len)
  z[i]<-ar1*z[i-1]+eps[i]
x<-z+cos((1:len)*pi/6)
plot_T<-F

file = paste("z_seas_a.pdf", sep = "")
pdf(file = paste(path.out,file,sep=""), paper = "special", width = 6, height = 6)
par(mfrow=c(3,1))
ts.plot(x)
acf(x)
plot(per(x,plot_T)$per,type="l",axes=F,ylab="Periodogram")
axis(1,at=1+0:6*len/12,labels=c("0","pi/6","2pi/6","3pi/6","4pi/6","5pi/6","pi"))
axis(2)
box()
dev.off()
@
<<label=z_seas_a.pdf,echo=FALSE,results=tex>>=
  file = paste("z_seas_a.pdf", sep = "")
  cat("\\begin{figure}[H]")
  cat("\\begin{center}")
  cat("\\includegraphics[height=6in, width=6in]{", file, "}\n",sep = "")
  cat("\\caption{Seasonal series (top), acf (middle) and periodogram (bottom)", sep = "")
  cat("\\label{z_seas_a}}", sep = "")
  cat("\\end{center}")
  cat("\\end{figure}")
@
In contrast to the acf, the periodogram is very explicit about the presence of a seasonal component with a single spectral peak
in $\pi/6$: efficiency of the DFA is closely linked to the resolution ability of the periodogram. We now attempt to seasonally adjust the data by damping the seasonal peak and by leaving all other components
`intact'\footnote{Since the seasonal component in Our example is a deterministic trigonometric signal (a line spectrum) We could remove
the undesirable term by simple regression methods. In practice, however, seasonal components are never deterministic and therefore
this approach is not to be recommended, in general.}.
Obviously, a time series with a single seasonal peak cannot be tackled by the classical seasonal difference filter $1-B^{12}$ whose amplitude
function was shown in fig.\ref{z_amp_pha_ma12}. Therefore, We first \textbf{customize the target signal} $\Gamma$ in view
of the seasonal adjustment (SA) task.

\subsubsection{Manual Gamma-Interface}


We now specify the transfer function $\Gamma(\cdot)$ of a symmetric target filter which proceeds to SA of $x_t$ by eliminating
the seasonal component in $\pi/6$ without affecting (noticeably) the interesting signal, namely the AR(1)-component $z_t$:
\[\Gamma(\omega)=\left\{\begin{array}{cc}1&\omega\not=\pi/6\\0&\omega=\pi/6\end{array}\right.\]
This target filter is plotted together with the amplitude of the classical seasonal adjustment filter $1-B^{12}$ in fig.\ref{z_seas_a_a}.

<<echo=True>>=
# assigning the periodogram
weight_func<-per(x,plot_T)$per
K<-length(weight_func)-1
# Define target Gamma
Gamma<-rep(1,K+1)
Gamma[1+(length(Gamma)/6):(length(Gamma)/6)]<-0
# Compute amplitude of seasonal difference
omega_k<-(0:K)*pi/K
b1<-1
b12<--1
trffkt<-1+b12*complex(arg=12*omega_k)
amplitude<-abs(trffkt)

file = paste("z_seas_a_a.pdf", sep = "")
pdf(file = paste(path.out,file,sep=""), paper = "special", width = 6, height = 6)
plot(Gamma,type="l",axes=F,col="black",ylim=c(0,2),ylab="",xlab="")
lines(amplitude,lty=1,col="red")
mtext("Target", side = 3, line = -1,at=K/2,col="black")
mtext("Amplitude seasonal differences", side = 3, line = -2,at=K/2,col="red")
axis(1,at=1+0:6*K/6,labels=c("0","pi/6","2pi/6","3pi/6","4pi/6","5pi/6","pi"))
axis(2)
box()
dev.off()
@
<<label=z_seas_a_a.pdf,echo=FALSE,results=tex>>=
  file = paste("z_seas_a_a.pdf", sep = "")
  cat("\\begin{figure}[H]")
  cat("\\begin{center}")
  cat("\\includegraphics[height=6in, width=6in]{", file, "}\n",sep = "")
  cat("\\caption{Seasonal adjustment: target (blue) vs. traditional seasonal adjustment filter (black)", sep = "")
  cat("\\label{z_seas_a_a}}", sep = "")
  cat("\\end{center}")
  cat("\\end{figure}")
@
The target $\Gamma(\omega_k)$ (black) removes the spectral peak in $\pi/6$ and leaves all other components intact\footnote{Strictly speaking
We remove components in a narrow interval centered in $\pi/6$ since in practice pure (deterministic) line spectra are unlikely to be
observed at least in the context of economic data.}. In contrast, the classical seasonal adjustment filter seems rather ill-suited
(heavily distorted) for the task at hand.\\

\textbf{Remark (practical hint)}
\begin{itemize}
\item When working with monthly seasonal data We recommend to select the sample length $T$ as a multiple of the seasonal
periodicity twelve (skip the first few observations
of the sample until the resulting $T$ is a multiple of twelve).
\item This way, the discrete frequency grid $\omega_k=k*2\pi/T$ tracks
the seasonal peaks since $\omega_{jT/12}=j\pi/6$ for $j=1,...,6$.
\end{itemize}

\subsubsection{A `Short' Real-Time MSE (SA-)Filter}

We now compute a MSE real-time filter of length $L=24$ (again a multiple of 12) based on \ref{dfa_ms}. Amplitude and
time-shift functions of the filter are plotted and compared to the target in fig.\ref{z_seas_a_a_at}.
<<echo=True>>=
# Length of Moving-Average:
# Trick: selecting L=24 to be a multiple of 6 (the quotient in the
#    frequency pi/6 of the cosine) allows
#    the filter to damp/eliminate the component more easily!
L<-24
# Customization: lambda=eta=0 implies minimal mean-square revisions
lambda<-0
eta<-0
# cutoff frequency: this parameter is inactive if eta=0
cutoff<-pi/6
# Real-time filter
Lag<-0
# We do not impose filter restrictions: see exercises below
i1<-F
i2<-F
# Compute DFA
#
dfa_mse<-dfa_analytic(L,lambda,weight_func,Lag,Gamma,eta,cutoff,i1,i2)
#
b_ms<-dfa_mse$b
trffkt_ms<-dfa_mse$trffkt

# Amplitude and time shift
amp_analytic_ms<-abs(trffkt_ms)
pha_analytic_ms<-Arg(trffkt_ms)/(pi*(0:(K))/(K))

file = paste("z_seas_a_a_at.pdf", sep = "")
pdf(file = paste(path.out,file,sep=""), paper = "special", width = 6, height = 6)
par(mfrow=c(2,1))
plot(Gamma,type="l",axes=F,col="black",ylim=c(0,max(amp_analytic_ms)),ylab="",
xlab="",main="Amplitude")
lines(amp_analytic_ms,lty=1,col="red")
mtext("Target", side = 3, line = -1,at=K/2,col="black")
mtext("Real-time amplitude MSE-filter", side = 3, line = -2,at=K/2,col="red")
axis(1,at=1+0:6*K/6,labels=c("0","pi/6","2pi/6","3pi/6","4pi/6","5pi/6","pi"))
axis(2)
box()
plot(rep(0,K+1),type="l",axes=F,col="black",ylim=c(min(na.exclude(pha_analytic_ms)),
max(na.exclude(pha_analytic_ms))),ylab="",xlab="",main="Time-shift")
lines(pha_analytic_ms,lty=1,col="red")
mtext("Target", side = 3, line = -1,at=K/2,col="black")
mtext("Real-time amplitude MSE-filter", side = 3, line = -2,at=K/2,col="red")
axis(1,at=1+0:6*K/6,labels=c("0","pi/6","2pi/6","3pi/6","4pi/6","5pi/6","pi"))
axis(2)
box()
dev.off()
@
<<label=z_seas_a_a_at.pdf,echo=FALSE,results=tex>>=
  file = paste("z_seas_a_a_at.pdf", sep = "")
  cat("\\begin{figure}[H]")
  cat("\\begin{center}")
  cat("\\includegraphics[height=6in, width=6in]{", file, "}\n",sep = "")
  cat("\\caption{Seasonal adjustment: real-time MSE-filter (red) vs. target (black): amplitude (top) and
  time-shift (bottom)", sep = "")
  cat("\\label{z_seas_a_a_at}}", sep = "")
  cat("\\end{center}")
  cat("\\end{figure}")
@
We observe that the amplitude function of the MSE real-time filter attempts to damp -- not eliminate -- the seasonal component.
We now apply the filter to the data $x_t$ and compare periodograms before/after filtering, see
fig.\ref{z_seas_a_a_afbe}.
<<echo=True>>=
# Filtering time series
xf_ms<-x
for (i in L:length(x))
{
  xf_ms[i]<-b_ms%*%x[i:(i-L+1)]
}

file = paste("z_seas_a_a_afbe.pdf", sep = "")
pdf(file = paste(path.out,file,sep=""), paper = "special", width = 6, height = 6)
par(mfrow=c(2,1))
ts.plot(xf_ms,col="red",ylab="")
lines(x,col="blue")
mtext("Input", side = 3, line = -1,at=K,col="blue")
mtext("Real-time output", side = 3, line = -2,at=K,col="red")
plot((per(xf_ms[L:len],plot_T)$per),type="l",axes=F,col="red",
ylab="Periodograms",xlab="Frequency")
lines((per(x[L:len],plot_T)$per),lty=2,col="blue")
mtext("Input", side = 3, line = -1,at=K/2,col="blue")
mtext("Real-time output", side = 3, line = -2,at=K/2,col="red")
axis(1,at=1+0:6*(len-L)/12,labels=c("0","pi/6","2pi/6","3pi/6",
"4pi/6","5pi/6","pi"))
axis(2)
box()
dev.off()
@
<<label=z_seas_a_a_afbe.pdf,echo=FALSE,results=tex>>=
  file = paste("z_seas_a_a_afbe.pdf", sep = "")
  cat("\\begin{figure}[H]")
  cat("\\begin{center}")
  cat("\\includegraphics[height=6in, width=6in]{", file, "}\n",sep = "")
  cat("\\caption{Filter effect: original (blue) and filtered (red) series (top graph) and
  periodograms (bottom graph)", sep = "")
  cat("\\label{z_seas_a_a_afbe}}", sep = "")
  cat("\\end{center}")
  cat("\\end{figure}")
@
As expected, the seasonal peak is damped but not entirely removed. The other components do not seem to be affected
by the filtering, as desired. We now attempt to improve the attenuation of the seasonal peak by the
real-time design, possibly without affecting the other components noticeably.

\subsubsection{Customizing the Real-Time (SA-)Filter}

\label{sa_cust}We just learned how to customize the target signal $\Gamma(\cdot)$ in order to perform seasonal adjustment (lowpass
and bandpass filtering were discussed in previous sections; highpass is generally uninteresting). We now customize the amplitude fit
in the stopband by relying on the general (analytically tractable) criterion \ref{idfatp}. Specifically, We assign an artificially large
weight $W(\pi/6)=100$ to frequency $\pi/6$ such that $\hat{\Gamma}(\pi/6)$ must be close to $\Gamma(\pi/6)=0$ (otherwise the value of
the criterion \ref{idfatp} would become excessively large). This way We force the real-time filter
to damp effectively the seasonal peak. We set $\lambda=0$. This is a `true' customization in the sense that the user
prioritizes an `effective' seasonal adjustment over the plain mean-square solution, as provided by criterion \ref{dfa_ms}.\\

We set
\begin{equation}\label{W_cust_sa}
W(\omega_k)=\left\{\begin{array}{cc}100&\omega_k=\pi/6\\1&\textrm{otherwise}\end{array}\right.
\end{equation}
and compute the corresponding customized real-time filter\footnote{Recall that We selected $T$ to be a multiple of the seasonal periodicity
12 such that $\omega_{T/12}=\pi/6$ i.e. frequency $\pi/6$ is an element of the set (frequency-grid)
$\omega_k,k=0,...,T/2$.}: outputs, amplitude and time-shift functions of the customized and of the previous
MSE-filter are plotted and compared in figs.\ref{z_seas_a_ms_c} and \ref{z_seas_per_ms_c}. Note that We can interpret $W(\omega_k,\eta)I_{TX}(\omega_k)$ in \ref{idfatp} as
a customized spectrum estimate $I_{TX'}(\omega_k)$ of a fictious series $x_t'$ with a very strong
seasonal component in $\pi/6$ i.e. We are performing ordinary mean-square optimization for a `suitably
distorted' DGP $x_t'$ which fits Our assigned priority.

<<echo=True>>=
weight_func_c<-weight_func
# Here We customize
weight_func_c[(length(weight_func)/6+1)]<-10^6*weight_func[length(weight_func)/6+1]

# Compute customized filter
dfa_c<-dfa_analytic(L,lambda,weight_func_c,Lag,Gamma,eta,cutoff,i1,i2)
b_c<-dfa_c$b
trffkt_c<-dfa_c$trffkt
# Amplitude and time shift
amp_analytic_c<-abs(trffkt_c)
pha_analytic_c<-Arg(trffkt_c)/(pi*(0:(K))/(K))

# Filter series
xf_c<-x
for (i in L:length(x))
{
  xf_c[i]<-b_c%*%x[i:(i-L+1)]
}

file = paste("z_seas_per_ms_c.pdf", sep = "")
pdf(file = paste(path.out,file,sep=""), paper = "special", width = 6, height = 6)

par(mfrow=c(2,1))
plot((per(xf_ms[L:len],plot_T)$per),type="l",axes=F,col="red",
ylab="Periodogram",xlab="Frequency",
main="Comparisons of periodograms of input and output signals")
lines((per(x[L:len],plot_T)$per),col="blue")
lines((per(xf_c[L:len],plot_T)$per),col="green")
mtext("Input", side = 3, line = -1,at=K/2,col="blue")
mtext("Output mean-square", side = 3, line = -2,at=K/2,col="red")
mtext("Output customized", side = 3, line = -3,at=K/2,col="green")
axis(1,at=1+0:6*(len-L)/12,labels=c("0","pi/6","2pi/6","3pi/6",
"4pi/6","5pi/6","pi"))
axis(2)
box()

hl<-10
plot((per(xf_ms[L:len],plot_T)$per)[((len-L)/12)+(-hl:hl)],type="l",
axes=F,col="red",ylab="Periodogram",xlab="Frequency",
main="Magnifying glass on pi/6",
ylim=c(0,max((per(x[L:len],plot_T)$per)[((len-L)/12)+(-hl:hl)])))
lines((per(x[L:len],plot_T)$per)[((len-L)/12)+(-hl:hl)],col="blue")
lines((per(xf_c[L:len],plot_T)$per)[((len-L)/12)+(-hl:hl)],col="green")
mtext("Input", side = 3, line = -1,at=K/2,col="blue")
mtext("Output mean-square", side = 3, line = -2,at=K/2,col="red")
mtext("Output customized", side = 3, line = -3,at=K/2,col="green")
axis(1,at=hl+2,labels="pi/6")
axis(2)
box()
dev.off()
@
<<label=z_seas_per_ms_c.pdf,echo=FALSE,results=tex>>=
  file = paste("z_seas_per_ms_c.pdf", sep = "")
  cat("\\begin{figure}[H]")
  cat("\\begin{center}")
  cat("\\includegraphics[height=4in, width=4in]{", file, "}\n",sep = "")
  cat("\\caption{Periodograms of input (blue), output MSE (red) and output customized (green): full
  bandwith (top) and vicinity of pi/6 (bottom)", sep = "")
  cat("\\label{z_seas_per_ms_c}}", sep = "")
  cat("\\end{center}")
  cat("\\end{figure}")
@
We can observe that the periodograms of filtered (green and red) and orifinal data are almost identical in the passband. In $\pi/6$
the periodogram of the customized filter is vanishingly small, as desired.


<<echo=True>>=
file = paste("z_seas_a_ms_c.pdf", sep = "")
pdf(file = paste(path.out,file,sep=""), paper = "special", width = 6, height = 6)

par(mfrow=c(2,1))
plot(Gamma,type="l",axes=F,col="black",ylim=c(0,max(amp_analytic_ms)),
ylab="",xlab="",main="Amplitude")
lines(amp_analytic_ms,lty=1,col="red")
lines(amp_analytic_c,lty=1,col="green")
mtext("Target", side = 3, line = -1,at=K/2,col="black")
mtext("Real-time amplitude MSE", side = 3, line = -2,at=K/2,col="red")
mtext("Real-time amplitude customized", side = 3, line = -3,at=K/2,col="green")
axis(1,at=1+0:6*K/6,labels=c("0","pi/6","2pi/6","3pi/6","4pi/6","5pi/6","pi"))
axis(2)
box()
plot(rep(0,K+1),type="l",axes=F,col="black",ylim=c(min(na.exclude(pha_analytic_c)),
max(na.exclude(pha_analytic_c))),ylab="",xlab="",main="Time-shift")
lines(pha_analytic_ms,lty=1,col="red")
lines(pha_analytic_c,lty=1,col="green")
mtext("Target", side = 3, line = -1,at=K/2,col="black")
mtext("Real-time amplitude MSE", side = 3, line = -2,at=K/2,col="red")
mtext("Real-time amplitude customized", side = 3, line = -3,at=K/2,col="green")
axis(1,at=1+0:6*K/6,labels=c("0","pi/6","2pi/6","3pi/6","4pi/6","5pi/6","pi"))
axis(2)
box()
dev.off()
@
<<label=z_seas_a_ms_c.pdf,echo=FALSE,results=tex>>=
  file = paste("z_seas_a_ms_c.pdf", sep = "")
  cat("\\begin{figure}[H]")
  cat("\\begin{center}")
  cat("\\includegraphics[height=6in, width=6in]{", file, "}\n",sep = "")
  cat("\\caption{Amplitude (top) and time-shifts (bottom) of MSE (red) and customized filters (green)", sep = "")
  cat("\\label{z_seas_a_ms_c}}", sep = "")
  cat("\\end{center}")
  cat("\\end{figure}")
@
Customizing the real-time filter by assigning a large weight to frequency $\pi/6$ in \ref{W_cust_sa} pulls the `green' amplitude function
down in order to match $\Gamma(\pi/6)=0$. As We can see, this is almost a `free lunch' since amplitude and time-shift functions are
not substantially affected by this customization.

\subsubsection{A High-Order (SA-) Filter}

We now propose an alternative approach which hopefully provides deeper insights into the structure of the estimation problem.
We first note that
the undesirable spectral peak in Our example is narrow, as is frequently the case for economic data
with regular but not necessarily deterministic seasonal patterns. The above fig.\ref{z_seas_a_ms_c}, top-graph, illustrated that the dip
of the amplitude function in $\pi/6$ is unnecessarily wide: as an undesirable side-effect, the filter removes spectral power on
the left and on the right of $\pi/6$ as can be seen by comparing periodograms of filterd (red and green) outputs vs. input series (blue)
in fig.\ref{z_seas_per_ms_c}, bottom-graph. This is because a MA-filter of length $L=\Sexpr{L}$ is a polynomial of order $\Sexpr{L}$
in $\exp(-ij\omega_k)$ which has at most $\Sexpr{L}$ roots or zeroes in $[-\pi,\pi]$: as can be seen in fig.\ref{z_seas_a_ms_c}, top-graph,
We count $L/2=\Sexpr{L/2}$ `ondulations' of the amplitude function in the half-width $[0,\pi]$.
Since the number of potential roots is`small', the resulting polynomial transfer function is unable to generate
a very narrow dip in $\pi/6$. Thus We could try to augment the filter order.\\

We now set $L=60$ and $L=120$ and compute corresponding \emph{mean-square}
real-time filters. Amplitude functions as well as periodograms of input and output signals are plotted in
figs.\ref{z_seas_a_mse_c_mse} and \ref{z_seas_per_ms_c_ms} together with our
previous filters.
<<echo=True>>=
# Length of Moving-Averages:
L_vec<-c(60,120)
# Compute analytical DFA: MSE-filters (no customization)
for (i in 1:length(L_vec))
{
  assign(paste("dfa_mse_",L_vec[i],sep=""),
  dfa_analytic(L_vec[i],lambda,weight_func,Lag,Gamma,eta,cutoff,i1,i2))
  assign(paste("b_ms_",L_vec[i],sep=""),
  get(paste("dfa_mse_",L_vec[i],sep=""))$b)
  assign(paste("trffkt_ms_",L_vec[i],sep=""),
  get(paste("dfa_mse_",L_vec[i],sep=""))$trffkt)
  assign(paste("amp_analytic_ms_",L_vec[i],sep=""),
  abs(get(paste("trffkt_ms_",L_vec[i],sep=""))))
  assign(paste("pha_analytic_ms_",L_vec[i],sep=""),
  Arg(get(paste("trffkt_ms_",L_vec[i],sep="")))/(pi*(0:(K))/(K)))
}

file = paste("z_seas_a_mse_c_mse.pdf", sep = "")
pdf(file = paste(path.out,file,sep=""), paper = "special", width = 6, height = 6)
par(mfrow=c(2,1))
plot(Gamma,type="l",axes=F,col="black",ylim=c(0,max(amp_analytic_c)),
ylab="",xlab="",main="Amplitude")
lines(amp_analytic_ms,lty=1,col="red")
lines(amp_analytic_c,lty=1,col="green")
mtext("Target", side = 3, line = -1,at=K/2,col="black")
mtext("MSE-filter: L=24", side = 3, line = -2,at=K/2,col="red")
mtext("Customized filter: L=24", side = 3, line = -3,at=K/2,col="green")
colo<-c("violet","orange")
for (i in 1:length(L_vec))
{
  lines(get(paste("amp_analytic_ms_",L_vec[i],sep="")),col=colo[i])
  mtext(paste("MSE-Filter: L=",L_vec[i],sep=""), side = 3, line = -3-i,at=K/2,col=colo[i])
}
axis(1,at=1+0:6*K/6,labels=c("0","pi/6","2pi/6","3pi/6","4pi/6","5pi/6","pi"))
axis(2)
box()
hl<-30
plot(Gamma[(K/6)+(-hl:hl)],type="l",axes=F,col="black",
ylim=c(0,max(amp_analytic_c)),ylab="",xlab="",
main="Amplitude: magnifying-glass in pi/6")
lines(amp_analytic_ms[(K/6)+(-hl:hl)],lty=1,col="red")
lines(amp_analytic_c[(K/6)+(-hl:hl)],lty=1,col="green")
mtext("Target", side = 3, line = -1,at=hl,col="black")
mtext("MSE-filter: L=24", side = 3, line = -2,at=hl,col="red")
mtext("Customized filter: L=24", side = 3, line = -3,at=hl,col="green")
for (i in 1:length(L_vec))
{
  lines(get(paste("amp_analytic_ms_",L_vec[i],sep=""))[(K/6)+(-hl:hl)],col=colo[i])
  mtext(paste("MSE-Filter: L=",L_vec[i],sep=""), side = 3, line = -3-i,at=hl,col=colo[i])
}
axis(1,at=hl+2,labels="pi/6")
axis(2)
box()
dev.off()
@
<<label=z_seas_a_mse_c_mse.pdf,echo=FALSE,results=tex>>=
  file = paste("z_seas_a_mse_c_mse.pdf", sep = "")
  cat("\\begin{figure}[H]")
  cat("\\begin{center}")
  cat("\\includegraphics[height=6in, width=6in]{", file, "}\n",sep = "")
  cat("\\caption{Amplitude functions full bandwith (top) and magnifying glass in pi/6 (bottom): MSE L=24 (red),
  customized L=24 (green), MSE L=60 (violet) and MSE L=120 (orange)", sep = "")
  cat("\\label{z_seas_a_mse_c_mse}}", sep = "")
  cat("\\end{center}")
  cat("\\end{figure}")
@
A closer look to frequency $\pi/6$ in the bottom graph illustrates that the highest order filter $L=120$ (orange)
matches Our intention
quite well: the dip is `deep' (the damping will be strong) and its width is substantially reduced as compared
to the customized filter (green). The top-graph confirms that the high-order filter (orange) is close to an identity
in the passband, as desired. Remarkably, overfitting does not seem to be an issue here, since the filter matches all
requirements for an effective (real-time) SA. \\

We now filter the data with the available designs and compare periodograms before and after filtering, see fig.\ref{z_seas_per_ms_c_ms}.


<<echo=True>>=
# Filter series
xf<-matrix(ncol=length(L_vec),nrow=length(x))
for (j in 1:length(L_vec))
{
  for (i in L_vec[j]:length(x))
  {
    xf[i,j]<-get(paste("b_ms_",L_vec[j],sep=""))%*%x[i:(i-L_vec[j]+1)]
  }
}

file = paste("z_seas_per_ms_c_ms.pdf", sep = "")
pdf(file = paste(path.out,file,sep=""), paper = "special", width = 6, height = 6)

par(mfrow=c(2,1))
plot((per(xf_ms[max(L_vec):len],plot_T)$per),type="l",axes=F,col="red",
ylab="Periodogram",xlab="Frequency",
main="Comparisons of periodograms of input and output signals")
lines((per(xf_c[max(L_vec):len],plot_T)$per),col="green")
mtext("MSE-Filter: L=24", side = 3, line = -3,at=K/2,col="red")
mtext("Output customized", side = 3, line = -2,at=K/2,col="green")
for (i in 1:length(L_vec))
{
  lines((per(xf[max(L_vec):len,i],plot_T)$per),col=colo[j])
  mtext(paste("MSE-Filter: L=",L_vec[i],sep=""), side = 3, line = -3-i,
  at=K/2,col=colo[i])
}
axis(1,at=1+0:6*(len-max(L_vec))/12,labels=c("0","pi/6","2pi/6","3pi/6",
"4pi/6","5pi/6","pi"))
axis(2)
box()

hl<-30
plot((per(xf_ms[max(L_vec):len],plot_T)$per)[(len-max(L_vec))/12+(-hl:hl)],type="l",axes=F,
col="red",ylab="Periodogram",xlab="Frequency",
main="Magnifying glass on pi/6")
lines((per(xf_c[max(L_vec):len],plot_T)$per)[(len-max(L_vec))/12+(-hl:hl)],col="green")
mtext("MSE-Filter: L=24", side = 3, line = -3,at=hl,col="red")
mtext("Output customized", side = 3, line = -2,at=hl,col="green")
for (i in 1:length(L_vec))   #i<-2
{
  lines((per(xf[max(L_vec):len,i],plot_T)$per)[(len-max(L_vec))/12+(-hl:hl)],col=colo[i])
  mtext(paste("MSE-Filter: L=",L_vec[i],sep=""), side = 3, line = -3-i,at=hl,col=colo[i])
}
axis(1,at=hl+2,labels="pi/6")
axis(2)
box()
dev.off()
@
<<label=z_seas_per_ms_c_ms.pdf,echo=FALSE,results=tex>>=
  file = paste("z_seas_per_ms_c_ms.pdf", sep = "")
  cat("\\begin{figure}[H]")
  cat("\\begin{center}")
  cat("\\includegraphics[height=6in, width=6in]{", file, "}\n",sep = "")
  cat("\\caption{Periodograms of  MSE L=24 (red), output customized L=24 (green),
  output MSE L=60 (violet) and output MSE L=120 (orange) : full
  bandwith (top) and vicinity of pi/6 (bottom)", sep = "")
  cat("\\label{z_seas_per_ms_c_ms}}", sep = "")
  cat("\\end{center}")
  cat("\\end{figure}")
@

To conclude We compare the periodogram of the input signal and of the MSE-filter with $L=120$, see fig.\ref{z_seas_per_ms_120_i}.
<<echo=True>>=
file = paste("z_seas_per_ms_120_i.pdf", sep = "")
pdf(file = paste(path.out,file,sep=""), paper = "special", width = 6, height = 6)

par(mfrow=c(2,1))
plot((per(x[max(L_vec):len],plot_T)$per),type="l",axes=F,col="blue",
ylab="Periodogram",xlab="Frequency",
main="Comparisons of periodograms of input and output signals")
lines(per(xf[max(L_vec):len,i],plot_T)$per,col="orange")
mtext("Input", side = 3, line = -1,at=K/2,col="blue")
mtext("Output MSE 120", side = 3, line = -2,at=K/2,col="orange")
axis(1,at=1+0:6*(len-max(L_vec))/12,labels=c("0","pi/6","2pi/6","3pi/6",
"4pi/6","5pi/6","pi"))
axis(2)
box()

hl<-30
plot((per(x[max(L_vec):len],plot_T)$per)[(len-max(L_vec))/12+(-hl:hl)],
type="l",axes=F,col="blue",ylab="Periodogram",xlab="Frequency",
main="Comparisons of periodograms of input and output signals")
lines((per(xf[max(L_vec):len,i],plot_T)$per)[(len-max(L_vec))/12+(-hl:hl)],col="orange")
mtext("Input", side = 3, line = -1,at=hl,col="blue")
mtext("Output MSE 120", side = 3, line = -2,at=hl,col="orange")
axis(1,at=hl+2,labels="pi/6")
axis(2)
box()
dev.off()
@
<<label=z_seas_per_ms_120_i.pdf,echo=FALSE,results=tex>>=
  file = paste("z_seas_per_ms_120_i.pdf", sep = "")
  cat("\\begin{figure}[H]")
  cat("\\begin{center}")
  cat("\\includegraphics[height=6in, width=6in]{", file, "}\n",sep = "")
  cat("\\caption{Periodograms of input (blue) and ouput MSE L=120 (orange): full
  bandwith (top) and vicinity of pi/6 (bottom)", sep = "")
  cat("\\label{z_seas_per_ms_120_i}}", sep = "")
  cat("\\end{center}")
  cat("\\end{figure}")
@
As expected, the highest-order filter ($L=\Sexpr{L_vec[length(L_vec)]}$) does a remarkable task in damping the seasonal component:
almost without affecting the non-seasonal AR(1)-component. Despite its large order,
the filter should be more or less immune against overfitting because the
time-shift is close to zero and because the amplitude is close to one except in $\pi/6$: clearly, these characteristics
should be useful out-of-sample too. We now check these claims.


\subsubsection{Out-of-Sample Performances and Finite Sample Distributions of Filter Coefficients, Amplitude and Time-Shift Functions}


We analyze overfitting effects, out-of-sample performances and finite-sample empirical distribution of coefficients, amplitude
and time-shift functions of filters with lengths
$L=24,60$ and $120$.
MSE-performances in- and out-of-sample are computed with respect to the true signal $z_t$ in
\begin{eqnarray}
z_t&=&0.9z_{t-1}+\epsilon_t\label{z_t}\\
x_t&=&z_t+cos(t\pi/6)\nonumber
\end{eqnarray}
We can thus avoid computation of a possibly cumbersome
high-order symmetric target filter. We generate 100 realizations of length 360 of the
above seasonal process: the data is splitted into the first 180 observations, used  for estimation (in-sample performances),
and the final 180 observations, used for out-of-sample evaluation. MSE's are reported in table \ref{perf_mat_idfa_i_o} and
the empirical distributions of filter coefficients, of real-time amplitude
and of time-shift functions are plotted in figs.\ref{z_idfa_dist_SA_coef}, \ref{z_idfa_dist_SA_amp} and \ref{z_idfa_dist_SA_shift}.
<<echo=True>>=
Lag<-0
anzsim<-100
len1<-360
len_sim<-180
K<-len_sim/2
set.seed(10)
L_vec<-c(24,60,120)
# Define target Gamma
Gamma_sim<-rep(1,K+1)
Gamma_sim[1+(length(Gamma_sim)/6):(length(Gamma_sim)/6)]<-0
b_sim<-as.list(1:anzsim)
b_sim<-as.list(1:anzsim)
amp_sim<-b_sim
shift_sim<-b_sim
b<-as.list(1:length(L_vec))
amp<-matrix(ncol=length(L_vec),nrow=K+1)
shift<-amp
mse_in_sample<-matrix(ncol=length(L_vec),nrow=anzsim)
mse_out_sample<-mse_in_sample
ymin<-rep(10^6,length(L_vec))
ymax<--ymin
# Start simulation
for (i in 1:simanz)
{
  z1<-arima.sim(list(ar=0.9),n=len1+max(L_vec))
  x1<-z1+cos((1:(len1+max(L_vec)))*pi/6)
  z_sim<-z1[max(L_vec)-1+1:len_sim]
  x_sim<-x1[max(L_vec)-1+1:len_sim]
  weight_func_sim<-per(x_sim,plot_T)$per
# Compute MSE-filters
  for (j in 1:length(L_vec))
  {
    dfa<-dfa_analytic(L_vec[j],lambda,weight_func_sim,Lag,Gamma_sim,eta,cutoff,i1,i2)
    b[[j]]<-dfa$b
    ymin[j]<-min(ymin[j],min(b[[j]]))
    ymax[j]<-max(ymax[j],max(b[[j]]))
    amp[,j]<-abs(dfa$trffkt)
    shift[,j]<-Arg(dfa$trffkt)/((0:K)*pi/K)
    xf<-x1
    for (k in 1:len1)
    {
      xf[max(L_vec)-1+k]<-b[[j]]%*%x1[max(L_vec)-1+k:(k-L_vec[j]+1)]
    }
    
    mse_in_sample[i,j]<-mean((z1-xf)[max(L_vec)-1+1:len_sim]^2)
    mse_out_sample[i,j]<-mean((z1-xf)[max(L_vec)-1+(len_sim+1):len1]^2)
  }
  b_sim[[i]]<-b
  amp_sim[[i]]<-amp
  shift_sim[[i]]<-shift
}
perf_mat<-rbind(apply(mse_in_sample,2,mean),apply(mse_out_sample,2,mean))
dimnames(perf_mat)[[2]]<-paste("L=",L_vec,sep="")
dimnames(perf_mat)[[1]]<-c("In-sample","Out-of-sample")

file = paste("z_idfa_dist_SA_coef.pdf", sep = "")
pdf(file = paste(path.out,file,sep=""), paper = "special", width = 6, height = 6)
par(mfrow=c(3,1))
for (j in 1:length(L_vec))
{
  b<-b_sim[[1]][[j]]
  ts.plot(b_sim[[1]][[j]],ylim=c(ymin[j],ymax[j]),
  main=paste("Empirical distribution of coefficients: L = ",L_vec[j],sep=""),
  xlab="lag",ylab="coefficients")
  for (i in 2:anzsim)
  {
    lines(b_sim[[i]][[j]])
    b<-b+b_sim[[i]][[j]]
  }
  lines(b/anzsim,col="red",lwd=2)
}
dev.off()
@
<<label=z_idfa_dist_SA_coef.pdf,echo=FALSE,results=tex>>=
  file = paste("z_idfa_dist_SA_coef.pdf", sep = "")
  cat("\\begin{figure}[H]")
  cat("\\begin{center}")
  cat("\\includegraphics[height=6in, width=6in]{", file, "}\n",sep = "")
  cat("\\caption{Empirical distribution of real-time filter coefficients of SA-filters of length 24, 60 and 120:
  mean of distribution highlighted in red", sep = "")
  cat("\\label{z_idfa_dist_SA_coef}}", sep = "")
  cat("\\end{center}")
  cat("\\end{figure}")
@
<<echo=True>>=
file = paste("z_idfa_dist_SA_amp.pdf", sep = "")
pdf(file = paste(path.out,file,sep=""), paper = "special", width = 6, height = 6)
par(mfrow=c(3,1))

for (j in 1:length(L_vec))
{
  amp<-amp_sim[[1]][,j]
  ts.plot(amp_sim[[1]][,j],ylim=c(0,1.8),
  main=paste("Empirical distribution of amplitude: L = ",L_vec[j],sep=""),
  xlab="lag",ylab="coefficients")
  for (i in 2:anzsim)
  {
    lines(amp_sim[[i]][,j])
    amp<-amp+amp_sim[[i]][,j]
  }
  lines(amp/anzsim,col="red",lwd=2)
}
dev.off()
@
<<label=z_idfa_dist_SA_amp.pdf,echo=FALSE,results=tex>>=
  file = paste("z_idfa_dist_SA_amp.pdf", sep = "")
  cat("\\begin{figure}[H]")
  cat("\\begin{center}")
  cat("\\includegraphics[height=6in, width=6in]{", file, "}\n",sep = "")
  cat("\\caption{Empirical distribution of real-time amplitude functions of SA-filters of length 24, 60 and 120: mean of
  distribution is highlighted in red", sep = "")
  cat("\\label{z_idfa_dist_SA_amp}}", sep = "")
  cat("\\end{center}")
  cat("\\end{figure}")
@
<<echo=True>>=
file = paste("z_idfa_dist_SA_shift.pdf", sep = "")
pdf(file = paste(path.out,file,sep=""), paper = "special", width = 6, height = 6)
par(mfrow=c(3,1))

for (j in 1:length(L_vec))
{
  shift<-shift_sim[[1]][,j]
  ts.plot(shift_sim[[1]][,j],ylim=c(-2,2),
  main=paste("Empirical distribution of time-shift: L = ",L_vec[j],sep=""),
  xlab="lag",ylab="coefficients")
  for (i in 2:anzsim)
  {
    lines(shift_sim[[i]][,j])
    shift<-shift+shift_sim[[i]][,j]
  }
  lines(shift/anzsim,col="red",lwd=2)
}
dev.off()
@
<<label=z_idfa_dist_SA_shift.pdf,echo=FALSE,results=tex>>=
  file = paste("z_idfa_dist_SA_shift.pdf", sep = "")
  cat("\\begin{figure}[H]")
  cat("\\begin{center}")
  cat("\\includegraphics[height=6in, width=6in]{", file, "}\n",sep = "")
  cat("\\caption{Empirical distribution of time-shift of SA-filters of length 24, 60 and 120: mean of
  distribution is highlighted in red", sep = "")
  cat("\\label{z_idfa_dist_SA_shift}}", sep = "")
  cat("\\end{center}")
  cat("\\end{figure}")
@

<<label=perf_mat_idfa_i_o,echo=FALSE,results=tex>>=
  library(Hmisc)
  require(xtable)
  #latex(cor_vec, dec = 1, , caption = "Example of using latex to create table",
  #center = "centering", file = "", floating = FALSE)
  xtable(perf_mat, dec = 1,digits=rep(3,dim(perf_mat)[2]+1),
  paste("In- and out-of-sample MSE-performances of real-time SA-filters of length 24, 60 and 120",sep=""),
  label=paste("perf_mat_idfa_i_o",sep=""),
  center = "centering", file = "", floating = FALSE)
@
Table \ref{perf_mat_idfa_i_o} confirms that in-sample and out-of-sample performances are close to a tie and that
out-of-sample performances of the highest-order filter ($L=\Sexpr{max(L_vec)}$) are better than in-sample performances
of the shorter filters involved: as claimed, overfitting is not an issue.
Damping or removing narrow spectral peaks is an \emph{easy estimation task}! As
an additional  confirmation fig.\ref{z_idfa_dist_SA_coef} illustrates that the empirical
distribution of filter coefficients is very tight: as expected, the filters are pretty close to identities since
$b_0\approx 1$ and $b_j\approx 0$ if $j>0$. In contrast, the empirical distribution of coefficients of
real-time \emph{trend}-extraction filters in fig.\ref{z_dfa_wn_b_dist} appears substantially
noisier which reflects the complexity of this problem: damping components in a broad stop-band
is much more difficult than tackling narrow (seasonal) peaks. Accordingly, overfitting was a serious
issue, recall MSE-performances reported in table \ref{perf_mat_dfa_i_o} as compared to table \ref{perf_mat_idfa_i_o} and
compare the filter-lengths involved.\\

\textbf{Recommendation}\\

Although I-DFA could perform seasonal adjustment and trend extraction at once, based on a single filter,
the previous results suggest the following proceeding in applications:
\begin{itemize}
\item first seasonally adjust a series
based on a real-time SA-filter of possibly high order, which is immanently immune to overfitting
\item then
apply a `specialized' trend-extraction filter  of much smaller order -- to contain overfitting -- to the previously adjusted series.
\end{itemize}

The empirical distribution of the amplitude function in fig.\ref{z_idfa_dist_SA_amp} suggests that
higher order filters $L=\Sexpr{L_vec[length(L_vec)]}$ are proner to `outliers' of the amplitude function in the
\emph{passband} $\omega_k\not= \pi/6$ (Gibbs phenomenon)
but they are more robust
in the stop-band i.e. the distribution is more tightly concentrated in $\omega_k=\pi/6$. The
empirical distribution of the time-shift in fig.\ref{z_idfa_dist_SA_shift} suggests that
the time-shift especially of higher-order filters is fairly tightly concentrated around zero in most of the passband.


\subsubsection{Revision Sequence: First, Final and Last Releases}

To conclude, We here analyze revision-sequences and vintages. Since real-time
SA-filters are close to an identity (up to narrow dips in potential seasonal
frequencies\footnote{Amplitude functions are close to an identity and time-shifts are nearly
vanishing, see figs.\ref{z_idfa_dist_SA_amp} and \ref{z_idfa_dist_SA_shift}: filter coefficients
are close to the identity too: $b_0=1$ and $b_k=0$ for $k>0$, see fig.\ref{z_idfa_dist_SA_coef}.}) we
expect real-time estimates to be fairly close to final releases. In order to check this claim we
rely on the above
SA-problem based on the realization of length $len=600$ of
\begin{eqnarray*}
z_t&=&0.9z_{t-1}+\epsilon_t\\
x_t&=&z_t+\cos(t\pi/6)
\end{eqnarray*}
with $\sigma^2=1$ and target signal $\Gamma(\cdot)$ as defined and computed in the previous exercises.\\

We set $L=121$\footnote{If $L$ is odd, then the filter corresponding to $Lag=(L-1)/2$ is symmetric.}
and compute SA-filters for $Lag=0,1,...,L-1=120$ -- We here go beyond the symmetric filter for reasons to be explained below --.
<<echo=True>>=
K<-length(weight_func)-1
L<-121
yhat_Lag<-array(dim=c(len,L))
trffkt<-array(dim=c(len/2+1,L))
b<-array(dim=c(L,L))
# Compute real-time filters for Lag=,...,L/2 and for the above three AR-processes
weight_func<-per(x,plot_T)$per
for (Lag in 0:(L-1))
{
# Optimize filters
  filt<-dfa_ms(L,weight_func,Lag,Gamma)
  trffkt[,Lag+1]<-filt$trffkt
  b[,Lag+1]<-filt$b
# Compute outputs
  for (j in L:len)
    yhat_Lag[j,Lag+1]<-filt$b%*%x[j:(j-L+1)]
}
@

We are now able to compute and plot amplitude and time-shift functions as well as filter-weights of $Lag=0$ (real-time),
$Lag=(L-1)/2=\Sexpr{(L-1)/2}$ (final or symmetric filter) and $Lag=L=\Sexpr{L-1}$ (last filter), see
fig.\ref{z_idfa_SA_Lag02L}.
<<echo=True>>=
omega_k<-pi*0:(len/2)/(len/2)
colo<-rainbow(L)
file = paste("z_idfa_SA_Lag02L.pdf", sep = "")
pdf(file = paste(path.out,file,sep=""), paper = "special", width = 6, height = 6)
par(mfrow=c(3,1))
amp<-abs(trffkt)
shift<-Arg(trffkt)/omega_k
ymin<-min(amp,na.rm=T)
ymax<-max(amp,na.rm=T)
plot(amp[,1],type="l",main="Amplitude functions",
  axes=F,xlab="Frequency",ylab="Amplitude",col="blue",ylim=c(ymin,ymax))
  mtext("Lag=0", side = 3, line = -1,at=len/4,col="blue")
select_i<-c(as.integer(L/2+1),L)
for (j in 1:length(select_i))
{
  lines(amp[,select_i[j]],col=colo[select_i[j]])
  mtext(paste("Lag=",select_i[j]-1,sep=""), side = 3, line = -j-1,
  at=len/4,col=colo[select_i[j]])
}
axis(1,at=c(0,1:6*len/12+1),labels=c("0","pi/6","2pi/6","3pi/6",
"4pi/6","5pi/6","pi"))
axis(2)
box()
ymin<-min(shift,na.rm=T)
ymax<-max(shift,na.rm=T)
plot(shift[,1],type="l",main="Time-Shifts",
  axes=F,xlab="Frequency",ylab="Shift",col="blue",ylim=c(-1,ymax))
  mtext("Lag=0", side = 3, line = -1,at=len/4,col="blue")
for (j in 1:length(select_i))
{
# We here recompute the shift because the function Arg has troubles
#   when dealing with the periodicity of the phase
  shift_s<-Arg(trffkt[,select_i[j]]*exp(-1.i*select_i[j]*omega_k))/omega_k
  lines(shift_s+select_i[j],col=colo[select_i[j]])
  mtext(paste("Lag=",select_i[j]-1,sep=""), side = 3,
  line = -j-1,at=len/4,col=colo[select_i[j]])
}
axis(1,at=c(0,1:6*len/12+1),labels=c("0","pi/6","2pi/6","3pi/6",
"4pi/6","5pi/6","pi"))
axis(2)
box()
ymin<-min(b,na.rm=T)
ymax<-max(b,na.rm=T)
plot(b[,1],type="l",col="blue",ylim=c(ymin,ymax),main="Filter coefficients",
ylab="Output",xlab="lag",
axes=F)
  mtext("Lag=0", side = 3, line = -1,at=L/2,col="blue")
for (j in 1:length(select_i))
{
  lines(b[,select_i[j]],col=colo[select_i[j]])
  mtext(paste("Lag=",select_i[j]-1,sep=""), side = 3, line = -j-1,
  at=L/2,col=colo[select_i[j]])
}
axis(1,at=20*(1:(L/20)),labels=20*(1:(L/20))-1)
axis(2)
box()
dev.off()
@
<<label=z_idfa_SA_Lag02L.pdf,echo=FALSE,results=tex>>=
  file = paste("z_idfa_SA_Lag02L.pdf", sep = "")
  cat("\\begin{figure}[H]")
  cat("\\begin{center}")
  cat("\\includegraphics[height=6in, width=6in]{", file, "}\n",sep = "")
  cat("\\caption{Amplitude (top), time-shift (middle) and coefficients (bottom) of filters with Lags 0 (blue), 60 (cyan) and 120 (red)", sep = "")
  cat("\\label{z_idfa_SA_Lag02L}}", sep = "")
  cat("\\end{center}")
  cat("\\end{figure}")
@
Amplitude functions of initial (dark blue: $Lag=0$) and of the `last' filter (red: $Lag=120$)
are identical which might be a bit surprising: the explanation is given by the bottom
graph where We see that both filters are identical up to exchanging $b_0$ and $b_{L-1}$. The time-shifts (middle-plot)
are therefore clearly
different since the initial release assigns most weight to the latest observation $b_0\approx 1$
whereas the last filter ($Lag=L-1$) reverts
time ordering, thus assigning most weight $b_{120}\approx 1$ to the first observation, lagged by $L-1=\Sexpr{L-1}$ time units.
Both asymmetric amplitude functions (top graph: red and blue are overlapping)
are very close to the amplitude of the final symmetric filter (light blue) which, together
with the nearly vanishing time-shift (middle graph, dark blue) suggests
that the initial release should be very close to the final (output of symmetric filter) release i.e. We expect the magnitude of revisions to be
\emph{small}.

\subsubsection{Revision Sequence: Vintages and Tentacle Plot}


The following R-chunk computes vintages of filters with $Lag=0,...,L-1=\Sexpr{L-1}$ and sets-up the the vintage triangle for the tentacle plot,
see fig.\ref{z_vintages_sa}.

<<echo=True>>=
vintage<-array(dim=c(len,len))
dim(vintage)
for (j in L:len)#j<-L
{
# first L/2 releases
  vintage[(j-as.integer(L/2)):j,j]<-yhat_Lag[j,(as.integer(L/2)+1):1]
# symmetric filter
  vintage[1:(j-as.integer(L/2)-1),j]<-yhat_Lag[(as.integer(L/2)+1):(j-1),
  as.integer(L/2)+1]
# last L/2 releases
  vintage[1:as.integer(L/2),j]<-yhat_Lag[L,L:(as.integer(L/2)+2)]
}
vintage_triangle<-vintage
dimnames(vintage_triangle)[[2]]<-paste("Publ. ",1:len,sep="")
dimnames(vintage_triangle)[[1]]<-paste("Target ",1:len,sep="")
@

<<echo=True>>=
colo<-rainbow(len)
file = paste("z_vintages_sa.pdf", sep = "")
pdf(file = paste(path.out,file,sep=""), paper = "special", width = 6, height = 6)
par(mfrow=c(2,1))
ymin<-min(vintage,na.rm=T)
ymax<-max(vintage,na.rm=T)
ts.plot(vintage[,L],col=colo[1],ylim=c(ymin,ymax),
main="Tentacle plot: vintages and full revision sequence",ylab="Vintages")
for (j in (L+1):len)
{
  lines(vintage[,j],col=colo[j])
}
abline(v=c(L,len-L+1),col="black",lty=2)
mtext("Start of symmetric (final) filter", side = 3, line = -1,at=L,col="black")
mtext("End of symmetric (final) filter", side = 3, line = -1,at=len-L+1,col="black")
ymin<-min(vintage,na.rm=T)
ymax<-max(vintage,na.rm=T)
ts.plot(yhat_Lag[,1],col="blue",ylim=c(ymin,ymax),
  main="Initial release (blue), last vintage (red) and true signal (green)",
  ylab="Vintages")
lines(vintage[,len],col="red")
lines(z,col="green")
abline(v=c(L,len-L+1),col="black",lty=2)
mtext("Start of symmetric (final) filter", side = 3, line = -1,at=L,col="black")
mtext("End of symmetric (final) filter", side = 3, line = -1,at=len-L+1,col="black")
dev.off()
@
<<label=z_vintages_sa.pdf,echo=FALSE,results=tex>>=
  file = paste("z_vintages_sa.pdf", sep = "")
  cat("\\begin{figure}[H]")
  cat("\\begin{center}")
  cat("\\includegraphics[height=6in, width=6in]{", file, "}\n",sep = "")
  cat("\\caption{Tentacle plot (top) and initial release vs. last vintage and true signal (bottom)", sep = "")
  cat("\\label{z_vintages_sa}}", sep = "")
  cat("\\end{center}")
  cat("\\end{figure}")
@
The tentacle plot in the upper graph is fairly close to a line -- no visisble tentacles actually --
which implies that revisions are negligible. In comparison, the plots in figs.\ref{z_vintages} and \ref{z_vintages_2}
obtained in the context of \emph{trend}-extraction, showed evidence of `tentacles' in particular in the vicinity
of turning-points. A comparison of the initial release (blue) and the final vintage (red) as well as the true
signal ($z_t$ in \ref{z_t}) in the bottom graph of
fig.\ref{z_vintages_sa} confirms Our initial claim: initial and `final' releases, as obtained between
the two vertical shaded lines, are almost indistinguishable (the green line dominates because it is printed last)
and in particular the initial release (blue) is not delayed
when compared to the output of the symmetric filter (red) or the true signal (green). Note that observations on the left of the first vertical (dashed) line
are obtained by filters with $Lag=(L+1)/2,(L+1)/2+1,...,L-1$. This facility is of practical relevance when $L$ is `large' because
the first $L-1$ observations of the initial (real-time) filter are unobservable (NA's); thus in the extreme case $L=T$ We would have
only one single observation available: by allowing $Lag$ to vary from $Lag=0$ to $Lag=L-1$, as We did above, We thus obtain a SA-series of full
length $T$. Interestingly, therefore, a full revision-sequence might still be useful: not for improving initial releases (which was
the original purpose and intention) but for computing \emph{full-length} SA.



\subsection{General Composite Seasonal Spectrum}            \label{composite_SA}

We now allow for a much richer seasonal structure, than the previous single line spectrum in $\pi/6$, by generating
a `mixed-bag' spectrum.


\subsubsection{The DGP: Deterministic, Stochastic, Stationary and Non-Stationary Seasonal `Mixed-Bag'}

We here consider a more plausible (but still artificial) design based on shorter
sample length (ten years of monthly data: $T=120$) and a more complex seasonal pattern based
on a mix of a deterministic seasonal in $\pi/6$, a non-stationary stochastic seasonal with
peaks in $\pi/2$ and $\pi$ and a less regular stationary seasonal with a peak in frequency
$4\pi/6$: the latter peak is a bit broader because a stationary seasonal component is
`less stable'\footnote{Economic data often shows evidence of broader peaks in
frequency $4\pi/6$ because of the potential existence of additional calendar effects.}. The data $x_t$
is generated as follows:
\begin{eqnarray*}
z_t&=&0.9z_{t-1}+\epsilon_{1t}\\
y_t&=&y_{t-4}+\epsilon_{2t}\\
w_t&=&-0.995w_{t-1}-0.990025w_{t-2}+\epsilon_{3t}\\
x_t&=&cos(t\pi/6)+z_t+y_t+w_t
\end{eqnarray*}
where $cos(t\pi/6)$ is the line-spectrum in $\pi/6$, $y_t$ is the non-stationary seasonal
with peaks in $\pi/2,\pi$, $w_t$ is the stationary seasonal with a peak in
$4\pi/6$\footnote{The coefficients $a_1,a_2$ were determined according to $a_2=-(0.995^2), a_1=
2*cos(4\pi/6)\sqrt(-a_2)$ which, according to the ARIMA-script (script number 1) generates
a cycle peaking in frequency $4\pi/6$.} and $z_t$ adds low-frequency mass frequently found in
economic data. This generic empirical setting allows to replicate much richer seasonal structures,
as typically found in applications, than `simple' SARIMA-models based on the
seasonal difference $\frac{1}{1-B^{12}}$ AR(12)-operator (as an example, the latter would generate peaks in
all seasonal frequencies $k\pi/6, k=1,...,6$ which is a rigid pattern).\\
The following
R-chunk generates all of the above series, computes the aggregate $x_t$ as well as its periodogram, see fig.\ref{z_seas_a_g}. Note that
We scaled (standardized) all stochastic seasonal components, for graphical convenience, and that We generated series of length
$T=120$ (as intended: 10 years of monthly data) as well as 1000: the shorter sample is for estimation whereas the longer
sample will be used for out-of-sample evaluation.

<<echo=True>>=
# Simulation of the `complex' seasonal series
len<-120
len1<-1000
set.seed(10)
ar1<-0.9
z<-arima.sim(list(ar=ar1),n=len1)
eps<-rnorm(3000)
yh<-eps
for (i in 5:length(eps))
  yh[i]<-yh[i-4]+eps[i]
y<-scale(yh[(length(eps)-len1+1):length(eps)])
a_2<--0.995^2
a_1<-2*cos(4*pi/6)*sqrt(-a_2)
w<-scale(arima.sim(list(ar=c(a_1,a_2)),n=len1))
xh<-z+cos((1:len1)*pi/6)+y+w
x<-xh[1:len]
plot_T<-F

file = paste("z_seas_a_g.pdf", sep = "")
pdf(file = paste(path.out,file,sep=""), paper = "special", width = 6, height = 6)
par(mfrow=c(3,1))
ts.plot(x)
acf(x)
plot(per(x,plot_T)$per,type="l",axes=F,ylab="Periodogram")
axis(1,at=1+0:6*len/12,labels=c("0","pi/6","2pi/6","3pi/6","4pi/6","5pi/6","pi"))
axis(2)
box()
dev.off()
@
<<label=z_seas_a_g.pdf,echo=FALSE,results=tex>>=
  file = paste("z_seas_a_g.pdf", sep = "")
  cat("\\begin{figure}[H]")
  cat("\\begin{center}")
  cat("\\includegraphics[height=4in, width=4in]{", file, "}\n",sep = "")
  cat("\\caption{Seasonal series (top), acf (middle) and periodogram (bottom)", sep = "")
  cat("\\label{z_seas_a_g}}", sep = "")
  cat("\\end{center}")
  cat("\\end{figure}")
@
As expected, the periodogram reflects all seasonal components (peaks). Note that We do not care
about the essence of a seasonal component: whether a peak is generated by a deterministic ($\pi/6$) or a
stochastic, stationary ($4\pi/6$) or non-stationary ($\pi/2,\pi$) component is irrelevant. We ultimately just care about matching
\emph{location}, \emph{height} (power/magnitude) and \emph{width} (regularity/stability) of
dominant peaks; in SA, in particular, as well as in signal extraction, in general.


\subsubsection{The Gamma-Interface}             \label{gamma_int}

We associate `naturally' spectral peaks in the seasonal frequencies $k\pi/6, k=1,...,6$ of a
monthly series with a seasonal component and We don't care whether this component is a mixed-bag -- a heterogeneous
aggregate of different components -- or a single homogenous process.
SA of $x_t$ requires a filter which matches location, height and width of the seasonal peaks, as revealed in fig.\ref{z_seas_a_g}, bottom plot. We could now specify
a symmetric target filter $\Gamma(\cdot)$ manually, by imposing zeroes at the relevant frequencies, as We did in section \ref{sa_ls}.
Alternatively, one could think about an automatic procedure which would design $\Gamma(\cdot)$ accordingly. The
function in the following R-chunk is a first attempt in this direction: it specifies a symmetric target $\Gamma(\cdot)$ based on the periodogram
as well as a list of pre-specified seasonal frequencies\footnote{The function doesn't `know' a priori at which rate
a series is sampled: monthly, daily, quarterly,...: therefore the user has to supply this information.} where potential peaks could (or not) occur. It allows also to tackle
calendar effects by specifying a corresponding list of candidate-frequencies. We would like to emphasize that
the following code is prototypical and thus subject to (multiple) improvements: but We are not interested in pushing this project
further. We invite interested readers to `go through' the code; in case of desinterest you may simply skip
the following chunk.

<<echo=True>>=
Gamma_interface<-function(list_freq_seasonal,list_freq_calendar,spect)
{
  K<-length(spect)-1
# determine asymmetrical width of seasonal peaks
#    (calendar peaks are assumed to be line spectra)
# Two rules are used:
#   The monotonic decay on both sides of the peak (as long as its decaying
#     it's still part of the peak)
#   The absolute value of the peak (which must be larger than
#     med_times the medians on both sides)
  med_times<-4
  spec_inflate<-4
  med_spec_u<-1:(length(list_freq_seasonal)-1)
  med_spec_l<-1:(length(list_freq_seasonal)-1)
  len_u<-list_freq_seasonal
  len_l<-len_u

  for (i in 1:(length(list_freq_seasonal)))#i<-6
  {
    med_spec_l[i]<-2*ifelse(i==1,NA,median(spect[(list_freq_seasonal[i]-1):
    (list_freq_seasonal[i-1]+1)]))
    med_spec_u[i]<-2*ifelse(i==length(list_freq_seasonal),
    NA,median(spect[(list_freq_seasonal[i]+1):(list_freq_seasonal[i+1]-1)]))
    if (i<length(list_freq_seasonal))
    {
# median rule on the right half of the peak (peaks can be asymmetric)
      med_u<-which(!(spect[list_freq_seasonal[i]:list_freq_seasonal[i+1]]>
      med_times*med_spec_u[i]))[1]-1
      med_u<-ifelse(length(med_u)==0,list_freq_seasonal[i+1]-list_freq_seasonal[i],med_u)
# monotonic decay rule on the right half of the peak
      diff_u<-which(!diff(spect[list_freq_seasonal[i]:list_freq_seasonal[i+1]])<0)[1]-1
      diff_u<-ifelse(length(diff_u)==0,list_freq_seasonal[i+1]-list_freq_seasonal[i],diff_u)
      len_u[i]<-min(diff_u,med_u)
    } else
    {
      len_u[i]<-0
    }
    if (i>1)
    {
# same as above but on the left side of the peak: median and monotonic decay rules
      med_l<-which(!(spect[list_freq_seasonal[i]:list_freq_seasonal[i-1]]>
      med_times*med_spec_l[i]))[1]-1
      med_l<-ifelse(length(med_l)==0,list_freq_seasonal[i]-list_freq_seasonal[i-1],med_l)
      diff_l<-which(!diff(spect[list_freq_seasonal[i]:list_freq_seasonal[i-1]])<0)[1]-1
      diff_l<-ifelse(length(diff_l)==0,list_freq_seasonal[i]-list_freq_seasonal[i-1],diff_l)
      len_l[i]<-min(diff_l,med_l)
    } else
    {
      len_l[i]<-0
    }
  #  which(diff(which(spect[list_freq[i]:list_freq[i-1]]>med_spec[i-1]))>1)[1]
  }

# Formatting Gamma based on location, width and height of peaks as determined above
  Gamma<-rep(1,K+1)
  for (i in 1:(length(list_freq_seasonal))) #i<-6
  {
  # If the width of the peak is not null (location problem: where do I have peaks)
    if (len_l[i]+len_u[i]>0)
    {
# determining the width
      width_peak<-(list_freq_seasonal[i]-max(0,len_l[i]-1)):
      (list_freq_seasonal[i]+max(0,len_u[i]-1))
# determining the height (deepness of target trough/dip)
      Gamma[width_peak]<-sqrt(min(na.exclude(c(med_spec_l[i],med_spec_u[i]))))/
      (spec_inflate*sqrt(spect[width_peak]))
# We dot not allow for amplitudes larger than one
      Gamma[which(Gamma>1)]<-1
# We drill a zero in the seasonal frequency (if it is loaded by a peak)
      Gamma[list_freq_seasonal[i]]<-0
    }
  }
  if (length(list_freq_calendar)>0)
    Gamma[list_freq_calendar]<-0

#  plot_T<-F
#  plot(Gamma,type="l",axes=F,ylim=c(0,1))
#  lines(per(x_data,plot_T)$per/max(per(x_data,plot_T)$per),col="red")
#  axis(1,at=1+(0:6)*K/6,labels=c("0","pi/6","2pi/6","3pi/6","4pi/6","5pi/6","pi"))
#  axis(2)
#  box()

  return(list(Gamma=Gamma))
}
@
The function requires the following inputs:
\begin{itemize}
\item $list\textunderscore freq\textunderscore seasonal$: a list of seasonal frequencies: the
function checks whether the frequencies are `loaded' by spectral peaks and assigns dips to
$\Gamma(\cdot)$ if affirmative\footnote{We deliberately ignore statistical tests about the significance
of a periodogram peak because classical inferential tools do not perform well in this
particular descriptive/explorative application. We are aware of the fact that Our `rule-based'
proecdure is rather ad hoc and might be subject to substantial improvement. Interested readers are
welcome to do so.}.
\item $list\textunderscore freq \textunderscore calendar$: a list of calendar frequencies. The function does
not check the presence or not of spectral peaks: a single zero is automatically
assigned to $\Gamma(\cdot)$ in each frequency of the submitted list\footnote{Calendar effects are generally
deterministic with narrow line spectra: therefore a single zero of $\Gamma(\cdot)$ is generally sufficient.}.
\item $spect$: a spectral estimate. In Our case the periodogram.
\end{itemize}


\subsubsection{Automatic Seasonal Adjustment}

We now feed Our data $x_t$ to the above automatic Gamma-interface and then proceed to real-time SA.
Fig.\ref{z_seas_agi} compares the generated target filter and the periodogram of $x_t$.


<<echo=True>>=
weight_func<-per(x,plot_T)$per
spect<-weight_func
K<-length(weight_func)-1
list_freq_seasonal<-round(1+K*c((1:6)/6))
list_freq_calendar<-NULL#round(1+K*(4/6+0.1/pi))

Gamma<-Gamma_interface(list_freq_seasonal,list_freq_calendar,weight_func)$Gamma

file = paste("z_seas_agi.pdf", sep = "")
pdf(file = paste(path.out,file,sep=""), paper = "special", width = 6, height = 6)
plot_T<-F
plot(Gamma,type="l",axes=F,ylim=c(0,1),
main="Periodogram of input (red) and automatic target (black)")
lines(per(x,plot_T)$per/max(per(x,plot_T)$per),col="red")
axis(1,at=1+(0:6)*K/6,labels=c("0","pi/6","2pi/6","3pi/6","4pi/6","5pi/6","pi"))
axis(2)
box()
dev.off()
@
<<label=z_seas_agi.pdf,echo=FALSE,results=tex>>=
  file = paste("z_seas_agi.pdf", sep = "")
  cat("\\begin{figure}[H]")
  cat("\\begin{center}")
  cat("\\includegraphics[height=4in, width=4in]{", file, "}\n",sep = "")
  cat("\\caption{Automatic Gamma-interface", sep = "")
  cat("\\label{z_seas_agi}}", sep = "")
  cat("\\end{center}")
  cat("\\end{figure}")
@
The dips of the target filter match location, height and width of the seasonal components quite well: We can see that
the dip of the potentially unstable stationary seasonal in $4\pi/6$ is treated differently by receiving a broader dip.
The next fig.\ref{z_seas_a_a_asag} plots amplitude and time-shift functions of two mean-square
real-time filters of lengths $L=24$ and $L=60$.


<<echo=True>>=
# Length of Moving-Average:
# Trick: selecting L=24 to be a multiple of 6 (the quotient in the
#   frequency pi/6 of the cosine) allows
#   the filter to damp/eliminate the component more easily!
L_vec<-c(24,len/2)
b_list<-as.list(1:length(L_vec))
xf<-matrix(nrow=len1,ncol=length(L_vec))
trffkt<-matrix(nrow=K+1,ncol=length(L_vec))
amp<-trffkt
shift<-trffkt
# Customization: lambda=eta=0 implies minimal mean-square revisions
lambda<-0
eta<-0
# cutoff frequency: this parameter is inactive if eta=0
cutoff<-pi/6
# Lag: Lag=0: real-time filter;
Lag<-0
# We do not impose filter restrictions: see exercises below
i1<-F
i2<-F
# Compute analytical DFA
for (i in 1:length(L_vec))
{
  dfa_mse<-dfa_analytic(L_vec[i],lambda,weight_func,Lag,Gamma,eta,cutoff,i1,i2)
  b_list[[i]]<-dfa_mse$b
  trffkt[,i]<-dfa_mse$trffkt
  amp[,i]<-abs(trffkt[,i])
  shift[,i]<-Arg(trffkt[,i])/(pi*(0:(K))/(K))
  for (j in L_vec[i]:len1)
  {
    xf[j,i]<-b_list[[i]]%*%xh[j:(j-L_vec[i]+1)]
  }
}
# Final Symmetric filter
L<-61
Lag<-(L-1)/2
dfa_symmetric<-dfa_analytic(L,lambda,weight_func,Lag,Gamma,eta,cutoff,i1,i2)
b_symmetric<-dfa_symmetric$b
xf_symmetric<-rep(NA,len1)
for (j in ((L-1)/2+1):(len1-(L-1)/2))
{
  xf_symmetric[j]<-b_symmetric%*%xh[(j+(L-1)/2):(j-(L-1)/2)]
}
colo<-rainbow(2*length(L_vec))
file = paste("z_seas_a_a_asag.pdf", sep = "")
pdf(file = paste(path.out,file,sep=""), paper = "special", width = 6, height = 6)
par(mfrow=c(2,1))
plot(Gamma,type="l",axes=F,col="black",ylim=c(0,max(amp)),ylab="",xlab="",main="Amplitude")
mtext("Target", side = 3, line = -1,at=K/2,col="black")
for (i in 1:length(L_vec))
{
  lines(amp[,i],lty=1,col=colo[i])
  mtext(paste("Amplitude: L=",L_vec[i],sep=""), side = 3, line = -1-i,at=K/2,col=colo[i])
}
axis(1,at=1+0:6*K/6,labels=c("0","pi/6","2pi/6","3pi/6","4pi/6","5pi/6","pi"))
axis(2)
box()
plot(rep(0,K+1),type="l",axes=F,col="black",ylim=c(min(na.exclude(shift)),
max(na.exclude(shift))),ylab="",xlab="",main="Time-shift")
mtext("Target", side = 3, line = -1,at=K/2,col="black")
for (i in 1:length(L_vec))
{
  lines(shift[,i],lty=1,col=colo[i])
  mtext(paste("Shift: L=",L_vec[i],sep=""), side = 3, line = -1-i,at=K/2,col=colo[i])
}
axis(1,at=1+0:6*K/6,labels=c("0","pi/6","2pi/6","3pi/6","4pi/6","5pi/6","pi"))
axis(2)
box()
dev.off()
@
<<label=z_seas_a_a_asag.pdf,echo=FALSE,results=tex>>=
  file = paste("z_seas_a_a_asag.pdf", sep = "")
  cat("\\begin{figure}[H]")
  cat("\\begin{center}")
  cat("\\includegraphics[height=6in, width=6in]{", file, "}\n",sep = "")
  cat("\\caption{Real-time SA-filters: amplitude (top) and shift (bottom)", sep = "")
  cat("\\label{z_seas_a_a_asag}}", sep = "")
  cat("\\end{center}")
  cat("\\end{figure}")
@
Both real-time filters damp the undesirable seasonal components in the automatically assigned stop-bands
and both filters are reasonably close to the identity in the passband. We therefore expect real-time estimates
to perform quite well and We expect revision errors to be small.

\subsubsection{Out-of-Sample Performances: Amplitude Effect}

In order to verify Our claims
We first plot the periodograms of input and output signals for a long \emph{out-of-sample}
period, see fig.\ref{z_seas_per_oos_sa}.

<<echo=True>>=
file = paste("z_seas_per_oos_sa.pdf", sep = "")
pdf(file = paste(path.out,file,sep=""), paper = "special", width = 6, height = 6)
plot(per(xh[max(L_vec):len1],plot_T)$per,type="l",axes=F,col="blue",
ylab="Periodogram",xlab="Frequency",
main="Comparisons of periodograms of input and output signals")
mtext("Input", side = 3, line = -1,at=len1/4,col="blue")
for (i in 1:length(L_vec))
{
  lines(per(xf[max(L_vec):len1,i],plot_T)$per,col=colo[i])
  mtext(paste("Output: L=",L_vec[i],sep=""), side = 3, line = -1-i,at=len1/4,col=colo[i])
}
axis(1,at=1+0:6*(len1-max(L_vec))/12,labels=c("0","pi/6","2pi/6","3pi/6",
"4pi/6","5pi/6","pi"))
axis(2)
box()
dev.off()
@
<<label=z_seas_per_oos_sa.pdf,echo=FALSE,results=tex>>=
  file = paste("z_seas_per_oos_sa.pdf", sep = "")
  cat("\\begin{figure}[H]")
  cat("\\begin{center}")
  cat("\\includegraphics[height=6in, width=6in]{", file, "}\n",sep = "")
  cat("\\caption{Periodograms of input and real-time outputs: out-of-sample", sep = "")
  cat("\\label{z_seas_per_oos_sa}}", sep = "")
  cat("\\end{center}")
  cat("\\end{figure}")
@
The periodograms are almost indistinguishable in the passband, as desired. In the stopband
the seasonal peaks of the input do not appear in the out-of-sample filter outputs: they have been effectively
damped\footnote{We could `eliminate' any residual power in the seasonal frequencies by customization i.e. by
artificially overemphasizing the height of the seasonal peaks, recall Our proceeding in section \ref{sa_ls}, p.\ref{sa_cust}.}.
We infer from this analysis that the SA-task seems to be effective in terms of amplitude effects (removal of seasonal components).
We now check whether the filter outputs are delayed or not: this effect cannot be quantified by the periodogram plots (the DFT's would
be informative though).\\


\subsubsection{Out-of-Sample Performances: Time-Shift and Revision Error}

We now
compare outputs of final symmetric filter and of the above real-time filters \emph{out-of-sample}, see fig.\ref{z_sym_rt_sa_oos}.
<<echo=True>>=
file = paste("z_sym_rt_sa_oos.pdf", sep = "")
pdf(file = paste(path.out,file,sep=""), paper = "special", width = 6, height = 6)
par(mfrow=c(2,1))
mplot<-cbind(xf_symmetric,xf)
ymin<-min(mplot,na.rm=T)
ymax<-max(mplot,na.rm=T)
ts.plot(mplot[,3],col="blue",ylim=c(ymin,ymax),
main="Final and real-time filter outputs (out-of-sample)",ylab="Vintages")
mtext("Final (symmetric) filter",col="blue", side = 3, line = -1,at=dim(mplot)[1]/2)
for (j in 1:length(L_vec))
{
  lines(mplot[,j],col=colo[j])
  mtext(paste("Real-time filter: L=",L_vec[j],sep=""),col=colo[j], side = 3,
  line = -1-j,at=dim(mplot)[1]/2)
}
abline(v=c(L,len1-L+1),col="black",lty=2)
mtext("Start of symmetric (final) filter", side = 3, line = -1,at=L,col="black")
mtext("End of symmetric (final) filter", side = 3, line = -1,
at=dim(mplot)[1]-L+1,col="black")
anf<-400
enf<-600
mplot<-cbind(xf_symmetric,xf)[400:600,]
ymin<-min(mplot,na.rm=T)
ymax<-max(mplot,na.rm=T)
plot(mplot[,3],type="l",col="blue",ylim=c(ymin,ymax),
main="Final and real-time filter outputs (out-of-sample)",
ylab="Outputs",xlab="", axes=F)
mtext("Final (symmetric) filter",col="blue", side = 3, line = -1,at=dim(mplot)[1]/2)
for (j in 1:length(L_vec))
{
  lines(mplot[,j],col=colo[j])
  mtext(paste("Real-time filter: L=",L_vec[j],sep=""),col=colo[j], side = 3,
  line = -1-j,at=dim(mplot)[1]/2)
}
axis(1,at=c(1,as.integer(((enf-anf)/4)*1:4)),labels=anf+c(0,as.integer(((enf-anf)/4)*1:4)))
axis(2)
box()
dev.off()
@
<<label=z_sym_rt_sa_oos.pdf,echo=FALSE,results=tex>>=
  file = paste("z_sym_rt_sa_oos.pdf", sep = "")
  cat("\\begin{figure}[H]")
  cat("\\begin{center}")
  cat("\\includegraphics[height=6in, width=6in]{", file, "}\n",sep = "")
  cat("\\caption{Comparison of real-time (green: L=60; red: L=24) and final (blue) out-of-sample outputs", sep = "")
  cat("\\label{z_sym_rt_sa_oos}}", sep = "")
  cat("\\end{center}")
  cat("\\end{figure}")
@
The out-of-sample outputs of the real-time filters (green and red) are pretty close to the final filter output (blue), as expected.
A closer view to the out-of-sample period from $t=400:600$ in the lower graph confirms that neither real-time filter is delayed, as
was to be expected from the time-shift plots in fig.\ref{z_seas_a_a_asag}. Let us emphasize, once again, that real-time SA is not
a difficult estimation problem because the stopband is invariably narrow: the amplitude function is close to an identity
up to a finite set of (generally) narrow seasonal dips. Therefore, revisions could be ignored by all practical means. To be more specific:
the revision error due to SA-filtering (increasing $Lag$) is likely to be negligible when compared to the magnitude of
`ordinary' data-revisions.

\subsection{Real-Time Seasonal Adjustment and Misspecification}

Real-time SA is invariably an easy estimation task -- a breeze when compared to trend extraction --. Interestingly,
data can be effectively adjusted even under deliberate (severe) model-misspecification. The interested reader is referred
to a comprehensive SEFBlog-tutorial on the topic: \url{http://blog.zhaw.ch/idp/sefblog/index.php?/archives/309-Seasonal-Adjustment-The-Epilogue.html}.






\section{Trend Extraction}  \label{rttee}

Real-time trend extraction is generally (much) challenging than real-time seasonal adjustment because
one attempts to remove or damp components in a broad/wide/large high-frequency stop-band. Therefore both the
amplitude as well as the time-shift of the real-time filter will more or less heavily deviate from the idealized lowpass target, in
contrast to real-time SA-filters, which are invariably close to the target filter (by transitivity since both are close to the identity).
As a result, customization of the amplitude-fit in the stop-band (Smoothness) by $\eta$ as well as the
customization of the phase or time-shift function in the passband (Timeliness) by $\lambda$, in criterion \ref{idfatp},
will affect noticeably real-time filter characteristics. The following sections are devoted to the topic.




\subsection{Performance Measures}\label{tdpm}


We here propose a set of four performances measures, two classical time-domain and two frequency-domain statistics, which
emphasize timeliness and smoothness issues but differently than (complementary to) the ATS-decomposition \ref{ats_mse}. Our intention
here is to provide additional evidence that customization obtained by \ref{ats_cust} or, more generally, by \ref{idfatp} indeed tackles
user-priorities `the right way'.

\subsubsection{Time-Domain: Curvature and Peak-Correlation}\label{curvature_peak_corr}

In contrast to SA, performances of real-time trend filters can substantially deviate from the target. In order to compare
filter-designs We therefore propose a set of well-known and well-documented time-domain performance measures which can be linked to
Selectivity and Timeliness as defined in section \ref{ats_section}. The first measure computes relative
\emph{mean-square second-order differences} of a filter-output
$\hat{y}_t$:
\begin{equation}\label{mse_2diff}
\textrm{Curvature}:=\frac{E\left[\Big((1-B)^2 \hat{y}_t\Big)^2\right]}{Var(\hat{y_t})}\approx
\frac{\sum_{t=3}^T \Big(\hat{y}_t-2\hat{y}_{t-1}+\hat{y}_{t-2}\Big)^2}{\sum_{t=1}^T(\hat{y}_t-\overline{\hat{y}})^2}
\end{equation}
where $\overline{\hat{y}}$ is the (arithmetic) mean of $\hat{y}_t$.
Note that We now identify `Curvature' with the sample-estimate
on the right hand-side. Mean-square second-order differences emphasize the geometric
curvature: if the filter-output $\hat{y}_t$ appears smooth then
Curvature is small. As an extreme example, if $\hat{y}_t$ is a linear function (of time), then Curvature vanishes. Note that
the squared second-order differences $\Big(\hat{y}_t-2\hat{y}_{t-1}+\hat{y}_{t-2}\Big)^2$ could be made arbitrarily small by
simple scaling of the filter output. In order to avoid easy `cheating' We therefore divide by the variance
of the signal. Curvature does not explicitly refer to Timeliness but it is related to Selectivity, as shown below.  \\

The concept of \emph{peak-correlation} is another well-established performance-measure which is defined by
\begin{equation}\label{peak_corr}
\max_j(cor(y_t,\hat{y}_{t+j}))\approx \max_j\sum_{k=1}^{T-j}({y}_{k}-\overline{{y}})(\hat{y}_{k+j}-\overline{\hat{y}})
\end{equation}
where $y_t$ is the target (in our case it is the output of the final symmetric filter) and $\hat{y}_t$ is the estimate (in our
case a real-time filter output). Note that We are not primarily interested in the magnitude of the correlations, but
in the lead or lag $j_0$ at which the correlation between
the target $y_t$ and the shifted (real-time) estimate $\hat{y}_{t+{j_0}}$ is maximized: this will
be called `Peak Correlation'.  A
positive Peak Correlation $j_0$ means a lag/delay by the real-time filter and turning-points are delayed and conversely for negative $j_0$.
Obviously, $j_0$ addresses
directly Timeliness since a higher time-shift of the filter  affects the lag at which
the above correlation `peaks'.


\subsubsection{Frequency-Domain: Selectivity and Mean-Shift}

Filter characteristics are uniquely described by amplitude und time-shift functions. We here propose two measures
which summarize potentially relevant effects in a way complementary (but not identical) to Timeliness and Smoothness error components.
\emph{Selectivity} relates pass- and stopband amplitude characteristics
\begin{equation}\label{effective_effect}
\textrm{Selectivity}(\alpha):=\frac{\int_{\textrm{Passband}}\hat{A}(\omega)^2h(\omega)d\omega}{\int_{\textrm{Stopband}}\hat{A}(\omega)^2h(\omega)|\omega|^{\alpha}\omega}\approx
\frac{\sum_{\textrm{Passband}}\hat{A}(\omega_k)^2I_{TX}(\omega_k)}{\sum_{\textrm{Stopband}}\hat{A}(\omega_k)^2I_{TX}(\omega_k)|\omega_k|^{\alpha}}
\end{equation}
For $\alpha=0$ this number can be interpreted as the ratio of the spectral mass of the filter-output in the \emph{passband} relative to the
spectral mass of the filter-output in the \emph{stopband}: if a filter separates passband and stopband components better, then
the Selectivity improves i.e. Selectivity$(\alpha)$ \emph{increases}. In practice, We have found that a broad user-community tends to prefer trend-estimates which appear `smooth'
in \emph{visual terms} i.e. with less noisy high-frequency ripples.
Since high-frequency noise components contaminate more heavily
the filter-output (the derivative of a trigonometric signal is proportional to frequency) We therefore propose to set $\alpha=2$ in
\ref{effective_effect}: noisy high-frequency components are then leveraged by their derivative (the square exponent reflects
the fact that $\hat{A}(\omega_k)^2I_{TX}(\omega_k)$ addresses a mean-square term). Selectivity(2) matches preferences of a broad
user-community better than Selectivity(0). We now drop $\alpha=2$ in the above
appellation and call the resulting performance measure `Selectivity', for short.
In contrast
to Smoothness, Selectivity is immune to scaling effects of the amplitude i.e. it is invariant to a constant
multiplying the amplitude function.\\

With respect to time-shift performances We propose the following Mean-Shift statistic
\begin{equation}\label{mean_shift}
\textrm{Mean-Shift}:=\frac{1}{\textrm{Number ~of~}\omega_k\textrm{~in ~passband}}\sum_{\textrm{Passband}}\hat{\phi}(\omega_k)
\end{equation}
where $\hat{\phi(\cdot)}$ is the time-shift function. Our focus on the passband in this latter measure
is due to the fact that potentially interesting
turning-points of a series are linked to passband-components: therefore the Mean-Shift is an approximate measure of
the relevant delay of a real-time filter. In contrast to Timeliness, in which the amplitude and the time-shift are entangled,
the Mean-Shift is insensitive to the amplitude function.\\

In contrast to Timeliness and Smoothness components all presented measures are insensitive
to scaling effects of the output signal (multiplying the output by a positive constant),
as desired.



\subsubsection{A Link Between Curvature and $\textrm{Smoothness}^{*}$}

A link between Smoothness and Curvature could be obtained by noting that
\begin{eqnarray*}
\textrm{Curvature}&=&\frac{\sum_{t=3}^T \Big(\hat{y}_t-2\hat{y}_{t-1}+\hat{y}_{t-2}\Big)^2}{\sum_{t=1}^T(\hat{y}_t-\overline{\hat{y}})^2}\\
&\approx&\frac{\sum_{k=-T/2}^{T/2}|1-\exp(-i\omega_k)|^4I_{T\hat{Y}}(\omega_k)}{\sum_{k=-T/2}^{T/2}I_{T\hat{Y}}(\omega_k)}
\end{eqnarray*}
where We used the decomposition \ref{spec_dec_per} for the denominator and the convolution result \ref{convolution_per} together
with the decomposition \ref{spec_dec_per} for the numerator in the above quotient, noting that $\hat{y}_t-2\hat{y}_{t-1}+\hat{y}_{t-2}$
is the output of the second-order difference filter (applied to $\hat{y}_t$) whose transferfunction is $(1-\exp(-i\omega_k))^2$. \\
The customized Smoothness-term is defined as
\begin{eqnarray*}
\textrm{S(moothness)}&:=&\frac{2\pi}{ T} \sum_{\textrm{Stopband}} (A(\omega_k)-\hat{A}(\omega_k))^2W_2(\omega_k) I_{TX}(\omega_k)\\
&:=&\frac{2\pi}{ T} \sum_{\textrm{Stopband}} \hat{A}(\omega_k)^2 W_2(\omega_k) I_{TX}(\omega_k)\\
&\approx&\frac{2\pi}{ T} \sum_{\textrm{Stopband}}W_2(\omega_k) I_{T\hat{Y}}(\omega_k)
\end{eqnarray*}
where We used the fact that $A(\omega_k)=0$ in the stopband and where $ \hat{A}(\omega_k)^2I_{TX}(\omega_k)\approx I_{T\hat{Y}}(\omega_k)$ by
convolution \ref{convolution_per}.  The numerator of Curvature differs from Smoothness by the respective weighting-functions assigned
to $I_{T\hat{Y}}(\omega_k)$\footnote{Note that both weighting-functions are monotonically increasing with frequency.}. Obviously,
there is a link between both expressions.

\subsubsection{R-Code: Performance Measures}


The following function computes the ATS-components as well as the above time-domain and frequency-domain performance measures:
Curvature, Peak-Correlation, Selectivity and Mean-Shift. The periodogram
will be based on a sample-length $T=120$ (10 years of monthly data) but time-domain performances will be computed on a much longer
series (x1) of length 1000 such that We can compute and compare in-sample and
out-of-sample performances. The function
computes a whole bunch of results useful for evaluation:
\begin{itemize}
\item Real-time filters ($Lag=0$) for designs specified in the vectors lambda\textunderscore vec and eta\textunderscore vec
(We can evaluate several designs at once).
\item A symmetric (final) MSE-filter which is our reference for deriving peak-correlations.
\item A theoretically best mean-square real-time filter, assuming knowledge of the true
DGP, which allows to benchmark customized designs in terms of
out-of-sample performances.
\item ATS-components and our four performance measures: Selectivity, Mean-Shift, Curvature
and Peak-Correlation.
\end{itemize}



<<echo=True>>=
# This function computes:
# 1. real-time and symmetric filters,
# 2. in- and out-of-sample performances: Selectivity, Mean-Shift, Curvature
#     and Peak-Correlation
# 3. the ATS error components
#
#-----------
#
Performance_func<-function(lambda_vec,eta_vec,len1,len,x1,weight_func,Gamma,
cutoff,L,L_sym,a1,mba)
{
  K<-length(weight_func)-1
  omega_k<-(0:K)*pi/K
  #
  amp<-matrix(ncol=length(lambda_vec)+1,nrow=K+1)
  shift<-amp
  selectivity_insamp<-rep(NA,dim(amp)[2])
  mean_shift_insamp<-rep(NA,dim(amp)[2])
  curvature_insamp<-rep(NA,dim(amp)[2])
  selectivity_outsamp<-rep(NA,dim(amp)[2])
  mean_shift_outsamp<-rep(NA,dim(amp)[2])
  curvature_outsamp<-rep(NA,dim(amp)[2])

  b<-matrix(ncol=dim(amp)[2],nrow=L)
  # Determine passband
  omega_Gamma<-as.integer(cutoff*K/pi)+1
  passband<-1:omega_Gamma
  stopband<-(omega_Gamma+1):(K+1)
  Accuracy<-rep(NA,dim(amp)[2])
  Smoothness<-Accuracy
  Timeliness<-Accuracy
  Residual<-Accuracy
  lag_cor<-0:10
  peak_cor_insamp<-matrix(ncol=length(lag_cor),nrow=dim(amp)[2])
  peak_cor_outsamp<-matrix(ncol=length(lag_cor),nrow=dim(amp)[2])
  lag_max_insamp<-rep(NA,dim(amp)[2])
  lag_max_outsamp<-rep(NA,dim(amp)[2])
  xf<-matrix(nrow=len1,ncol=dim(amp)[2])
# Final symmetric MSE-filter
  Lag_sym<-(L_sym-1)/2
  lambda<-0
  eta<-0
  filt_sym<-dfa_analytic(L_sym,lambda,weight_func,Lag_sym,Gamma,eta,cutoff,i1,i2)
  trffkt_sym<-filt_sym$trffkt
  amp_sym<-abs(trffkt_sym)
  shift_sym<-Arg(trffkt_sym)/omega_k
  b_sym<-filt_sym$b
# Compute outputs on long sample
  xf_sym<-rep(NA,len1)
  for (j in (1+(L_sym-1)/2):(len1-(L_sym-1)/2))
    xf_sym[j]<-b_sym%*%x1[(L_sym-1)/2+j:(j-L_sym+1)]
  weight_func_best<-rep(1,K+1)/abs(1-a1*exp(1.i*omega_k))^2               #a1<-0
#
# Estimation loop for all eta/lambda values
  for (i in 0:(dim(amp)[2]-1))         #i<-1
  {
    if (i==0)
    {
      if (mba)
      {
        dfa<-dfa_analytic(L,lambda,weight_func,Lag,Gamma,eta,cutoff,i1,i2)
      } else
      {
        dfa<-dfa_analytic(L,lambda,weight_func_best,Lag,Gamma,eta,cutoff,i1,i2)
      }
    } else
    {
      dfa<-dfa_analytic(L,lambda_vec[i],weight_func,Lag,Gamma,eta_vec[i],cutoff,i1,i2)
    }

    b[,i+1]<-dfa$b                   #
    amp[,i+1]<-abs(dfa$trffkt)
# Selectivity: immunized against scaling effects
    selectivity_insamp[i+1]<-(amp[1:(K*cutoff/pi),i+1]^2%*%weight_func[1:(K*cutoff/pi)])/
    (amp[(K*cutoff/pi+1):(K+1),i+1]^2%*%(weight_func[(K*cutoff/pi+1):(K+1)]*
    (1+(0:(K-K*cutoff/pi)))^2))
    shift[1+1:K,i+1]<-Arg(dfa$trffkt)[1+1:K]/((pi*1:K)/K)
# Shift in frequency zero
    shift[1,i+1]<-b[,i+1]%*%(0:(L-1))/amp[1,i+1]
# Mean-shift statistic: immunized against scaling effects
    mean_shift_insamp[i+1]<-mean(shift[1:(K*cutoff/pi),i+1])
    amp_error<-((Gamma-amp[,i+1]))^2*weight_func
    shift_error<-4*Gamma*amp[,i+1]*sin(shift[,i+1]*omega_k/2)^2*weight_func
    Accuracy[i+1]<-sum(amp_error[passband])
    Smoothness[i+1]<-sum(amp_error[stopband])
    Timeliness[i+1]<-sum(shift_error[passband])
    Residual[i+1]<-sum(shift_error[stopband])
    yhat<-rep(NA,len1)
    for (j in L:len1)
      yhat[j]<-b[,i+1]%*%x1[j:(j-L+1)]
    xf[,i+1]<-yhat
# Relative Curvature (immunized against scaling effects)
    curvature_insamp[i+1]<-mean(diff(yhat[1:len],diff=2)^2,na.rm=T)/
    var(yhat[1:len],na.rm=T)
    curvature_outsamp[i+1]<-mean(diff(yhat[(len+1):len1],diff=2)^2,na.rm=T)/
    var(yhat[(len+1):len1],na.rm=T)
    for (j in lag_cor)   #j<-0
    {
      if (!mba)
         peak_cor_insamp[i+1,j+1]<-cor(xf_sym[(1+(L_sym-1)/2):len],
         yhat[j+((L_sym-1)/2+1):len])
       peak_cor_outsamp[i+1,j+1]<-cor(xf_sym[(len+1):(len1-L_sym)],
       yhat[j+(len+1):(len1-L_sym)])
    }
    if (!mba)
      lag_max_insamp[i+1]<-which(peak_cor_insamp[i+1,]==max(peak_cor_insamp[i+1,]))-1
    lag_max_outsamp[i+1]<-which(peak_cor_outsamp[i+1,]==max(peak_cor_outsamp[i+1,]))-1
  }
# Rownames
  if (!mba)
  {
    dim_names<-c("Theoretical best MSE",paste("Lambda=",lambda_vec,", eta=",eta_vec,sep=""))
    if (prod(lambda_vec*eta_vec)==0)
    {
      dim_names[1+which(lambda_vec==0&eta_vec==0)]<-"DFA-MSE"
    }
  } else
  {
    dim_names<-c("MBA-MSE",paste("Lambda=",lambda_vec,", eta=",eta_vec,sep=""))
  }

  dimnames(xf)[[2]]<-dim_names
  dimnames(amp)[[2]]<-dim_names
  dimnames(shift)[[2]]<-dim_names
  dimnames(b)[[2]]<-dim_names
#
# We collect all frequency-domain performance statistics in corresponding tables
  if (!mba)
  {
    amp_shift_mat_insamp<-cbind(selectivity_insamp,mean_shift_insamp,
    curvature_insamp,lag_max_insamp)
    dimnames(amp_shift_mat_insamp)[[2]]<-c("Selectivity","Mean-shift",
    "Curvature","Peak-Correlation lag")
    dimnames(amp_shift_mat_insamp)[[1]]<-dim_names
  } else
  {
    amp_shift_mat_insamp<-cbind(selectivity_insamp,mean_shift_insamp)
    dimnames(amp_shift_mat_insamp)[[2]]<-c("Selectivity","Mean-shift")
    dimnames(amp_shift_mat_insamp)[[1]]<-dim_names
  }
  amp_shift_mat_outsamp<-cbind(curvature_outsamp,lag_max_outsamp)
  dimnames(amp_shift_mat_outsamp)[[1]]<-dim_names
  if (mba)
  {
    dimnames(amp_shift_mat_outsamp)[[2]]<-c("Curvature","Peak-Correlation")
  } else
  {
    dimnames(amp_shift_mat_outsamp)[[2]]<-c("Curvature (out-of-sample)",
    "Peak-Correlation (out-of-sample)")
    dimnames(amp_shift_mat_outsamp)[[1]]<-dim_names
  }

  ats_mat<-cbind(Accuracy,Timeliness,Smoothness,Residual,
  Accuracy+Timeliness+Smoothness+Residual)
  dimnames(ats_mat)[[2]][dim(ats_mat)[2]]<-"Total MSE"
  dimnames(ats_mat)[[1]]<-dim_names
  return(list(xf=xf,amp_shift_mat_insamp=amp_shift_mat_insamp,
  amp_shift_mat_outsamp=amp_shift_mat_outsamp,
  ats_mat=ats_mat,amp=amp,shift=shift,b=b,xf_sym=xf_sym))
}
@



\subsection{MSE-Criterion}

The material on this topic has been presented in sections \ref{ex_lag_amp} (real-time performances, smoothing,
overfitting and finite-sample distributions of filter-coefficients, amplitude and time-shift functions) and \ref{vintages_triangle_revision}
(revisions, vintage triangle, tentacle plot). For this purpose we relied on a simpler DFA-MSE estimation routine $dfa\textunderscore ms$, proposed in
section \ref{ex_lag_amp}, which can be replicated perfectly by the new more general function $dfa\textunderscore analytic$.\\

Therefore we here limit Ourself to computing two MSE-filters which will be used as benchmarks for evaluating customized
designs: the ordinary DFA MSE-solution ($\lambda=\eta=0$ in \ref{dfatp} or \ref{idfatp}) and the best
theoretical MSE-filter, assuming knowledge of the true DGP: the latter in particular should be an irrefutable (because slightly unfair)
candidate for out-of-sample evaluation of customized designs.

\subsubsection{Empirical setting}

In order to comply with the problem structure of some practically relevant and challenging real-time applications
(real-time economic indicators, financial trading) We here rely on a `flat-spectrum' white-noise input process: something
that will not be too far away from log-returns of typical (seasonally adjusted) macro- or financial data:
\begin{eqnarray*}
x_t&=&\epsilon_t
\end{eqnarray*}
Targeting the low-frequency trend-growth in log-returns of economic data allows to track turning-points, if done properly.
For simplicity We here assume monthly data and track the synthetic output of a bi-infinite ideal lowpass with cutoff $\pi/12$
\[\Gamma(\omega_k)=\left\{\begin{array}{cc}1 &|\omega_k|<\pi/12\\0&\textrm{otherwise}\end{array}\right.\]
in real-time. The cutoff-frequency $\pi/12$ means that We attempt to eliminate components whose duractions exceeds $\frac{2\pi}{\pi/12}=24$
time-units: this is a reasonable setting for economic-monitoring since the duration of business-cycles is typically above 2
years\footnote{Omitting the double-dip in the early 80's We observe that recessions are generally separated by more than two years in
historical data.}. This is a very tough but practically relevant estimation problem!\\

In the following R-chunk We generate a realization of length $1000$ of a standardized
white noise: the first $T=120$ observations
are used for in-sample estimation. We also define the target signal.

<<echo=True>>=
# Generate series
gen_ser<-function(setseed,len,len1,cutoff_period)
{
  set.seed(setseed)

  # The longer sample is used for implementing the symmetric filter and
  # for computing peak-correlations
  x1<-rnorm(len1)
  # The shorter sample of length 120 is used for estimating the real-time filter
  x<-x1[1:len]
  plot_T<-F
  yhat<-x
  Gamma<-c(1,(1:(len/2))<len/(2*cutoff_period))
  # Compute periodogram (in-sample)
  weight_func<-per(x,plot_T)$per
  K<-length(weight_func)-1
  omega_k<-(0:K)*pi/K
  return(list(omega_k=omega_k,x1=x1,x=x,Gamma=Gamma,weight_func=weight_func))
}
#
setseed<-10
len<-120
len1<-1000
cutoff_period<-12
cutoff<-pi/cutoff_period
#
gen_obj<-gen_ser(setseed,len,len1,cutoff_period)
#
x1<-gen_obj$x1
x<-gen_obj$x
omega_k<-gen_obj$omega_k
Gamma<-gen_obj$Gamma
weight_func<-gen_obj$weight_func
K<-length(weight_func)-1
@


\subsubsection{Unconstrained MSE-Solution}



We compute a real-time MSE estimate $\hat{y}_t$ based
on the MSE-criterion \ref{dfa_ms}, applying a real-time filter of length $L=24$\footnote{Shorter filters are unable to
damp sufficiently well components of frequency close to the cutoff $\pi/12$ and longer filters are possibly subject to overfitting,
recall table \ref{perf_mat_dfa_i_o}.}. We also compute a symmetric (final) MSE-filter of length $L=61$
(for computing peak-correlations) as well as the theoretically best real-time MSE-filter of length $L=24$ (assuming knowledge
of the true DGP: white noise). The following R-chunk initializes the filter design.


<<echo=True>>=
# Generate series
L_sym<-61
L<-24
# Specify filter settings
Lag<-0
# No filter restrictions
i1<-i2<-F
# Optimize unconstrained real-time MSE-filter
eta_vec<-0
lambda_vec<-0
# True DGP: white noise spectrum
a1<-0
# mba<-T is used when replicating model-based approaches later on
mba<-F
@
We can now proceed to estimation and evaluation of the MSE-solution.


<<echo=True>>=
# Start processing
#
perf<-Performance_func(lambda_vec,eta_vec,len1,len,x1,weight_func,
Gamma,cutoff,L,L_sym,a1,mba)

xf<-perf$xf
amp_shift_mat_insamp<-perf$amp_shift_mat_insamp
amp_shift_mat_outsamp<-perf$amp_shift_mat_outsamp
ats_mat<-perf$ats_mat
amp<-perf$amp
shift<-perf$shift
@
Input and output series as well as periodograms thereof and amplitude/time-shift functions
are plotted in fig.\ref{z_dfa_ar1_output_e}.
<<echo=True>>=
colo<-rainbow(dim(xf)[2])
file = paste("z_dfa_ar1_output_e.pdf", sep = "")
pdf(file = paste(path.out,file,sep=""), paper = "special", width = 6, height = 6)
par(mfrow=c(2,2))
ts.plot(x,main="DFA (red) and theoretical MSE (cyan) vs. input (black)",col="black")
mtext("Input", side = 3, line = -1,at=len/2,col="black",ylab="series")
for (i in 1:dim(xf)[2])
{
  lines(xf[,i],col=colo[i])
  mtext(dimnames(xf)[[2]][i], side = 3, line = -i-1,at=len/2,col=colo[i])
}
plot(per(x[L:len],plot_T)$per,type="l",axes=F,col="black",ylim=c(0,max(amp)),
ylab="Periodogram",xlab="",main="Periodograms")
mtext("Input", side = 3, line = -1,at=(K-L/2)/2,col="black")
lines(per(xf[L:len,2],plot_T)$per,lty=1,col="red")
mtext("MSE-Output", side = 3, line = -2,at=(K-L/2)/2,col="red")
axis(1,at=1+0:6*(K-L/2)/6,labels=c("0","pi/6","2pi/6","3pi/6","4pi/6","5pi/6","pi"))
axis(2)
box()
plot(Gamma,type="l",axes=F,col="black",ylim=c(0,max(amp,1)),ylab="",xlab="",
main=paste("Amplitude MSE-Filter: L=",L,sep=""))
mtext("Target", side = 3, line = -1,at=K/2,col="black")
for (i in 1:dim(xf)[2])
{
  lines(amp[,i],col=colo[i])
  mtext(dimnames(amp)[[2]][i], side = 3, line = -i-1,at=K/2,col=colo[i])
}
axis(1,at=1+0:6*K/6,labels=c("0","pi/6","2pi/6","3pi/6","4pi/6","5pi/6","pi"))
axis(2)
box()
plot(rep(0,K+1),type="l",axes=F,col="black",ylim=c(0,max(na.exclude(shift[2:(K+1),]))),
ylab="",xlab="",main=paste("Shift MSE-Filter: L=",L,sep=""))
mtext("Target", side = 3, line = -1,at=K/2,col="black")
for (i in 1:dim(xf)[2])
{
  lines(shift[,i],col=colo[i])
  mtext(dimnames(shift)[[2]][i], side = 3, line = -i-1,at=K/2,col=colo[i])
}
axis(1,at=1+0:6*K/6,labels=c("0","pi/6","2pi/6","3pi/6","4pi/6","5pi/6","pi"))
axis(2)
box()
dev.off()
@
<<label=z_dfa_ar1_output_e.pdf,echo=FALSE,results=tex>>=
  file = paste("z_dfa_ar1_output_e", sep = "")
  cat("\\begin{figure}[H]")
  cat("\\begin{center}")
  cat("\\includegraphics[height=6in, width=6in]{", file, "}\n",sep = "")
  cat("\\caption{Inputs (blue) and real-time lowpass MSE output (red) (top left); periodograms (top right);
  amplitude functions (bottom left) and time-shifts (bottom right): the theoretically best filter (blue)
  assumes knowledge of the DGP (white noise)", sep = "")
  cat("\\label{z_dfa_ar1_output_e}}", sep = "")
  cat("\\end{center}")
  cat("\\end{figure}")
@
We observe that the rea-time amplitude functions (bottom left) are noticeably smaller than one in the passband. As an effect,
the periodogram of the filtered output (top right) is smaller than the periodogram of the input in the passband. The
time-shift (bottom right) is somewhere between $\Sexpr{round(min(shift[2:(K/6)]),1)}$ and $\Sexpr{round(max(shift[2:(K/6)]),1)}$
time-units in the passband. As an effect, the output (top left)
is delayed. The amplitude (bottom left) attempts to `eliminate' undesirable high-frequency components: the damping
is not excessively heavy, though marked, in the stopband. As a result We still observe tiny spectral mass in the stop-band
of the periodogram (top right) and the output
series (top left) is still slightly noisy (undesirable high-frequency ripples). These issues
can and will be addressed by systematic customization of Timeliness and
Smoothness error components, see below.\\




\subsubsection{Constrained MSE-Solutions: i1 and i2 Constraints}

The input parameters $i1$ and $i2$ in the head of the function $dfa\textunderscore analytic$ allow to impose filter constraints
in frequency zero:
\begin{itemize}
\item $i1<-T$ imposes an amplitude-match $\hat{A}(0)=\Gamma(0)$
\item $i2<-T$ imposes a vanishing time-shift $\hat{\phi}(0)=0$.
\end{itemize}
Both
constraints can be combined, too, and We now estimate filters for all four combinations.


<<echo=True>>=

# i1 restriction only (not i2)
i1<-T
i2<-F
# Compute filter and performances
perf<-Performance_func(lambda_vec,eta_vec,len1,len,x1,weight_func,Gamma,cutoff,L,L_sym,a1,mba)
xf_i1<-perf$xf
amp_i1<-perf$amp
shift_i1<-perf$shift
#
# i2 restriction only (not (i1)
i1<-F
i2<-T
perf<-Performance_func(lambda_vec,eta_vec,len1,len,x1,weight_func,Gamma,cutoff,L,L_sym,a1,mba)
xf_i2<-perf$xf
amp_i2<-perf$amp
shift_i2<-perf$shift
#
# i1 and i2 restrictions
i1<-T
i2<-T
perf<-Performance_func(lambda_vec,eta_vec,len1,len,x1,weight_func,Gamma,cutoff,L,L_sym,a1,mba)
xf_i1i2<-perf$xf
amp_i1i2<-perf$amp
shift_i1i2<-perf$shift
@

We now plot and compare amplitude and time-shift
functions\footnote{The time-shift in frequency zero is computed according to \ref{shift_zero_eq}.}, see
fig.\ref{z_dfa_ar1_output_ee}.

<<echo=True>>=
#
# Generate graphs
file = paste("z_dfa_ar1_output_ee.pdf", sep = "")
pdf(file = paste(path.out,file,sep=""), paper = "special", width = 6, height = 6)
par(mfrow=c(1,2))
plot(Gamma,type="l",axes=F,col="black",ylim=c(0,max(amp_i1i2,1)),
ylab="",xlab="",main="Amplitude")
mtext("Target", side = 3, line = -1,at=K/2,col="black")
lines(amp[,2],lty=1,col="red")
lines(amp_i1[,2],lty=1,col="orange")
lines(amp_i2[,2],lty=1,col="green")
lines(amp_i1i2[,2],lty=1,col="violet")
mtext("Amplitude i1=i2=F", side = 3, line = -2,at=K/2,col="red")
mtext("Amplitude i1=T,i2=F", side = 3, line = -3,at=K/2,col="orange")
mtext("Amplitude i1=F,i2=T", side = 3, line = -4,at=K/2,col="green")
mtext("Amplitude i1=i2=T", side = 3, line = -5,at=K/2,col="violet")
axis(1,at=1+0:6*K/6,labels=c("0","pi/6","2pi/6","3pi/6","4pi/6","5pi/6","pi"))
axis(2)
box()
plot(rep(0,K+1),type="l",axes=F,col="black",ylim=c(0,max(na.exclude(shift_i1))),
ylab="",xlab="",main="Shift")
mtext("Target", side = 3, line = -1,at=K/2,col="black")
lines(shift[,2],lty=1,col="red")
lines(shift_i1[,2],lty=1,col="orange")
lines(shift_i2[,2],lty=1,col="green")
lines(shift_i1i2[,2],lty=1,col="violet")
mtext("Shift i1=i2=F", side = 3, line = -2,at=K/2,col="red")
mtext("Shift i1=T,i2=F", side = 3, line = -3,at=K/2,col="orange")
mtext("Shift i1=F,i2=T", side = 3, line = -4,at=K/2,col="green")
mtext("Shift i1=i2=T", side = 3, line = -5,at=K/2,col="violet")
axis(1,at=1+0:6*K/6,labels=c("0","pi/6","2pi/6","3pi/6","4pi/6","5pi/6","pi"))
axis(2)
box()
dev.off()
@
<<label=z_dfa_ar1_output_ee.pdf,echo=FALSE,results=tex>>=
  file = paste("z_dfa_ar1_output_ee", sep = "")
  cat("\\begin{figure}[H]")
  cat("\\begin{center}")
  cat("\\includegraphics[height=4in, width=4in]{", file, "}\n",sep = "")
  cat("\\caption{Effect of Filter restrictions in frequency zero on
  amplitude and time-shifts. Input: white noise.", sep = "")
  cat("\\label{z_dfa_ar1_output_ee}}", sep = "")
  cat("\\end{center}")
  cat("\\end{figure}")
@
The amplitude functions of the filters with i1<-T (orange and violet)
start in $\Gamma(0)=1$, as desired, and the shifts of the two filters
with i2<-T (green and violet) start in zero, as expected. These features -- restrictions -- were less effective in the context of SA
because unrestricted filters are close to identities in the passband and therefore both restrictions are closed to be satisfied anyway.
In the context of real-time trend extraction, however, real-time filters can substantially deviate from the target in frequency zero.
Depending on the particular application, frequency zero may (or may not) deserve special care and attention by imposing i1 and/or i2: as an example,
tracking the level of the trend of a non-stationary series might benefit from these
restrictions\footnote{Formally, $i1<-T$ is necessary to ensure a finite mean-square filter error when the DGP of the input signal is I(1) with
a single unit-root in frequency zero. Both restrictions $i1<-i2<-T$ are necessary when the DGP is I(2) with a double root in frequency zero.
However, We are not interested in `unit-roots' or DGP's and therefore We impose these restrictions, in any possible combination,
when felt useful or practically relevant.}.
In most applications We find the i2-restriction (vanishing time-shift)
potentially more interesting because it complies with a faster detection of turning-points. All the following
examples are based on unconstrained filters $i_1=i_2=F$.



\subsubsection{Revision Sequence: Vintage Triangle}

We did not found any relevant practical application of revision sequences in the context of trend-extraction so far and therefore We skip this topic.
Interested readers are referred to sections \ref{vintages_triangle_revision} and \ref{tentacle_plot} for corresponding empirical material.





\subsection{Customization: Enhancing \emph{Either} Speed or Noise-suppression (Timeliness-Smoothness Dilemma)}

Now that We have Our benchmark MSE-filter at disposal, We can proceed to customization of the real-time trend filter by selecting
$\lambda>0$ or $\eta>0$ in the general criterion
\ref{idfatp}. We first illustrate the effect of  $\eta$, fixing $\lambda=0$: this is the `Smoothness only' experiment.
The `Timeliness only' experiment is then obtained by fixing $\eta=0$ and increasing $\lambda$. By addressing a single
dimension only We allow the MSE-mass to `circulate freely' among the other two error components in the ATS-trilemma: therefore,
We expect to observe classical dilemma at work.


\subsubsection{Emphasizing Smoothness Only} \label{S_only}

We  fix $\lambda=0$ and compute real-time filters for $\eta$ from 0 to 1.5 by incremental steps of 0.25.

<<echo=True>>=

eta_vec<-(0:6)/4
lambda_vec<-rep(0,length(eta_vec))
i1<-F
i2<-F
# Compute in/out-of-sample performances

perf<-Performance_func(lambda_vec,eta_vec,len1,len,x1,weight_func,Gamma,cutoff,L,L_sym,a1,mba)

xf<-perf$xf
amp_shift_mat_insamp<-perf$amp_shift_mat_insamp
amp_shift_mat_outsamp<-perf$amp_shift_mat_outsamp
ats_mat<-perf$ats_mat
amp<-perf$amp
shift<-perf$shift
@
Filter outputs, amplitude and
time-shift functions are plotted in fig.\ref{z_dfa_cust_amp}.
<<echo=True>>=
#
# Plots
colo<-rainbow(2*dim(amp)[2])
file = paste("z_dfa_cust_amp.pdf", sep = "")
pdf(file = paste(path.out,file,sep=""), paper = "special", width = 6, height = 6)
par(mfrow=c(1,2))
plot(Gamma,type="l",axes=F,col="black",ylim=c(0,1),ylab="",xlab="",main="Amplitude")
mtext("Target", side = 3, line = -1,at=K/2,col="black")
for (i in 1:dim(amp)[2])
{
  lines(amp[,i],lty=1,col=colo[i])
  mtext(dimnames(amp)[[2]][i], side = 3, line = -1-i,at=K/2,col=colo[i])
}
axis(1,at=1+0:6*K/6,labels=c("0","pi/6","2pi/6","3pi/6","4pi/6","5pi/6","pi"))
axis(2)
box()
plot(rep(0,K+1),type="l",axes=F,col="black",ylim=c(0,max(na.exclude(shift))),
ylab="",xlab="",main="Shift")
mtext("Target", side = 3, line = -1,at=K/2,col="black")
for (i in 1:dim(amp)[2])
{
  lines(shift[,i],lty=1,col=colo[i])
  mtext(dimnames(shift)[[2]][i], side = 3, line = -1-i,at=K/2,col=colo[i])
}
axis(1,at=1+0:6*K/6,labels=c("0","pi/6","2pi/6","3pi/6","4pi/6","5pi/6","pi"))
axis(2)
box()
dev.off()
@
<<label=z_dfa_cust_amp.pdf,echo=FALSE,results=tex>>=
  file = paste("z_dfa_cust_amp", sep = "")
  cat("\\begin{figure}[H]")
  cat("\\begin{center}")
  cat("\\includegraphics[height=4in, width=4in]{", file, "}\n",sep = "")
  cat("\\caption{Customized filters: the effect of eta (strength of noise supression/smoothness) on amplitude and time-shifts (delays)", sep = "")
  cat("\\label{z_dfa_cust_amp}}", sep = "")
  cat("\\end{center}")
  cat("\\end{figure}")
@
The amplitude functions decrease (approach zero) monotonically in the stopband with increasing $\eta$, as desired. An undesirable
side-effect is that the amplitudes are also slightly pulled-down in the passband but this effect could be compensated either by rescaling
the output series (for example by $1/\hat{A}(0)$) or by imposing i1<-T: in applications, We generally prefer the former scaling
because it does not affect the effective filter properties. Note that the performance measures proposed in section \ref{tdpm}
were explicitly immunized against `amplitude-shrinkage': either by computing relative numbers or by disentangling the time-shift
from the amplitude.



\subsubsection{Dilemma: Timeliness vs. Smoothness, Selectivity vs. Mean-Shift and Curvature vs. Peak-Correlation}


Table \ref{ats_mat_1} reports the ATS-components, based on \ref{mse_dec_ats}.
<<label=ats_mat_1,echo=FALSE,results=tex>>=
  library(Hmisc)
  require(xtable)
  #latex(cor_vec, dec = 1, , caption = "Example of using latex to create table",
  #center = "centering", file = "", floating = FALSE)
  xtable(ats_mat, dec = 1,digits=rep(3,dim(ats_mat)[2]+1),
  paste("ATS-components: emphasizing Smoothness only",sep=""),
  label=paste("ats_mat_1",sep=""),
  center = "centering", file = "", floating = FALSE)
@
The Residual vanishes because the target filter vanishes in the stop-band.
As expected, the Smoothness error-component decreases and the Timeliness-component increases with increasing $\eta$. Also,
Accuracy increases monotonically because larger $\eta$ shrink the amplitude function in the passband, see fig.\ref{z_dfa_cust_amp}. Finally,
the total MSE (the sum of the ATS-components) increases because stronger customization takes us `farther away' from the MSE-filter.
Note that
`Total MSE' of the DFA filter is smaller than for the best possible MSE-design because We optimized Our criterion accordingly: We thus
observe overfitting. Also, the `total MSE' of the customized designs could be significantly reduced by simple scaling in order to
compensate for the observed amplitude-shrinkage in the passband (but since We are not interested in MSE-performances of these designs We left
this an exercise). \\

Table \ref{amp_shift_mat_insamp_1} provides additional \emph{in-sample} evidence based on Our set of performance-measures: larger $\eta$ lead to better
Selectivity at costs of larger Mean-Shift or, alternatively, smaller
curvature at costs of increasing peak-correlation lag: three different dilemma anchored in the same fundamental tradeoff. As
expected, the Mean-Shift is closely linked to Peak-Correlation.
<<label=amp_shift_mat_insamp_1,echo=FALSE,results=tex>>=
  library(Hmisc)
  require(xtable)
  #latex(cor_vec, dec = 1, , caption = "Example of using latex to create table",
  #center = "centering", file = "", floating = FALSE)
  xtable(amp_shift_mat_insamp, dec = 1,digits=rep(3,dim(amp_shift_mat_insamp)[2]+1),
  paste("In-sample performance measures: Selectivity vs. Mean-Shift and Curvature vs. Peak-Correlation: emphasizing Smoothness only",sep=""),
  label=paste("amp_shift_mat_insamp_1",sep=""),
  center = "centering", file = "", floating = FALSE)
@
Out-of-sample performances in table \ref{amp_shift_mat_outsamp_1} confirm consistently the previous findings.
<<label=amp_shift_mat_outsamp_1,echo=FALSE,results=tex>>=
  library(Hmisc)
  require(xtable)
  #latex(cor_vec, dec = 1, , caption = "Example of using latex to create table",
  #center = "centering", file = "", floating = FALSE)
  xtable(amp_shift_mat_outsamp, dec = 1,digits=rep(3,dim(amp_shift_mat_outsamp)[2]+1),
  paste("Out-of-sample performance measures: Selectivity vs. Mean-Shift and Curvature vs. Peak-Correlation: emphasizing Smoothness only",sep=""),
  label=paste("amp_shift_mat_outsamp_1",sep=""),
  center = "centering", file = "", floating = FALSE)
@
We now briefly compare the filter outputs of MSE ($\eta=0$) and strongly customized ($\eta=1.5$) filters, see fig.\ref{z_dfa_cust_amp_out_iut}.
<<echo=True>>=

#-------------------
# Compare MSE and heavily customized filter
xf0<-xf[L+1:len,1]
xf1<-xf[L+1:len,dim(amp)[2]]
file = paste("z_dfa_cust_amp_out_iut.pdf", sep = "")
pdf(file = paste(path.out,file,sep=""), paper = "special", width = 6, height = 6)
ts.plot(as.ts(xf0),type="l",axes=F,col=colo[1],ylim=c(min(na.exclude(xf0)),
max(na.exclude(xf0))),
ylab="",xlab="",main=paste("Filter outputs: MSE (eta=0) vs. customized (eta=",
eta_vec[length(eta_vec)],")",sep=""))
mtext("MSE (eta=0)", side = 3, line = -1,at=len/2,col=colo[1])
lines(as.ts(xf1),col=colo[dim(amp)[2]])
mtext(paste("Customized (eta=",eta_vec[length(eta_vec)],")",sep=""),
side = 3, line = -2,at=len/2,col=colo[dim(amp)[2]])
dev.off()
@
<<label=z_dfa_cust_amp_out_iut.pdf,echo=FALSE,results=tex>>=
  file = paste("z_dfa_cust_amp_out_iut", sep = "")
  cat("\\begin{figure}[H]")
  cat("\\begin{center}")
  cat("\\includegraphics[height=4in, width=4in]{", file, "}\n",sep = "")
  cat("\\caption{Filter outputs: MSE vs. customized", sep = "")
  cat("\\label{z_dfa_cust_amp_out_iut}}", sep = "")
  cat("\\end{center}")
  cat("\\end{figure}")
@
As expected, the output of the customized filter is substantially smoother but turning-points are delayed
in comparison to the
MSE-filter (table \ref{amp_shift_mat_insamp_1} suggests a time differential of approximately
$\Sexpr{round(amp_shift_mat_insamp[dim(amp_shift_mat_insamp)[1],2]-amp_shift_mat_insamp[1,2])}$ time-units)
which confirms, once again, the
Timeliness-Smoothness dilemma. We notice the different scales as a consequence of the amplitude-shrinkage (which explains part
of the larger `total MSE' of customized filters). \\




\subsubsection{Emphasizing Timeliness Only} \label{T_only}

We here fix $\eta=0$ and compute real-time filters for $\lambda=0,1,2,4,8,16$. Since $\eta$ is fixed We expect to observe the classical
dilemma between Smoothness and Timeliness (Selectivity and Mean-Shift, Curvature and Peak-Correlation):
still no use of the full potential beared in the trilemma!

<<echo=True>>=

lambda_vec<-c(0,2^(0:4))
eta_vec<-rep(0,length(lambda_vec))
i1<-F
i2<-F
# Compute in/out-of-sample performances

perf<-Performance_func(lambda_vec,eta_vec,len1,len,x1,weight_func,Gamma,cutoff,L,L_sym,a1,mba)

xf<-perf$xf
amp_shift_mat_insamp<-perf$amp_shift_mat_insamp
amp_shift_mat_outsamp<-perf$amp_shift_mat_outsamp
ats_mat<-perf$ats_mat
amp<-perf$amp
shift<-perf$shift
@
Filter outputs, amplitude and
time-shift functions are plotted in fig.\ref{z_dfa_cust_shift}.

<<echo=True>>=
#
# Plots
colo<-rainbow(2*dim(amp)[2])
file = paste("z_dfa_cust_shift.pdf", sep = "")
pdf(file = paste(path.out,file,sep=""), paper = "special", width = 6, height = 6)
par(mfrow=c(1,2))
plot(Gamma,type="l",axes=F,col="black",ylim=c(0,1),ylab="",xlab="",main="Amplitude")
mtext("Target", side = 3, line = -1,at=K/2,col="black")
for (i in 1:dim(amp)[2])
{
  lines(amp[,i],lty=1,col=colo[i])
  mtext(dimnames(amp)[[2]][i], side = 3, line = -1-i,
  at=K/2,col=colo[i])
}
axis(1,at=1+0:6*K/6,labels=c("0","pi/6","2pi/6","3pi/6","4pi/6","5pi/6","pi"))
axis(2)
box()
plot(rep(0,K+1),type="l",axes=F,col="black",ylim=c(min(shift),max(na.exclude(shift))),
ylab="",xlab="",main="Shift")
mtext("Target", side = 3, line = -1,at=K/2,col="black")
for (i in 1:dim(amp)[2])
{
  lines(shift[,i],lty=1,col=colo[i])
  mtext(dimnames(shift)[[2]][i], side = 3,
  line = -1-i,at=K/2,col=colo[i])
}
axis(1,at=1+0:6*K/6,labels=c("0","pi/6","2pi/6","3pi/6","4pi/6","5pi/6","pi"))
axis(2)
box()
dev.off()
@
<<label=z_dfa_cust_shift.pdf,echo=FALSE,results=tex>>=
  file = paste("z_dfa_cust_shift", sep = "")
  cat("\\begin{figure}[H]")
  cat("\\begin{center}")
  cat("\\includegraphics[height=4in, width=4in]{", file, "}\n",sep = "")
  cat("\\caption{Customized filters: emphasize timeliness", sep = "")
  cat("\\label{z_dfa_cust_shift}}", sep = "")
  cat("\\end{center}")
  cat("\\end{figure}")
@
As $\lambda$ increases, the time-shift decreases, as desired. The effect on the amplitude functions is interesting: whereas
the stopband remains more or less unaffected, the amplitudes are shrunken in the passband\footnote{This is because $\lambda$ in \ref{idfatp} magnifies the imaginary part of the real-time filter:
by shrinking the amplitude, the imaginary part is automatically shrunken, too.}. As a result, Selectivity must worsen (with increasing $\lambda$).



\subsubsection{Dilemma: Selectivity vs. Mean-Shift and Timeliness vs. Smoothness and Curvature vs. Peak-Correlation}



Table \ref{ats_mat_2} reports the ATS-components.
<<label=ats_mat_2,echo=FALSE,results=tex>>=
  library(Hmisc)
  require(xtable)
  #latex(cor_vec, dec = 1, , caption = "Example of using latex to create table",
  #center = "centering", file = "", floating = FALSE)
  xtable(ats_mat, dec = 1,digits=rep(3,dim(ats_mat)[2]+1),
  paste("ATS-components: emphasizing Timeliness only",sep=""),
  label=paste("ats_mat_2",sep=""),
  center = "centering", file = "", floating = FALSE)
@
As expected, the Timeliness error-component decreases and the Smoothness-component increases with increasing $\lambda$.  Also,
Accuracy increases because the amplitude functions are shrunken in the passband, see fig.\ref{z_dfa_cust_shift}. Finally,
total MSE (sum of ATS-components) increases because stronger customization takes us `farther away' from the MSE-filter
(part of this undesirable MSE-increase could be avoided by simple scaling in order to compensate for the shrinkage-phenomenon but
We do not address this topic because customized filters are not explicitly designed for MSE-performances).\\

Table \ref{amp_shift_mat_insamp_2}
confirms these in-sample findings: larger $\lambda$ lead to worse Selectivity at the benefit of smaller Mean-Shift or
worse Curvature
at the benefit of smaller Peak-Correlation lag. Once again, We sollicited dichotomic tradeoffs.
<<label=amp_shift_mat_insamp_2,echo=FALSE,results=tex>>=
  library(Hmisc)
  require(xtable)
  #latex(cor_vec, dec = 1, , caption = "Example of using latex to create table",
  #center = "centering", file = "", floating = FALSE)
  xtable(amp_shift_mat_insamp, dec = 1,digits=rep(3,dim(amp_shift_mat_insamp)[2]+1),
  paste("In-sample performances measures: Selectivity vs. Mean-Shift and Curvature vs. Peak-Correlation: emphasizing Timeliness only",sep=""),
  label=paste("amp_shift_mat_insamp_2",sep=""),
  center = "centering", file = "", floating = FALSE)
@
Out-of-sample performances in table \ref{amp_shift_mat_outsamp_2} confirm consistently in-sample findings.

<<label=amp_shift_mat_outsamp_2,echo=FALSE,results=tex>>=
  library(Hmisc)
  require(xtable)
  #latex(cor_vec, dec = 1, , caption = "Example of using latex to create table",
  #center = "centering", file = "", floating = FALSE)
  xtable(amp_shift_mat_outsamp, dec = 1,digits=rep(3,dim(amp_shift_mat_outsamp)[2]+1),
  paste("Out-of-sample performances measures: Selectivity vs. Mean-Shift and Curvature vs. Peak-Correlation: emphasizing Timeliness only",sep=""),
  label=paste("amp_shift_mat_outsamp_2",sep=""),
  center = "centering", file = "", floating = FALSE)
@





We now briefly compare the filter outputs of MSE ($\eta=\lambda=0$) and heavily customized ($\lambda=16,\eta=0$) filters,
see fig.\ref{z_dfa_cust_amp_out_puti}.
<<echo=True>>=

#-------------------
# Compare MSE and heavily customized filter
xf0<-xf[L+1:len,1]
xf1<-xf[L+1:len,dim(amp)[2]]
file = paste("z_dfa_cust_amp_out_puti.pdf", sep = "")
pdf(file = paste(path.out,file,sep=""), paper = "special", width = 6, height = 6)
ts.plot(as.ts(xf0),type="l",axes=F,col=colo[1],ylim=c(min(na.exclude(xf1)),max(na.exclude(xf1))),ylab="",xlab="",
main="Filter outputs: MSE (lambda=0) vs. customized (lambda=16)")
mtext("MSE (lambda=0)", side = 3, line = -1,at=len/2,col=colo[1])
lines(as.ts(xf1),col=colo[dim(amp)[2]])
mtext("Customized (lambda=16)", side = 3, line = -2,at=len/2,col=colo[dim(amp)[2]])
dev.off()
@
<<label=z_dfa_cust_amp_out_puti.pdf,echo=FALSE,results=tex>>=
  file = paste("z_dfa_cust_amp_out_puti", sep = "")
  cat("\\begin{figure}[H]")
  cat("\\begin{center}")
  cat("\\includegraphics[height=4in, width=4in]{", file, "}\n",sep = "")
  cat("\\caption{Filter outputs: MSE vs. customized", sep = "")
  cat("\\label{z_dfa_cust_amp_out_puti}}", sep = "")
  cat("\\end{center}")
  cat("\\end{figure}")
@
As expected, the customized filter (green) is faster but noisier than the MSE-filter (red). We now proceed to
a more sophisticated estimation problem, namely tackling the time-shift in the passband \emph{and} the amplitude function in the stop-band
\emph{simultaneously}: we'll make use of the full potential of the ATS-Trilemma.




\subsection{Customization: Enhancing Speed \emph{And} Smoothness in the ATS-Trilemma}



\subsubsection{Experimental Design}

We here select $\eta>0$ \emph{and} $\lambda>0$, compute corresponding real-time filters of length $L=24$ and compare performances
with the previously obtained MSE-design $\eta=\lambda=0$ ($L=24$). Specifically We analyze the following three customized filters
\begin{itemize}
\item $\lambda=32,\eta=1$: a filter which emphasizes Timeliness as well as Smoothness more or less heavily.
\item $\lambda=100$, $\eta=0.7$: a filter which  prioritizes strongly Timeliness, though Smoothness is emphasized too.
\item $\lambda=8,\eta=1.5$: a filter which prioritizes Smoothness over Timeliness.
\end{itemize}
We then compute in-sample as well as `true' out-of-sample performances as referenced against the DFA-MSE and the
best theoretical MSE-filter (assuming knowledge of the DGP).

\subsubsection{In-Sample Analysis: Frequency-Domain Statistics}


We proceed to estimation:
<<echo=True>>=
# Filter length
L<-24
# No filter restrictions
i1<-F
i2<-F
# Designs: MSE and two customized filters
lambda_vec<-c(0,32,100,8)
eta_vec<-c(0,1,0.7,1.5)
mba<-F
# Compute in/out-of-sample performances

perf<-Performance_func(lambda_vec,eta_vec,len1,len,x1,weight_func,Gamma,cutoff,L,L_sym,a1,mba)

xf<-perf$xf
amp_shift_mat_insamp<-perf$amp_shift_mat_insamp
amp_shift_mat_outsamp<-perf$amp_shift_mat_outsamp
ats_mat<-perf$ats_mat
amp<-perf$amp
shift<-perf$shift
xf_sym<-perf$xf_sym
@
Amplitude and time-shift functions of the real-time filters are plotted in fig.\ref{z_dfa_cust_ats}.
<<echo=True>>=
#
# Plots
colo<-rainbow(dim(amp)[2])
file = paste("z_dfa_cust_ats.pdf", sep = "")
pdf(file = paste(path.out,file,sep=""), paper = "special", width = 6, height = 6)
par(mfrow=c(1,2))
plot(Gamma,type="l",axes=F,col="black",ylim=c(0,1),ylab="",xlab="",main="Amplitude")
mtext("Target", side = 3, line = -1,at=K/2,col="black")
for (i in 1:dim(amp)[2])
{
  lines(amp[,i],lty=1,col=colo[i])
  mtext(dimnames(amp)[[2]][i], side = 3,
  line = -1-i,at=K/2,col=colo[i])
}
axis(1,at=1+0:6*K/6,labels=c("0","pi/6","2pi/6","3pi/6","4pi/6","5pi/6","pi"))
axis(2)
box()
plot(rep(0,K+1),type="l",axes=F,col="black",ylim=c(min(shift),max(na.exclude(shift))),
ylab="",xlab="",main="Shift")
mtext("Target", side = 3, line = -1,at=K/2,col="black")
for (i in 1:dim(amp)[2])
{
  lines(shift[,i],lty=1,col=colo[i])
  mtext(dimnames(shift)[[2]][i], side = 3,
  line = -1-i,at=K/2,col=colo[i])
}
axis(1,at=1+0:6*K/6,labels=c("0","pi/6","2pi/6","3pi/6","4pi/6","5pi/6","pi"))
axis(2)
box()
dev.off()
@
<<label=z_dfa_cust_ats.pdf,echo=FALSE,results=tex>>=
  file = paste("z_dfa_cust_ats", sep = "")
  cat("\\begin{figure}[H]")
  cat("\\begin{center}")
  cat("\\includegraphics[height=4in, width=6in]{", file, "}\n",sep = "")
  cat("\\caption{MSE vs. customized filters: emphasize timeliness and smoothness simultaneously", sep = "")
  cat("\\label{z_dfa_cust_ats}}", sep = "")
  cat("\\end{center}")
  cat("\\end{figure}")
@
The relatively strong Timeliness-Smoothness weights $\lambda,\eta$ assigned to all three customized filters shrinks their
amplitudes towards zero in the passband which is not really an issue since We could easily re-scale all outputs by the inverse
amplitude functions $1/\hat{A}^{i}(0)$ in frequency zero. As explained previously, Our performance measures were specifically
immunized against this customization effect.


\subsubsection{Trilemma: Improving Selectivity and Mean-Shift; Timeliness and Smoothness; Curvature and Peak-Correlation}



Table \ref{ats_mat_3} reports the ATS-components.
<<label=ats_mat_2,echo=FALSE,results=tex>>=
  library(Hmisc)
  require(xtable)
  #latex(cor_vec, dec = 1, , caption = "Example of using latex to create table",
  #center = "centering", file = "", floating = FALSE)
  xtable(ats_mat, dec = 1,digits=rep(3,dim(ats_mat)[2]+1),
  paste("ATS-components: emphasizing Timeliness only",sep=""),
  label=paste("ats_mat_3",sep=""),
  center = "centering", file = "", floating = FALSE)
@
Unlike the previous dilemma in sections \ref{S_only} and \ref{T_only} We now observe that the customized filters can outperform the benchmark MSE-design
in terms of Timeliness \emph{and} Smoothness but at costs of Accuracy. However... We know that the amplitude shrinkage in the passband
can distord Timeliness and Smoothness since both depend upon the amplitude function. Therefore We look
at Our alternative performance measures which
are immunized against amplitude-shrinkage, see tables \ref{amp_shift_mat_insamp_3} (in-sample) and \ref{amp_shift_mat_outsamp_3} (out-of-sample).
<<label=amp_shift_mat_insamp_3,echo=FALSE,results=tex>>=
  library(Hmisc)
  require(xtable)
  #latex(cor_vec, dec = 1, , caption = "Example of using latex to create table",
  #center = "centering", file = "", floating = FALSE)
  xtable(amp_shift_mat_insamp, dec = 1,digits=rep(3,dim(amp_shift_mat_insamp)[2]+1),
  paste("In-sample performances measures: Selectivity vs. Mean-Shift and Curvature vs. Peak-Correlation: emphasizing
  Smoothness and Timeliness",sep=""),
  label=paste("amp_shift_mat_insamp_3",sep=""),
  center = "centering", file = "", floating = FALSE)
@
We can appreciate the double-scoring in terms of Selectivity and Mean-Shift or, alternatively, in terms of
Curvature\footnote{Recall that We use a relative measure
which is immune to scaling effects such as entailed by amplitude shrinkage.} and Peak-Correlation, of the `balanced' customized
filter $\lambda=\Sexpr{lambda_vec[2]}, \eta=\Sexpr{eta_vec[2]}$ over the DFA MSE-filter ($\eta=\lambda=0$). The best theoretical
MSE-design is beaten too. The two `extreme' designs (last two rows)
perform as desired: they strongly outperform the theoretically best MSE-filter in their stronger attributes and the loss incurred
in their weaker dimension is mild.
<<label=amp_shift_mat_outsamp_3,echo=FALSE,results=tex>>=
  library(Hmisc)
  require(xtable)
  #latex(cor_vec, dec = 1, , caption = "Example of using latex to create table",
  #center = "centering", file = "", floating = FALSE)
  xtable(amp_shift_mat_outsamp, dec = 1,digits=rep(3,dim(amp_shift_mat_outsamp)[2]+1),
  paste("Out-of-sample performances measures: Selectivity vs. Mean-Shift and Curvature vs. Peak-Correlation:
  emphasizing Smoothness and Timeliness",sep=""),
  label=paste("amp_shift_mat_outsamp_3",sep=""),
  center = "centering", file = "", floating = FALSE)
@
Out-of-sample performances confirm consistently Our in-sample findings. \\





We now briefly compare the four different \emph{out-of-sample} real-time filter outputs and reference them against the symmetric filter, see
fig.\ref{z_dfa_cust_amp_out_ats}.
<<echo=True>>=
colo<-rainbow(dim(amp)[2]+1)
file = paste("z_dfa_cust_amp_out_ats.pdf", sep = "")
pdf(file = paste(path.out,file,sep=""), paper = "special", width = 6, height = 6)
# Select out-of-sample period
anf<-4*len#len+1
enf<-5*len
sel<-1:dim(amp)[2]
mplot<-scale(cbind(xf_sym,xf[,sel])[anf:enf,])
plot(as.ts(mplot[,1]),type="l",axes=F,col="black",ylim=c(min(na.exclude(mplot)),
max(na.exclude(mplot))),ylab="",xlab="",
main="Out-of-sample final symmetric vs. real-time MSE and customized",lwd=2)
mtext("Final symmetric", side = 3, line = -1,at=(enf-anf)/2,col="black")
for (i in 1:length(sel))
{
  lines(as.ts(mplot[,i+1]),col=colo[sel[i]],lwd=2)
  mtext(dimnames(amp)[[2]][i], side = 3, line = -1-i,at=(enf-anf)/2,col=colo[sel[i]])
}
axis(1,at=c(1,rep(0,6))+as.integer((0:6)*(enf-anf)/6),
labels=as.integer(anf+(0:6)*(enf-anf)/6))
axis(2)
box()
dev.off()
@
<<label=z_dfa_cust_amp_out_ats.pdf,echo=FALSE,results=tex>>=
  file = paste("z_dfa_cust_amp_out_ats", sep = "")
  cat("\\begin{figure}[H]")
  cat("\\begin{center}")
  cat("\\includegraphics[height=6in, width=6in]{", file, "}\n",sep = "")
  cat("\\caption{Standardized filter outputs out-of-sample: final symmetric vs. real-time MSE and customized designs", sep = "")
  cat("\\label{z_dfa_cust_amp_out_ats}}", sep = "")
  cat("\\end{center}")
  cat("\\end{figure}")
@
All series are standardized in order to match scales (overcome the amplitude shrinkage). We can recognize the difficulty of the estimation problem by the fact that the output of the symmetric filter of length $L=61$ (black line) is
still subject to noisy random high-frequency noise which leakes the filter despite its substantial length of 5 years.
We observe that the $\lambda=100,\eta=0.7$-filter (cyan) systematically lies on the
left\footnote{Not all of the noisy local peaks and troughs can be anticipated by this filter, of course,
because they belong
to the stopband (We control the time-shift in the passband) and because the input series is white
noise; as claimed: this is a tough
estimation problem.} of the other real-time filters (it is faster) but
the residual noise is important. \\
The filter with $\lambda=8,\eta=1.5$ (violet) is much smoother than the MSE-filter (red) without being systematically delayed thus
confirming reported performance numbers.
The filter $\lambda=32,\eta=1$ lies slightly on the left of the MSE filter (smaller Peak-Correlation lag)
and it is slightly `un-noisier' (smaller Curvature), as desired. \\


\subsubsection{In- and Out-of-Sample Empirical Distributions of Curvature and Peak-Correlation}

We here replicate the above experiment 200-times and report box-plots of in- and out-of-sample distributions
of Curvature and Peak-Correlation measures, see figs.\ref{z_box_plot_in} and \ref{z_box_plot_out}.

<<echo=True>>=

# Filter length
L<-24
# No filter restrictions
i1<-F
i2<-F
# Designs: MSE and two customized filters
lambda_vec<-c(0,32,100,8)
eta_vec<-c(0,1,0.7,1.5)
# Compute in/out-of-sample performances
setseed<-10
len<-120
len1<-1000
cutoff_period<-12
cutoff<-pi/cutoff_period
anzsim<-200
amp_shift_mat_insamp_array<-array(dim=c(dim(amp_shift_mat_insamp),anzsim))
amp_shift_mat_outsamp_array<-array(dim=c(dim(amp_shift_mat_outsamp),anzsim))
ats_mat_array<-array(dim=c(dim(ats_mat),anzsim))
amp_array<-array(dim=c(dim(amp),anzsim))
shift_array<-array(dim=c(dim(shift),anzsim))
dimnames(amp_shift_mat_insamp_array)<-list(dimnames(amp_shift_mat_insamp)[[1]],
dimnames(amp_shift_mat_insamp)[[2]],1:anzsim)
dimnames(amp_shift_mat_outsamp_array)<-list(dimnames(amp_shift_mat_outsamp)[[1]],
dimnames(amp_shift_mat_outsamp)[[2]],1:anzsim)
dimnames(ats_mat_array)<-list(dimnames(ats_mat)[[1]],
dimnames(ats_mat)[[2]],1:anzsim)
for (i in 1:anzsim)      #i<-1
{
  setseed<-setseed+1
# generate series and in-sample periodogram
  gen_obj<-gen_ser(setseed,len,len1,cutoff_period)
#
  x1<-gen_obj$x1
  x<-gen_obj$x
  omega_k<-gen_obj$omega_k
  Gamma<-gen_obj$Gamma
  weight_func<-gen_obj$weight_func
#
# Compute real-time filters
  perf<-Performance_func(lambda_vec,eta_vec,len1,len,x1,weight_func,
  Gamma,cutoff,L,L_sym,a1,mba)

  xf<-perf$xf
  amp_shift_mat_insamp_array[,,i]<-perf$amp_shift_mat_insamp
  amp_shift_mat_outsamp_array[,,i]<-perf$amp_shift_mat_outsamp
  ats_mat_array[,,i]<-perf$ats_mat
  amp_array[,,i]<-perf$amp
  shift_array[,,i]<-perf$shift
}
  
@

<<echo=True>>=
colo<-rainbow(dim(amp)[2]+1)[1:dim(amp)[2]]
file = paste("z_box_plot_in.pdf", sep = "")
pdf(file = paste(path.out,file,sep=""), paper = "special", width = 6, height = 6)
par(mfrow=c(2,1))
boxplot(list(amp_shift_mat_insamp_array[1,3,],amp_shift_mat_insamp_array[2,3,],
amp_shift_mat_insamp_array[3,3,],amp_shift_mat_insamp_array[4,3,],
amp_shift_mat_insamp_array[5,3,]),outline=F,
names=c("Best MSE",paste("DFA(",lambda_vec,",",eta_vec,")",sep="")),
main="Curvature in-sample",cex.axis=0.8,col=colo)
boxplot(list(amp_shift_mat_insamp_array[1,4,],amp_shift_mat_insamp_array[2,4,],
amp_shift_mat_insamp_array[3,4,],amp_shift_mat_insamp_array[4,4,],
amp_shift_mat_insamp_array[5,4,]),outline=F,
names=c("Best MSE",paste("DFA(",lambda_vec,",",eta_vec,")",sep="")),
main="Peak-correlation in-sample",cex.axis=0.8,col=colo)
dev.off()
@
<<label=z_box_plot_in.pdf,echo=FALSE,results=tex>>=
  file = paste("z_box_plot_in", sep = "")
  cat("\\begin{figure}[H]")
  cat("\\begin{center}")
  cat("\\includegraphics[height=6in, width=6in]{", file, "}\n",sep = "")
  cat("\\caption{Box-plots of in-sample empirical distributions of Curvature (top) and Peak-Correlation (bottom)", sep = "")
  cat("\\label{z_box_plot_in}}", sep = "")
  cat("\\end{center}")
  cat("\\end{figure}")
@

<<echo=True>>=
file = paste("z_box_plot_out.pdf", sep = "")
pdf(file = paste(path.out,file,sep=""), paper = "special", width = 6, height = 6)
par(mfrow=c(2,1))
boxplot(list(amp_shift_mat_outsamp_array[1,1,],amp_shift_mat_outsamp_array[2,1,],
amp_shift_mat_outsamp_array[3,1,],amp_shift_mat_outsamp_array[4,1,],
amp_shift_mat_outsamp_array[5,1,]),outline=F,names=c("Best MSE",paste("DFA(",
lambda_vec,",",eta_vec,")",sep="")),#dimnames(amp_shift_mat_outsamp_array)[[1]],
main="Curvature out-of-sample",cex.axis=0.8,col=colo)
boxplot(list(amp_shift_mat_outsamp_array[1,2,],amp_shift_mat_outsamp_array[2,2,],
amp_shift_mat_outsamp_array[3,2,],amp_shift_mat_outsamp_array[4,2,],
amp_shift_mat_outsamp_array[5,2,]),outline=T,names=c("Best MSE",paste("DFA(",
lambda_vec,",",eta_vec,")",sep="")),#dimnames(amp_shift_mat_outsamp_array)[[1]],
main="Peak-correlation out-of-sample",cex.axis=0.8,col=colo)
dev.off()
@
<<label=z_box_plot_out.pdf,echo=FALSE,results=tex>>=
  file = paste("z_box_plot_out", sep = "")
  cat("\\begin{figure}[H]")
  cat("\\begin{center}")
  cat("\\includegraphics[height=6in, width=6in]{", file, "}\n",sep = "")
  cat("\\caption{Box-plots of out-of-sample empirical distributions of Curvature (top) and Peak-Correlation (bottom)", sep = "")
  cat("\\label{z_box_plot_out}}", sep = "")
  cat("\\end{center}")
  cat("\\end{figure}")
@
Since Curvature and Peak-Correlation are antagonistic we expect that designs which perform well
in one-dimension loose in the other attribute. Therefore it would be interesting to compute some
`aggregate' measure. Fig.\ref{z_box_plot_agg} shows the empirical out-of-sample distributions of two
simple aggregates: the product (top) and the weighted sum $(10*Curvature)+(Peak-Correlation)$: the scaling
of the Curvature-term is derived from visual inspection of the plots in fig.\ref{z_box_plot_out} where
we can see that the scales of both measures differ roughly by a factor of ten.

<<echo=True>>=
file = paste("z_box_plot_agg.pdf", sep = "")
pdf(file = paste(path.out,file,sep=""), paper = "special", width = 6, height = 6)
par(mfrow=c(2,1))
boxplot(list(amp_shift_mat_outsamp_array[1,1,]*amp_shift_mat_outsamp_array[1,2,],
amp_shift_mat_outsamp_array[2,1,]*amp_shift_mat_outsamp_array[2,2,],
amp_shift_mat_outsamp_array[3,1,]*amp_shift_mat_outsamp_array[3,2,],
amp_shift_mat_outsamp_array[4,1,]*amp_shift_mat_outsamp_array[4,2,],
amp_shift_mat_outsamp_array[5,1,]*amp_shift_mat_outsamp_array[5,2,]),
outline=F,names=c("Best MSE",paste("DFA(",
lambda_vec,",",eta_vec,")",sep="")),#dimnames(amp_shift_mat_outsamp_array)[[1]],
main="Aggregate performance: Curvature*Peakcorr out-of-sample",cex.axis=0.8,col=colo)
boxplot(list(10*amp_shift_mat_outsamp_array[1,1,]+amp_shift_mat_outsamp_array[1,2,],
10*amp_shift_mat_outsamp_array[2,1,]+amp_shift_mat_outsamp_array[2,2,],
10*amp_shift_mat_outsamp_array[3,1,]+amp_shift_mat_outsamp_array[3,2,],
10*amp_shift_mat_outsamp_array[4,1,]+amp_shift_mat_outsamp_array[4,2,],
10*amp_shift_mat_outsamp_array[5,1,]+amp_shift_mat_outsamp_array[5,2,]),
outline=F,names=c("Best MSE",paste("DFA(",
lambda_vec,",",eta_vec,")",sep="")),#dimnames(amp_shift_mat_outsamp_array)[[1]],
main="Aggregate performance: 10*Curvature+Peakcorr out-of-sample",
cex.axis=0.8,col=colo)
dev.off()
@
<<label=z_box_plot_agg.pdf,echo=FALSE,results=tex>>=
  file = paste("z_box_plot_agg", sep = "")
  cat("\\begin{figure}[H]")
  cat("\\begin{center}")
  cat("\\includegraphics[height=6in, width=6in]{", file, "}\n",sep = "")
  cat("\\caption{Box-plots of product (top) and sum (bottom) of Curvature and Peak-Correlation: out-of-sample", sep = "")
  cat("\\label{z_box_plot_agg}}", sep = "")
  cat("\\end{center}")
  cat("\\end{figure}")
@
One could deduce a lot of information from the above plots but the main message is simple and clear: suitably
customized filters (green) outperform the theoretically best MSE-filter (red)
in both dimensions, as measured by Curvature and Peak-Correlation,
simultaneously \emph{out-of-sample}; `extreme' designs (cyan/blue) outperform the MSE-filter markedly in their
stronger attribute without giving-up
too much (or not at all...) in their weaker dimension. Performances by the DFA-MSE filter (yellow) are more ambiguous: it seems to
outperform the best MSE-filter in terms of Peak-Correlation but it is outperformed in terms of Curvature
(but it does not loose on both counts simultaneously). \\

We now derive some other potentially useful conclusions from the above results. We first focus on the disaggregated
measures in figs.\ref{z_box_plot_in} and \ref{z_box_plot_out}. We note that the \emph{out-of-sample} box-plots of the
best MSE-filter (red) are narrower (the distributions
are tighter) than \emph{in-sample} plots because the former evaluation period is much longer (1000-120=880 observations) than the in-sample span ($T=120$).
We remark, also, that the first three DFA-filters (yellow, green, cyan) are subject to slight overfitting since out-of-sample performances are
slightly worse. In contrast, the `extreme' filter (blue) is remarkably immune against overfitting:
the strong customization entailed by $\eta=1.5$ implicitly `freezes' degrees of freedom by shrinking the amplitude function
towards zero in the stopband (kind of implicit regularization). This strong regularization tightens noticeably the Curvature-distribution.
The tradeoff is a flatter Peak-Correlation distribution (higher entropy). In contrast, the two `fastest' filters have a tighter Peak-Correlation
distribution. In general, We observe that customized filters have tighter distributions than the DFA MSE-filter:
emphasizing Timeliness, through $\lambda$, tightens the distribution of the Peak-Correlation;
emphasizing Smoothness, via $\eta$, concentrates the distribution of Curvature. In contrast, the DFA-MSE filter
is `unrestricted' and therefore it is also more sensitive to overfitting, recall
Our results in section \ref{ex_lag_amp}, table \ref{perf_mat_dfa_i_o} and the finite-sample distributions of filter coefficients, amplitude and
time-shift functions in figs. \ref{z_dfa_wn_b_dist}, \ref{z_dfa_wn_amp_dist} and \ref{z_dfa_wn_shift_dist}.\\

When looking at the aggregate measures in fig.\ref{z_box_plot_agg} we see that the customized filters strongly outperform
both MSE-filters when looking at the combined product (top): this is to be expected especially for the two extreme filters (cyan/blue)
because of the marked performance-disequilibria (in such a case the product ought to be smaller).
Interestingly, the outperformance is still marked when looking at the additive aggregation scheme (bottom graph).
We now see that the balanced design
(green) outperforms the other designs, as expected. Finally, the DFA-MSE filter (yellow)
is outperformed by all other filters which was to be expected, too. Of course, our proposed aggregate-measures are to some
extent arbitrary but given the empirical distributions of the individual performance-measures we expect our conclusions to
be fairly robust against alternative aggregation- and/or weighting-schemes: we expect the balanced design (green) to outperform the
(theoretically best) MSE-filter (red) systematically out-of-sample.\\




\textbf{Summary}\\
The `balanced' customized filter $\lambda=\Sexpr{lambda_vec[2]},\eta=\Sexpr{eta_vec[2]}$ outperforms the (DFA) MSE-filter both in terms of
Timeliness \emph{and} Smoothness, Curvature and Peak-Correlation, Selectivity and Mean-Shift and its \emph{out-of-sample} performances
are remarkably consistent (robust against overfitting),
despite the fact that We estimated $L=24$ coefficients based on $T=120$ (10 years)
in-sample data only. In particular, the customized filter outperforms the best theoretical MSE-filter out-of-sample
in both dimensions (Curvature/Peak-Correlation) simultaneously.
The two `unbalanced' filters, prioritizing either Timeliness ($\lambda=\Sexpr{lambda_vec[3]},\eta=\Sexpr{eta_vec[3]}$)
or Smoothness ($\lambda=\Sexpr{lambda_vec[4]},\eta=\Sexpr{eta_vec[4]}$), clearly outperform the MSE-design in their stronger
disciplin without giving-up `too much' in their weaker attribute. Suitably customized filters are less prone to overfitting
than the ordinary MSE-design: emphasizing Timeliness, through $\lambda$, tightens the distribution of the Peak-Correlation and
emphasizing Smoothness, via $\eta$, concentrates the distribution of Curvature.





\section{Replication and Customization of Model-Based and Classic (HP/CF/Henderson) Filters by DFA}\label{replica}


Until yet We used the DFT (the periodogram) as a natural spectral estimate. But We could plug-in
any `reasonable' or interesting alternative statistic, instead. We call this possibility `Spectrum
Interface' since We establish a natural link between the user and the optimization criterion by means
of the spectrum estimate: the DFA is a completely agnostic approach though his author has strong subjective
preferences (let Us confess that `the periodogram and We' are a love affair).
By opening Our mind, We are now in a position to replicate and, more interestingly, to customize classic filter-designs.



\subsection{Replication}

\subsubsection{True Spectrum when the DGP is Known}

Model-based approaches such as implemented in TRAMO/SEATS, X-13 or Stamp rely on ARIMA-models for fitting/forecasting the data. A general
ARIMA-model is a one-sided filter applied to white noise. For illustration We here assume that the DGP is a
stationary AR(1)-process but Our generic approach readily extends to arbitrary ARIMA-models. Let
\[x_t=a_1x_{t-1}+\epsilon_t\]
where $\epsilon_t$ is a white noise process with variance $Var(\epsilon_t)=\sigma^2$. The spectral density of white noise is a flat line
\[h_{\epsilon}(\omega)=\frac{\sigma^2}{2\pi}\]
with the property that the autocovariance satisfies the following system of equations
\[R(k)=\int_{-\pi}^{\pi}h(\omega)\exp(-ik\omega)d\omega=\left\{\begin{array}{cc}\sigma^2&k=0\\0&k\not= 0\end{array}\right.\]
Since the amplitude function of the AR(1)-filter is
\[\frac{1}{1-a_1\exp(-i\omega)}\]
We deduce  that the spectrum of the AR(1)-process $x_t$ is
\begin{equation}\label{spectrum_ar1}
h_{x}(\omega)=\frac{\sigma^2}{2\pi |1-a_1\exp(-i\omega)|^2}
\end{equation}
(convolution theorem). Straightforward generalizations apply to arbitrary (S)ARIMA-processes\footnote{Unit-roots lead to singularities and
the resulting $h_x(\omega)$ is called-pseudo-spectral density, see section \ref{pseudo_pseudo}. Nothing wrong with that,
see Wildi (2008) \url{http://blog.zhaw.ch/idp/sefblog/index.php?/archives/168-RTSE-My-Good-Old-Book.html}
and McElroy-Wildi (2012) for technical details: \url{http://blog.zhaw.ch/idp/sefblog/index.php?/archives/271-Trilemma-Paper-Latest-Sweaved-Version-Available.html}.}.\\

Until yet, We plugged the periodogram $I_{TX}(\omega_k)$, $k=-T/2,...,T/2$ into DFA. But We could have
used $h_x(\omega)$ instead: if $a_1$ is unknown then We could supply an estimate and obtain a corresponding empirical spectrum.
In fact We could rely on the whole SARIMA-apparatus (identification, estimation, diagnostics) and derive corresponding
empirical spectral estimates. By doing so We would just replicate the traditional
model-based approach as shown below.

\subsubsection{Model-Based Solution}

Estimating the output $y_T$ of a (possibly bi-infinite) target filter $\gamma_k$
\[y_T=\sum_{k=-\infty}^{\infty}\gamma_{k} x_{T-k}\]
entails estimation of future observations $x_{T+1},x_{T+2},...$\footnote{Note that we
do not assume symmetry of the filter. Therefore We could easily replicate a pure forecasting problem:
$h$-step ahead forecasting is obtained by specifying $\gamma_{k}=\left\{\begin{array}{cc}1&k=-h\\0& otherwise\end{array}\right.$.
The resulting target transfer function $\Gamma(\omega)=\exp(ih\omega_k)$ is an anticipative \emph{allpass}. Note that the phase $\Phi(\omega)=h\omega$
of the target filter would not vanish anymore but who cares? Just plug that phase into DFA by specifying $Lag=-h$: a breeze!}.
Cleveland proposed to forecast the missing data and then to
apply the target filter to the artificially extended time series\footnote{Backcasts of $x_{0},x_{-1},...$ could be provided as well but
in general filter weights $\gamma_k$ decay sufficiently rapidly to zero such that We could ignore this issue.
Recall that
We tackled the problem in the SA-section by implementing full revision sequencies in DFA.}. We here briefly verify that this (model-based) approach
is fully compatible with DFA when supplying the corresponding model-based spectrum, as proposed above. We
provide purely empirical evidences by comparing amplitude functions of corresponding real-time filters
(the literature is sufficiently enriched by elegant mathematical proofs).\\

In order to pursue We need a target filter: We here propose the ideal lowpass with cutoff $\pi/12$, as used in Our previous framework. Since Our approach is generic we
could also provide any model-based (symmetric) trend-extraction filter (as generated by SEATS or STAMP for example) or any of the classic
HP (Hodrick-Prescott) or CF (Christiano Fitzgerald) or H (Henderson) filters: We really don't care about this choice in the sense that we
could replicate any target. The filter weights $\gamma_k$ of Our target were derived in \ref{gamma_kkk}
\begin{eqnarray*}
\gamma_j&=&\left\{\begin{array}{cc}\displaystyle{\frac{\sin(j\cdot\textrm{cutoff})}{j\pi}}&j\not= 0\\
\displaystyle{\frac{\textrm{cutoff}}{\pi}}&j=0\end{array}\right.
\end{eqnarray*}
Let's now derive the model-based real-time filter for estimating $y_T$
whereby missing (future) data is replaced by forecasts:
\begin{eqnarray}
\hat{y}_T&=&\sum_{k=-\infty}^{-1}\gamma_k\hat{x}_{T-k}+\sum_{k=0}^{T-1}\gamma_kx_{T-k}+\sum_{k=T}^{\infty}\gamma_k\hat{x}_{T-k}\nonumber\\
&\approx&\sum_{k=-\infty}^{-1}\gamma_k\hat{x}_{T-k}+\sum_{k=0}^{T-1}\gamma_kx_{T-k}\nonumber\\
&=&\sum_{k=-\infty}^{-1}\gamma_ka_1^{|k|}x_{T}+\sum_{k=0}^{T-1}\gamma_kx_{T-k}\nonumber\\
&=&\left(\sum_{k=-\infty}^{0}\gamma_ka_1^{|k|}\right)x_{T}+\sum_{k=1}^{T-1}\gamma_kx_{T-k}\label{rt_mba}
\end{eqnarray}
where $a_1^{|k|}x_T$ is the forecast of $x_{T+|k|}$. For simplicity We assumed that $T$ is sufficiently large such that the above approximation applies i.e. backcasts are
neglected. This proceeding is generic, too, and therefore it
straightforwardly extends to arbitrary (S)ARIMA-processes. Expression \ref{rt_mba} is the optimal model-based real-time MSE-filter and
We now compare these coefficients with those obtained by DFA, when plugging
\ref{spectrum_ar1} into Our function $dfa\textunderscore analytic$, see fig.\ref{z_mbaedfa}.
Note that We don't need to generate artificial
data since We are just
interested in confirming the equality of two filters (for an arbitrary AR(1)-parameter $a_1$) and note also that We can rely
on an arbitrarily tight frequency-grid $\omega_k$ since We do not rely on the perioodgram anymore.\\

\subsubsection{DFA Replicates MBA}

We first specify the target (symmetric) lowpass (We assume a relatively
 high resolution $K=1200$ of the discrete frequency-grid):

<<echo=True>>=
# Frequency resolution
K<-1200
# Hypothetical sample length
len<-120
# Symmetric lowpass target
cutoff<-pi/12
# Order of approximation : 10, 100, 1000, 10000
ord<-len
# Compute coefficients gamma
gamma_k<-c(cutoff/pi,(1/pi)*sin(cutoff*1:ord)/(1:ord))
#sum(gamma_k)+sum(gamma_k[2:ord])
# Target in frequency-domain
Gamma<-(0:K)<as.integer(cutoff*K/pi)+1
@
 We now compute the real-time model-based filter:
<<echo=True>>=
omega_k<-(0:K)*pi/K
# AR(1)-coefficient
a1<-0.5
# Model-based filter
gamma_0<-gamma_k%*%a1^(0:ord)
gamma_mba_rt<-c(gamma_0,gamma_k[2:(ord)])
trffkt_mba<-rep(NA,K+1)
weight_func<-trffkt_mba
for (i in 0:K)
{
  trffkt_mba[i+1]<-gamma_mba_rt%*%exp(1.i*omega_k[i+1]*(0:(ord-1)))

}
amp_mba<-abs(trffkt_mba)
@
Finally We proceed to replication by DFA: We compute a filter of lengths $L=120$ and
$L=12$ in order to inspect finite-(short) sample issues.

<<echo=True>>=
L<-len
# model-based spectrum
weight_func<-1/abs(1-a1*exp(1.i*omega_k))^2
# MSE-filter
lambda<-0
eta<-0
# Real-time design
Lag<-0
# Unconstrained filter
i1<-F
i2<-F

dfa_ar1_long<-dfa_analytic(L,lambda,weight_func,Lag,Gamma,eta,cutoff,i1,i2)

# Much shorter sample length
L<-len/10
lambda<-0
eta<-0
Lag<-0
i1<-F
i2<-F

dfa_ar1_short<-dfa_analytic(L,lambda,weight_func,Lag,Gamma,eta,cutoff,i1,i2)
@
The amplitude functions of MBA- and DFA-filters are plotted in fig.\ref{z_mbaedfa}.
<<echo=True>>=
file = paste("z_mbaedfa.pdf", sep = "")
pdf(file = paste(path.out,file,sep=""), paper = "special", width = 6, height = 6)
par(mfrow=c(2,1))
plot(dfa_ar1_long$b,type="l",axes=F,col="blue",ylim=c(min(dfa_ar1_long$b),
max(dfa_ar1_long$b)),ylab="",xlab="",main=paste("Filter coefficients of real-time
model-based (red) and DFA-MSE (blue): a1=",a1,", T=120",sep=""),lwd=2)
mtext("DFA", side = 3, line = -1,at=len/2,col="blue")
lines(gamma_mba_rt,col="red",lwd=1)
mtext("Model-based", side = 3, line = -2,at=len/2,col="red")
axis(1,at=c(1,rep(0,6))+as.integer((0:6)*len/6),
labels=as.integer(0+(0:6)*(len)/6))
axis(2)
box()
plot(dfa_ar1_short$b,type="l",axes=F,col="blue",ylim=c(min(dfa_ar1_short$b),
max(dfa_ar1_short$b)),ylab="",xlab="",main=paste("Filter coefficients of real-time
model-based (red) and DFA-MSE (blue): a1=",a1,", T=12",sep=""),lwd=2)
mtext("DFA", side = 3, line = -1,at=len/20,col="blue")
lines(gamma_mba_rt[1:12],col="red",lwd=1)
mtext("Model-based", side = 3, line = -2,at=len/20,col="red")
axis(1,at=1:12,labels=0:11)
axis(2)
box()
dev.off()
@
<<label=z_mbaedfa.pdf,echo=FALSE,results=tex>>=
  file = paste("z_mbaedfa", sep = "")
  cat("\\begin{figure}[H]")
  cat("\\begin{center}")
  cat("\\includegraphics[height=6in, width=6in]{", file, "}\n",sep = "")
  cat("\\caption{Filter coefficients of real-time model-based (red) and DFA-MSE (blue)", sep = "")
  cat("\\label{z_mbaedfa}}", sep = "")
  cat("\\end{center}")
  cat("\\end{figure}")
@
One can run the above code for arbitrary $a_1, T$ (sample length) and $K$ (resolution of frequency-grid) and verify the claimed identity.
For small $T$ (bottom graph, $T=12$) one notices small differences between \ref{rt_mba} and DFA because the latter
computes best \emph{finite sample} filters whereas Our approximation \ref{rt_mba} ignores backcasts i.e. it assumes a
semi-infinite sample (which is
contradicted by relying on a short sample $T=12$). The resolution of the frequency-grid has an impact too but for $K$ sufficiently
large (say $K>100$) the differences are negligible by all practical means. Finally, let us emphasize here that We can safely estimate
filters of length $L=T$ because a model-based (empirical) spectrum is smooth, assuming the model does not
suffer overfitting.\\

\textbf{Summary}\\
If We ignore backcasts (large sample size $T$) and if We assume that the frequency-resolution is sufficiently tight, then the
DFA replicates the MBA arbitrarily well. This replication would still hold for (very) small sample sizes but then We would have
to account explicitly for the possibility of backcasts in the MBA (which was ignored merely for convenience).





\subsection{Customization: Improving Timeliness and Smoothness, Selectivity and Mean-Shift, Curvature and Peak-Correlation
of Real-Time Model-Based Filters}


Obviously there is no direct benefit to expect from a pure replication of the classic time-domain model-based approach (MBA) by DFA.
But once replicated, We could
be tempted to customize the MBA. We acknowledge that many users rely on models; also, We are aware that many users are
not unconditionally happy with the outcome of the MBA (say in terms of classic Curvature and/or Peak-Correlation measures);
therefore We  here provide an original contribution for model-based proponents:
\textbf{enhance real-time characteristics of pure model-based approaches without leaving the maximum-likelihood paradigm!}\\

We here apply the previously defined $Performance\textunderscore func$ to artificial data generated by an AR(1) model-specification
and compare real-time MBA-designs -- MSE and \emph{customized MBA} -- , assuming knowledge of the true DGP (therefore We do not
distinguish in-sample and out-of-sample time spans). We first generate the data:


<<echo=True>>=
set.seed(10)
len<-120
len1<-1000
# The longer sample is used for implementing the symmetric filter and
# for computing peak-correlations
x1<-arima.sim(list(ar=a1),n=len1)
# In previous sections the shorter sample of length 120 was used for
# estimating the real-time DFA-filters
# We do not use in-sample data here because we know the DGP
#   (but we still have to define x in the head of the function)
x<-x1[1:len]
@
We now compute model-based MSE and model-based customized filters by relying on our
work-horse $Performance\textunderscore func$-function. Model-based AR(1)-spectra are obtained
by specifying $mba<-T$ (otherwise the periodogram will be used).
<<echo=True>>=
# Length of symmetric target filter
L_sym<-61
# Length of model-based filters
L<-120
# Real-time
Lag<-0
# No filter restrictions
i1<-i2<-F
# Designs: three customized filters (the MSE is automatically computed)
lambda_vec<-c(32,100,500)
eta_vec<-c(0.5,1,0.3)
# Use model-based spectrum
mba<-T

# Compute in/out-of-sample performances
perf<-Performance_func(lambda_vec,eta_vec,len1,len,x1,weight_func,Gamma,cutoff,L,L_sym,a1,mba)

xf<-perf$xf
amp_shift_mat<-cbind(perf$amp_shift_mat_insamp,perf$amp_shift_mat_outsamp)
ats_mat<-perf$ats_mat
amp<-perf$amp
shift<-perf$shift
xf_sym<-perf$xf_sym
@
Amplitude and time-shift functions are plotted in fig.\ref{z_dfa_cust_ats_mba}.

<<echo=True>>=
#
# Plots
colo<-rainbow(dim(amp)[2]+1)
file = paste("z_dfa_cust_ats_mba.pdf", sep = "")
pdf(file = paste(path.out,file,sep=""), paper = "special", width = 6, height = 6)
par(mfrow=c(1,2))
plot(Gamma,type="l",axes=F,col="black",ylim=c(0,1),ylab="",xlab="",main="Amplitude")
mtext("Target", side = 3, line = -1,at=K/2,col="black")
for (i in 1:dim(amp)[2])
{
  lines(amp[,i],lty=1,col=colo[i])
  mtext(dimnames(amp)[[2]][i], side = 3,
  line = -1-i,at=K/2,col=colo[i])
}
axis(1,at=1+0:6*K/6,labels=c("0","pi/6","2pi/6","3pi/6","4pi/6","5pi/6","pi"))
axis(2)
box()
plot(rep(0,K+1),type="l",axes=F,col="black",ylim=c(min(shift),max(na.exclude(shift))),
ylab="",xlab="",main="Shift")
mtext("Target", side = 3, line = -1,at=K/2,col="black")
for (i in 1:dim(amp)[2])
{
  lines(shift[,i],lty=1,col=colo[i])
  mtext(dimnames(shift)[[2]][i], side = 3,
  line = -1-i,at=K/2,col=colo[i])
}
axis(1,at=1+0:6*K/6,labels=c("0","pi/6","2pi/6","3pi/6","4pi/6","5pi/6","pi"))
axis(2)
box()
dev.off()
@
<<label=z_dfa_cust_ats_mba.pdf,echo=FALSE,results=tex>>=
  file = paste("z_dfa_cust_ats_mba", sep = "")
  cat("\\begin{figure}[H]")
  cat("\\begin{center}")
  cat("\\includegraphics[height=4in, width=6in]{", file, "}\n",sep = "")
  cat("\\caption{MBA vs. customized filters: amplitude and time-shifts", sep = "")
  cat("\\label{z_dfa_cust_ats_mba}}", sep = "")
  cat("\\end{center}")
  cat("\\end{figure}")
@
The amplitude functions are shrunken by the more or less heavy customization: so We need to look at Our (normalized)
performance measures in order to get a better feeling of the resulting `noise-suppression'.
Increasing $\lambda$ pulls the time-shift down in
the passband, as desired. We expect that Timeliness, Peak-Correlation and Mean-Shift should be positively affected.
Performance measures are reported in table \ref{amp_shift_mat_mba}: note that We do not distinguish in- and out-of-sample
performances since the DGP is known.
<<label=amp_shift_mat_mba,echo=FALSE,results=tex>>=
  library(Hmisc)
  require(xtable)
  #latex(cor_vec, dec = 1, , caption = "Example of using latex to create table",
  #center = "centering", file = "", floating = FALSE)
  xtable(amp_shift_mat, dec = 1,digits=rep(6,dim(amp_shift_mat)[2]+1),
  paste("In-sample performances measures: MBA-MSE vs. MBA-customized",sep=""),
  label=paste("amp_shift_mat_mba",sep=""),
  center = "centering", file = "", floating = FALSE)
@
The customized designs generally beat the `plain-vanilla' MSE-design in all dimensions (except for Selectivity
of the extreme design in the last row).  \\

\textbf{Remark}\\
Since the DGP is known, fiddling and tweaking $(\lambda,\eta)$ is not subject to overfitting (at least with
respect to the frequency-domain measures Selectivity and Mean-Shift). Therefore performances apply to the
DGP rather than to a singular realization. As a result, one
could experiment with `extreme' customization-settings. Also, one can compute filters of arbitrary length
(We selected $L=120$).\\




To conclude, We briefly compare the obtained model-based filter outputs and reference them against the symmetric filter, see
fig.\ref{z_dfa_cust_amp_out_ats_mba}.
<<echo=True>>=
file = paste("z_dfa_cust_amp_out_ats_mba.pdf", sep = "")
pdf(file = paste(path.out,file,sep=""), paper = "special", width = 6, height = 6)
# Select out-of-sample period
anf<-3*len
enf<-5*len
sel<-1:dim(amp)[2]
mplot<-scale(cbind(xf_sym,xf)[anf:enf,])
plot(as.ts(mplot[,1]),type="l",axes=F,col="black",ylim=c(min(na.exclude(mplot)),
max(na.exclude(mplot))),ylab="",xlab="",
main="Final symmetric vs. real-time MBA: MSE and customized",lwd=2)
mtext("Final symmetric", side = 3, line = -1,at=(enf-anf)/2,col="black")
for (i in 1:length(sel))
{
  lines(as.ts(mplot[,i+1]),col=colo[sel[i]],lwd=2)
  mtext(dimnames(amp)[[2]][i], side = 3, line = -1-i,at=(enf-anf)/2,col=colo[sel[i]])
}
axis(1,at=c(1,rep(0,6))+as.integer((0:6)*(enf-anf)/6),
labels=as.integer(anf+(0:6)*(enf-anf)/6))
axis(2)
box()
dev.off()
@
<<label=z_dfa_cust_amp_out_ats_mba.pdf,echo=FALSE,results=tex>>=
  file = paste("z_dfa_cust_amp_out_ats_mba", sep = "")
  cat("\\begin{figure}[H]")
  cat("\\begin{center}")
  cat("\\includegraphics[height=6in, width=6in]{", file, "}\n",sep = "")
  cat("\\caption{Filter outputs out-of-sample: final symmetric vs. real-time MSE and customized designs", sep = "")
  cat("\\label{z_dfa_cust_amp_out_ats_mba}}", sep = "")
  cat("\\end{center}")
  cat("\\end{figure}")
@
All series are standardized in order to match scales.  We can observe that the customized filters tend to lie `on the left'
of the MSE-filter, as measured by Peak-Correlations, and that the series are smoother (except the extreme design),
as measured by Curvature or Selectivity.\\




\subsection{Replication of HP- and CF-Filters}

HP and CF-filters can be replicated by model-based designs and therefore they can be replicated
and customized by the DFA (transitivity). We refer the interested reader to SEFBlog:
\begin{itemize}
\item HP-filter: \url{http://blog.zhaw.ch/idp/sefblog/index.php?/archives/261-Replication-of-the-Hodrick-Prescott-Filter-by-I-MDFA.html}
\item CF-filter: \url{http://blog.zhaw.ch/idp/sefblog/index.php?/archives/264-Replication-of-the-Christiano-Fitzgerald-CF-Filter-by-I-MDFA.html}
\end{itemize}
Obviously, any classic (or otherwise extravagant) filter can be customized.





\section{Multivariate Direct Filter Approach: MDFA}

The additional M... I don't have time to put this stuff into a unifying format right now. The interested reader is referred
to SEFBlog for code, paper and tutorials on the topic.


\section{Appendix}

\subsection{A Proof}

\begin{Proposition}\label{dper3}
The periodogram of a sequence \(x_1,...,x_T\) satisfies
\begin{equation}\label{per3_i}I_{TX}(\omega_k)= \left\{\begin{array}{ccc}\displaystyle{
\frac{1}{2\pi} \sum_{j=-(T-1)}^{T-1} \hat{R}(j) \exp(-ij\omega_k)}&,&|k|=1,...,T/2\\
\displaystyle{\frac{T}{2\pi}}\overline{x}^2&,&k=0 \end{array}\right.\end{equation}
Furthermore, for $|t|=0,...,T-1$ the following holds
\begin{equation}\label{disdv_i}
\frac{2\pi}{T} \sum_{k=-T/2}^{T/2} w_k\exp(-it\omega_k)I_{TX}(\omega_k)=\left\{\begin{array}{ccc}
\hat{R}(t)+\hat{R}(T-t)&,&t\not= 0\\
\hat{R}(0)&,&\textrm{otherwise}
\end{array}\right.
\end{equation}
\end{Proposition}
\textrm{Proof}\\

A proof of the first claim is proposed in Brockwell and Davis \cite{brockwelldavis93},
proposition 10.1.2. The second
assertion follows from
\begin{eqnarray}
&&\frac{{2\pi}}{T} \sum_{k=-T/2}^{T/2}w_k \exp(-i t\omega_k) I_{TX}(\omega_k)\nonumber\\
&=&\frac{2\pi}{T} \sum_{k=-T/2}^{T/2} w_k\exp(-i t\omega_k)\frac{1}{2\pi}
\sum_{j=-(T-1)}^{T-1} \hat{R}(j) \exp(-ij\omega_k)\nonumber\\
&=&\frac{1}{T} \sum_{j=-(T-1)}^{T-1} \hat{R}(j) \sum_{k=-T/2}^{T/2} w_k\exp(-i (t+j)\omega_k)
\nonumber\\
&=&\frac{1}{T} \sum_{j=-(T-1)}^{T-1} \hat{R}(j) \exp(-i(j+t)\omega_{-T/2})
\sum_{k=0}^{T} w_k\exp(-i(j+t)\omega_k)
\nonumber\\
&=&\frac{1}{T} \sum_{j=-(T-1)}^{T-1} \hat{R}(j) \exp(-i(j+t)\omega_{-T/2})
\sum_{k=1}^{T} \exp(-i(j+t)\omega_k)
\nonumber\\
&=&\left\{\begin{array}{ccc}
\hat{R}(t)+\hat{R}(T-t)&,&t\not= 0\\
\hat{R}(0)&,&\textrm{otherwise}
\end{array}\right.\nonumber
\end{eqnarray}
where the last equality follows from the orthogonality relations \ref{orthrel},
noting that $j+t=0$ `generates' $\hat{R}(-t)$ or, equivalently, $\hat{R}(t)$ and that
$j+t=T$ `generates' $\hat{R}(T-t)$ if $t\not= 0$.\\

Note also that \ref{disdv_i} is a consequence of the implicit periodicity of the data assumed by
the periodogram-construct. To see this assume $t=1$ and let $X_0=X_T$ (`cyclical' data). Then
\begin{eqnarray*}
\frac{2\pi}{T} \sum_{k=-T/2}^{T/2} w_k\exp(-i\omega_k)I_{TX}(\omega_k)&=&
\hat{R}(1)+\hat{R}(T-1)\\
&=&\frac{1}{T}\sum_{k=1}^{T-1}X_kX_{k+1}+\frac{1}{T}X_1X_T\\
&=&\frac{1}{T}\sum_{k=1}^{T-1}X_kX_{k+1}+\frac{1}{T}X_1X_0\\
&=&\frac{1}{T}\sum_{k=0}^{T-1}X_kX_{k+1}
\end{eqnarray*}
which is the `natural' estimate of $R(1)$ (if $E[X_t]=0$).\\



\subsection{A Bad `Smart' Idea}

An utterly simple proceeding:
\begin{itemize}
\item Transform the data $x_t$ to the frequency-domain via DFT.
\item Multiply the DFT by the transferfunction $\Gamma(\cdot)$ of the target filter.
\item Transform the obtained DFT back to the time-domain, by means of the IDFT.
\end{itemize}
If this worked We could have skipped everything past section \ref{dft_idft_dft}. So: what could be wrong with Fourier?
See exercise \ref{period_e}, p.\pageref{period_e} and a SEFBlog-entry devoted to the topic
\url{http://blog.zhaw.ch/idp/sefblog/index.php?/archives/145-Filtering-in-the-Frequency-Domain-a-Bad-Smart-Idea.html}. The
resulting filtering could work well for smoothing historical data but real-time performances for typical econonomic
data are likely to be intolerable.







\end{document}


