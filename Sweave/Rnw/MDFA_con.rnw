

\chapter{Multivariate Direct Filter Analysis for Non-stationary Processes}
\label{chap:int}

 We now extend the basic MDFA of Chapter \ref{chap:basic}  by considering
 the method's application to  non-stationary processes.  
 Section \ref{sec:constraint} introduces the idea of filter constraints
arising from time-varying means, a form of non-stationarity.
 This treatment is generalized in Section \ref{sec:non-stat}
  by the definition of non-stationary processes, and theory for the corresponding
   model-based filters is developed.  Finally, the MDFA criterion for
    non-stationary processes is discussed in Section \ref{sec:mdfa-nonstat}.
 
   
% Section \ref{i1i2_intr} sets-up the context; a link to integrated processes is proposed in 
% section \ref{pseudo_dft}; a general matrix notation is proposed in 
% section \ref{cons_gen_par}; section \ref{optim_stat} extends the former 
% (unconstrained) MDFA-criterion to the constrained case; finally, section 
% \ref{const_impl_mdfa} illustrates effects (of the constraints) on 
%  characteristics of real-time filters.  



\section{Constrained MDFA}
\label{sec:constraint}

 Various constraints upon the concurrent filter can be envisioned, 
   and imposing such strictures results in  a constrained MDFA. 
   A chief case of interest arises when the 
    data process has a time-varying mean (which is a form of  non-stationarity);
  then it is necessary to impose additional filter constraints -- otherwise
   the filter error will not have mean zero.    To see why, 
   Write $\Delta (L) = \Psi (L) - \widehat{\Psi} (L)$ as the discrepancy filter,
   so that we see  from (\ref{eq:dfa-error})  
   that $\EE [ E_t ] = \Delta (L) \, \EE [ X_t ]$; 
   by Definition \ref{def:lpp}, we require
 that $\EE [ E_t ] = 0$ for any LPP.  
  If $\EE [ X_t] = 0$ then this condition is always satisfied, but
   for most time series of interest the mean will be nonzero, and is typically
    time-varying.  For such cases additional constraints on $\Delta (L)$ must be imposed,
    which implicitly amount to constraints on $\widehat{\Psi} (L)$.
    
\begin{Example}    {\bf Constant Mean.}  \rm
\label{exam:constant.mean}
  If $\EE [ X_t ] = \mu$, some nonzero constant,  then we require $\Delta (1) = 0$.
  This is because the mean of the filter error is
  \[
   \Delta (B) \, \EE [ X_t] = \Delta(B) \, \mu = \sum_j \delta (j) \, \mu =
   \Delta (1) \, \mu,
  \]
  and this is zero only if $\Delta (1) = 0$.  This is called a Level Constraint (LC).
\end{Example}  

\begin{Example}    {\bf Linear Mean.}  \rm
\label{exam:linear.mean}
  Suppose that $\EE [ X_t ] = \mu \, t$, where $\mu$ is a nonzero slope
 of a linear time trend.  Then it is required that  $\partial {\Delta} (1) = 0$
  in addition to the LC,  which is seen as follows:
  \[
   \Delta (B) \, \EE [ X_t] = \Delta(B) \, \mu \, t =   \mu \, \sum_j \delta (j) \, (t-j)
   = \mu \, \left(t \, \sum_j \delta (j) - \sum_j j \,\delta (j) \right)
    = \mu \, t \, \Delta(1) - \mu \, \partial \Delta (1).
  \]
  This mean of the filter error  is zero only if both $\Delta(1)=0$ and
  $\partial \Delta (1)=0$; the latter condition is called the
   Time-Shift Constraint (TSC).  
\end{Example}  

     Hence, for linear means we obtain
 three fundamental types of constraints: LC, TSC, and Level plus 
 Time-Shift Constraint (LTSC), which combines both LC and TSC.
  Using the fact that $\Delta (L) = \Psi (L) - \widehat{\Psi} (L)$,
   these three constaints can be described as follows:
\begin{align*}
 \mbox{LC} : &  \;  \Delta (1) = 0 \quad \mbox{or} \quad \Psi (1) = \widehat{\Psi} (1) \\
 \mbox{TSC} : &  \;   \partial {\Delta} (1) = 0 \quad \mbox{or} \quad 
 \partial {\Psi} (1) = \partial {\widehat{\Psi}} (1)  \\
 \mbox{LTSC} : &  \;  \Delta (1) = 0,  \,  \partial {\Delta} (1) = 0 \quad 
 \mbox{or} \quad \Psi (1) = \widehat{\Psi} (1), \; \partial {\Psi} (1) =
 \partial {\widehat{\Psi}} (1).
\end{align*}
 In the case of  concurrent filters of form  (\ref{eq:conc.filter}), 
 LC is accomplished by demanding that 
  $\sum_{j=0}^{q-1} \widehat{\psi} (j) = \Psi(1)$.   More generally, we consider  linear constraints  formulated via
\begin{equation}
\label{eq:concurrent-constrain}
  \vartheta = R \, \varphi + Q,
\end{equation}
 where $R$ is $n q \times n r$ and $\varphi$ is $n r \times 1$ dimensional, consisting of 
 free parameters; $Q$ is a matrix of constants, and is $n q \times 1$ dimensional.


\begin{Illustration}  {\bf Level Constraint (LC).}   \rm
\label{ill:lc}
 Note that $\sum_{j=0}^{q-1} \widehat{\psi} (j) = \Psi(1)$ implies that
\begin{equation}
\label{eq:lc-gamma0}
 \widehat{\psi} (0) = \Psi(1) - \sum_{j=1}^{q-1} \widehat{\psi} (j).
\end{equation}
 Hence  $ \varphi^{\prime}  = [ \widehat{\psi} (1), \widehat{\psi} (2), \ldots, \widehat{\psi} (q-1) ] $ and
\[
	R  = \left[ \begin{array}{ccc} -1 & \ldots & -1 \\ 1 & 0 & 0 \\
		\vdots & \ddots & \vdots \\ 0 & 0 & 1  \end{array} \right]  \otimes 1_n \qquad
	Q = \left[ \begin{array}{c} \Psi (1) \\ 0 \\ \vdots \\ 0 \end{array} \right].
\]
\end{Illustration}
 
 
\begin{Illustration}  {\bf Time Shift Constraint (TSC).}   \rm
\label{ill:tsc}
   The constraint is $\partial {\Psi} (1) = \partial \widehat{\Psi} (1)
   = \sum_{j=0}^{q-1} j \, \widehat{\psi} (j)$,
 or $\widehat{\psi} (1)  = \partial {\Psi} (1)  -  \sum_{j=2}^{q-1} j \, \widehat{\psi} (j) $.
 Hence  $ \varphi^{\prime}  = [ \widehat{\psi} (0), \widehat{\psi} (2), \ldots, \widehat{\psi} (q-1) ] $ and
\[
	R  = \left[ \begin{array}{cccc} 1 & 0 &  \ldots &  0  \\  0 & -2  &  -3  & \ldots  \\
		0 & 1 & 0 & \ldots \\ 
		\vdots & \ddots & \vdots & \vdots \\ 0 & \ldots & 0 & 1 \end{array} \right] \otimes 1_n \qquad
	Q = \left[ \begin{array}{c} 0 \\ \partial {\Psi} (1) \\ 0 \\ \vdots \\ 0 \end{array} \right].
\]
\end{Illustration}
 
 
\begin{Illustration}  {\bf Level and Time Shift Constraint (LTSC).}  \rm
\label{ill:ltsc}
   Take the Time Shift Constraint formula for $\widehat{\psi} (1)$,
 and plug this into (\ref{eq:lc-gamma0}), to obtain
\begin{align*}
 \widehat{\psi} (0)  & = \Psi (1) - \left( \partial {\Psi} (1)  -  \sum_{j=2}^{q-1} j  \, \widehat{\psi} (j) \right) -  \sum_{j=2}^{q-1} 
 \widehat{\psi} (j)  \\
	& = \Psi (1) -  \partial {\Psi} (1)  +  \sum_{j=2}^{q-1} (j-1)  \, \widehat{\psi} (j).
\end{align*}
 Hence  $ \varphi^{\prime}  = [  \widehat{\psi} (2), \ldots, \widehat{\psi} (q-1)  ] $ and
\[
	R  = \left[ \begin{array}{cccc} 1 & 2  &  3  &   \ldots    \\  -2  & -3  &  -4  & \ldots  \\
		 1  & 0 & \ldots & 0 \\ 
		\vdots & \ddots & \vdots & \vdots \\ 0 & \ldots & 0 & 1 \end{array} \right]  \otimes 1_n \qquad
	Q = \left[ \begin{array}{c} \Psi (1) - \partial {\Psi} (1)  \\  \partial {\Psi} (1) \\ 0 \\ \vdots \\ 0 \end{array} \right].
\]
\end{Illustration}



 More generally, we can envision an LPP involving $M$ linear constraints on 
  $\vartheta$, taking the form
 $   A = [ J \otimes 1_n ] \, \vartheta$, where $J$ is $M \times q$ 
 dimensional ($M < q$) and $A$ is $n M \times 1$ dimensional.
 (The LC, TSC, and LTSC examples all have this form.)  In order to express 
 this constraint in the form 
 (\ref{eq:concurrent-constrain}), we use the Q-R decomposition 
 (Golub and Van Loan, 1996) of $J$, writing
 $J = C \, G \, \Pi$ for an orthogonal matrix $C$ (which is $M \times M$ dimensional),
 a rectangular upper triangular matrix $G$
 (which is $M \times q$ dimensional), and a permuation matrix $\Pi$ 
 (which is $q \times q$ dimensional).  
 Standard matrix software such as $\textsc{R}$ will provide the Q-R decomposition $J$,
 and should produce the rank of $J$ as  a by-product --
 if this is less than $M$, then there are redundancies in the 
 constraints that should first be eliminated. 
 
 \begin{Exercise} {\bf QR Decomposition.} \rm
 \label{exer:qr.constraint}
 TO DO
 \end{Exercise}
 
 Hence  proceeding with a full rank $J$, we partition $G$ as $G = [ G_1 \, G_2]$ 
 such that $G_1$ has $M$ columns and $G_2$
 has $q-M$ columns.  This quantity $q-M$ corresponds to the number 
 of free coefficient matrices, and is therefore the same as $r$.
 The Q-R decomposition guarantees that $G_1$ is an upper triangular matrix, 
 and moreover it is invertible.  Therefore
\[
  \left[ G_1^{-1} \, C^{-1} \otimes 1_N \right] \, A  = \left( \left[ 1_M , \, G_1^{-1} \, G_2 \right] \, \Pi \otimes 1_N  \right) \, \vartheta,
\]
 and the action of $\Pi$ (together with the tensor product) amounts 
 to a   permutation of the elements of $\vartheta$.
  Let the output of this permutation be denoted
\[
   \left[ \begin{array}{l} \overline{\vartheta} \\ \underline{\vartheta} \end{array} \right]
   = \left( \Pi \otimes 1_N \right) \, \vartheta,
\]
 where $\overline{\vartheta}$ is $N M \times 1$ dimensional and
 $\underline{\vartheta}$ is $N r \times 1$ dimensional.  
 Then  by substitution we can solve for $\overline{\vartheta}$ in terms 
 of $\underline{\vartheta}$:
\[
   \overline{\vartheta} =  \left[ G_1^{-1} \, C^{-1} \otimes 1_N \right] \, A - 
   \left[  G_1^{-1} \, G_2  \otimes 1_N   \right] \, \underline{\vartheta}.
\]
 Therefore we recognize the free variables $\varphi = \underline{\vartheta}$, 
 and obtain $R$ and $Q$ in (\ref{eq:concurrent-constrain}) via
\begin{align*}
   R & = \Pi^{-1} \, \left[ \begin{array}{c} - G_1^{-1} \, G_2 \\ 1_{r} \end{array} \right] \otimes 1_N  \\
  Q & = \left( \Pi^{-1}  \, \left[ \begin{array}{c}  G_1^{-1} \, C^{-1} \\ 0 \end{array} \right] \otimes 1_N  \right) \, A.
\end{align*}
  These formulas allow one to compute the   form (\ref{eq:concurrent-constrain}) 
   from given constraints, and
 an analytical solution to the resulting MDFA criterion  be obtained from the following result.

\begin{Proposition}
\label{prop:mdfa.quadsoln-constrain}
 The minimizer of the  MDFA criterion given by   (\ref{eq:mdfa-criterion})
 with respect to  $\mathcal{G}$ -- consisting of all length $q$ concurrent filters 
 subject to  linear constraints of the form (\ref{eq:concurrent-constrain}) -- is
\begin{equation}
\label{eq:phi.soln-constained}
 \varphi =  { \left[ R^{\prime} \, B \, R \right] }^{-1} \, R^{\prime} \,
 \left( b - B \, Q \right).
\end{equation}
  Letting $H = 1_{Nq} - R \,   { \left[ R^{\prime} \, B \, R \right] }^{-1} \,
  R^{\prime} \, B$, the minimal value is  
\begin{equation}
\label{eq:opt.val.mdfa-constrained}
{ \langle \Psi (z) \, G \, { \Psi (z) }^* \rangle }_0 - b^{\prime} \, 
R \, { \left[ R^{\prime} \, B \, R \right] }^{-1} \, R^{\prime} \,  b
	+ Q^{\prime} \, B \, H \, Q - 2 \, b^{\prime} \, H \, Q.
\end{equation}
\end{Proposition}

For computation, we utilize the same approximations to $B$ and $b$ as discussed 
in  Chapter \ref{chap:basic},
 obtaining the constrained MDFA filter $\vartheta$ via (\ref{eq:phi.soln-constained})
 followed by (\ref{eq:concurrent-constrain}).

%HERE  add exercise on ideal low-pass with various constraints 
%  for a WN process with linear trend.  
% repeat with VAR(1) with linear trend, Modify following:

% 
% We suppose the true process is a VAR($1$), and apply the MB trend filter defined in Example \ref{exam:trend-i1},
%   where the parameters are given by
% \[
%  \Sigma_{\mu} = \left[ \begin{array}{ll} 
%    2.32 \cdot 10^{-4} &  5.04 \cdot 10^{-4} \\
%    5.04 \cdot 10^{-4}  & 34.73 \cdot 10^{-4}  \end{array}  \right]
%  \qquad  \Sigma_{\iota} = \left[ \begin{array}{ll}
%         110.44 \cdot 10^{-5} &  7.17 \cdot 10^{-5}  \\
%         7.17 \cdot 10^{-5} & 128.57 \cdot 10^{-5}   \end{array} \right].
% \]
%   Because the trend variance for the second component is $15$ times larger than that of the first component,
%  the correspoding trend filter  does less smoothing.  
%  
% \begin{figure}[htb!]
% \centering
% \includegraphics[]{petrolVAR1trends}
% \caption{\baselineskip=10pt Bivariate  trend  filter applied to VAR($1$) simulation (grey), with trends in black. }
% \label{fig:petrol.var1trends}
% \end{figure}
% 
% 
%   We seek to solve the corresponding trend extraction LPP.    First, we can use the optimal solution  (\ref{eq:var1.lpp-opt})
% given in   Illustration \ref{ill:var1}, supposing that we know  that the VAR($1$) is correctly specified.  
%  Second, we can use MDFA, proceeding as if we do not know the true process is a VAR($1$), as we would in practice, 
% and hence use the periodogram;  MDFA should be able to replicate the optimal solution, so long as the filter class
%  $\mathcal{G}$ is sufficiently rich.    The VAR($1$)  is defined by
% \[
%   X_t =  \left[ \begin{array}{ll}  1.0  & 0.5 \\    -0.2  &  0.3  \end{array} \right] \, X_{t-1} + \epsilon_t,
% \]
%  with stationary initialization, and $\{ \epsilon_t \}$ a Gaussian white noise of identity innovation variance.
%   Operationally, we simulate this process with sample size $4500$.  Then the 
%   ideal trends $\Psi (B) X_t$ are produced by truncating the  MB  filter to length $4001$ (it is symmetric, so the indices
%  range between $-2000$ and $2000$) and applying to the  simulation, only retaining the central $500$ data points,
%  as displayed in Figure \ref{fig:petrol.var1trends}.   (In this way we can dispense with edge effects, and the extra
%  $4000$ observations are not used in the MDFA.)     The grey lines of Figure \ref{fig:petrol.var1trends}
%  are the central $500$ observations of the VAR($1$)
%  simulation, and the  black  line is the target.     We wish to use MDFA (setting $q=30$) 
%  with various constraints (LC, TSC, LTSC) 
% to obtain a   real-time estimate,  comparing  the result  to the optimal solution given by implementing  Illustration \ref{ill:var1}.  
%   In that case  we find that
% \[
%   A_{\Psi} (\Phi) = \left[ \begin{array}{ll}  0.317  &  0.218 \\    -0.054  &  0.027  \end{array} \right]
% \]
%  by direct calculation, 
% and hence the optimal filter is easily computed.  The in-sample MSEs of the various methods are displayed in Table \ref{tab:petrol.var1.mdfa}.
%   Note that the basic MDFA (no constraints) replicates the optimal filter, as their MSE is the same up to negligible   error.
%   When imposing a level constraint (LC and LTSC) there is a loss to the MDFA performance, which makes sense given that the optimal
%  filter does {\it not} impose a level constraint -- in fact, the value of the optimal concurrent filter at frequency zero is
% \[
%   \widehat{\Psi} (1) =  \left[ \begin{array}{ll}  0.914 &  0.251 \\    -0.030  &  0.842  \end{array} \right],
% \]
%  which is quite different from $1_2$.  On the other hand, the time shift constraint alone (TSC) has little impact on the performance of MDFA,
%  because $\partial \widehat{\Psi} (1) \approx 0 \cdot 1_2$, i.e., the optimal filter already has this property of zero time shift.
% 
%  
% 
% \begin{table}[!htb]
% \centering
% \begin{tabular}{clllll}
% \hline
%   Series  &  LPP Opt  &  MDFA Basic &  MDFA LC &  MDFA TSC   &  MDFA LTSC \\
%   1 & .2404  &  .2388  &  .2877  &   .2363  & .4367  \\
%   2 & .0224 &   .0217 &   .0229  &   .0216  & .0264 \\
% \hline
% \end{tabular}
% \caption{\baselineskip=10pt  LPP MSE for bivariate VAR($1$) process --  with target trend
%  given by the LLM MB trend -- for various concurrent filters: LPP Opt is the optimal filter,
%  whereas the MDFA filters are labeled according to the constraints imposed. }
% \label{tab:petrol.var1.mdfa}
% \end{table}
%  




\section{Background on Non-stationary Vector Time Series }
\label{sec:non-stat}



We next consider processes that when differenced are 
stationary, which are the most common type occuring in econometrics and finance.   The general treatment of co-integration is complicated when
 multiple unit roots are present.  For example, if there are trend
 and seasonal roots present, application of a co-integrating vector
 to the data process may only reduce the order of non-stationarity
 somewhat, rather than making the series stationary. 

 We first illustrate this point through dynamic factor component models.  Let
 the differencing polynomial be $\delta (B) = \prod_{\ell=1}^p {(1 -
 e^{i \omega_{\ell}} B )}^{q_{\ell}}$, where $q_{\ell}$ is the
 multiplicity of each unit root at frequency $\omega_{\ell}$.  When $\omega_{\ell}$ is not $0$ or $\pi$,
 we know a conjugate factor must appear in $\delta (B)$.  By a convenient abuse of notation, we denote
the pairing of such conjugate factors by ${(1 - e^{i \omega_{\ell} } B)}^{q_{\ell}}$, with the understanding that
 $q_{\ell} $ is even and denotes the produce of conjugate factors.

Suppose that the data process can be written as the sum of
 non-stationary latent processes, each of which has differencing
 polynomial ${(1 - e^{i \omega_{\ell}} B )}^{q_{\ell}}$, plus a
 residual stationary process.  We write this as
\begin{equation}
 \label{eq:chapnstat_structural}
 x_t = \sum_{\ell=1}^p s^{(\ell)}_t + s^{(0)}_t,
\end{equation}
 where ${(1 - e^{i \omega_{\ell}} B )}^{q_{\ell}} s^{(\ell)}_t$ is
 stationary for each $1 \leq \ell \leq p$, and $s^{(0)}_t$ is
 stationary as well.  Let the
 reduced polynomials $\delta^{(\ell)} (B) = \delta (B) \, {(1 -
 e^{i \omega_{\ell}} B )}^{-q_{\ell}}$ be defined.  Then applying
 $\delta (B)$ to the structural equation (\ref{eq:chapnstat_structural}) yields
\[
 \partial x_t  : = \delta^{(\ell)} (B) x_t = \sum_{\ell=1}^p \, \delta^{(\ell)} (B) \partial
 s^{(\ell)}_t + \delta (B) s^{(0)}_t.
\]
 Here the $\partial$ notation before a process refers to the
 suitably differenced version of that process, which is stationary.
  Each stationary latent process $\partial s^{(\ell)}_t$ may have
  singularities in its spectral density matrix, such that it can be
  represented as $\Lambda^{(\ell)}$ times some $c^{(\ell)}_t$, a
 stationary process of reduced dimension with spectral density
 matrix invertible at all frequencies.  Such a latent process is
 governed by a dynamic factor model (DFM), with $\Lambda^{(\ell)} =
 I_m$ recovering the general case.  We actually require
 $\Lambda^{(0)} = I_m$ in order to guarantee that the spectrum of
 $\partial x_t$ is non-singular except at a finite number of
 frequencies.

 Suppose that $\beta$ is a vector such that $\beta^{\prime}
 \Lambda^{(k)} = 0$ for some $1 \leq k \leq p$.  Then
\[
 \beta^{\prime} \, \partial x_t = \sum_{\ell \neq k} \,
 \beta^{\prime} \Lambda^{(\ell)} \, \delta^{(\ell)} (B)
 c^{(\ell)}_t + \beta^{\prime} \delta (B) s^{(0)}_t,
\]
 and note that ${(1 - e^{i \omega_{k}} B )}^{q_{k}}$ can be factored
 out of all terms on the right hand side.  Hence $\beta^{\prime}
 x_t$ only requires $\delta^{(k)} (B)$ differencing to become
 stationary; the frequency $\omega_k$ co-integrating vector $\beta$
 reduces the order of non-stationarity by the factor ${(1 - e^{i \omega_{k}} B
 )}^{q_{k}}$.  Moreover, if $\beta$ is in the left null space of
 several factor loadings $\Lambda^{(\ell)}$, the order of
 non-stationarity can be reduced further.  In an extreme case,
 $\beta^{\prime} \Lambda^{(\ell)} = 0$ for $1 \leq \ell \leq p$, so
 that $\beta^{\prime} x_t$ is stationary; however, whether or not
 the factor loadings have a non-trivial intersection of left null
 space depends on each process.

 We now proceed with a general treatment of vector non-stationary
 processes to explore what types of filter constraints are necessary
 when co-integrating vectors are present.  Crucially supposing that the
 differencing operator $\delta (B)$ is the same for each component
 series, we can write
\[
 x_t = \sum_{j=1}^d A_{j, t+d} \, x_{j-d} + \int_{-\pi}^{\pi}
 \frac{ e^{i \omega t} - \sum_{j=1}^d A_{j,t+d} \, e^{-i \omega
 (d-j)} }{ \delta (e^{-i \omega}) } \; d \ZZ (\omega),
\]
 where $d = \sum_{\ell=1}^m q_{\ell}$ and each $A_{j, t+d}$ is a
 time varying function for each $j$, and the $x_{j-d}$ are initial
 values.  This representation is chiefly useful when $t > 1$, though
 it is still valid when $t \leq 0$.  The $\ZZ (\omega)$ is the
 orthogonal increments process in the spectral representation of
 $\partial x_t$. 

 Each of the time-varying functions is in the null
 space of $\delta (B)$, i.e., $\delta (B) A_{j, t+d} = 0$ for $1 \leq
 j \leq d$, where the backshift operator works on the $t+d$ index.
 As a consequence, we can rewrite each $A_{j,t+d}$ as a linear
 combination of the basis functions of the null space of $\delta
 (B)$, which yields a more convenient representation.  Let the basis
 functions be $\phi_j (t) $ for $1 \leq j \leq d$; the existence and
 form of such functions are a basic staple of difference equation
 theory, treated briefly in Brockwell and Davis (1991).  Then we can
 write $A_{j,t+d} = \sum_{k=1}^d G_{jk} \phi_k (t)$ for each $1 \leq
 j \leq d$, for some coefficients $G_{jk}$.  It follows that
\[
 \sum_{j=1}^d A_{j,t+d} B^{d-j} = \sum_{k=1}^d \phi_k (t)  \; \left(
 \sum_{j=1}^d G_{jk} B^{d-j} \right).
\]
 Each expression in parentheses on the right hand side is a degree
 $d-1$ polynomial in $B$, and will henceforth be denoted as $p^{(k)}
 (B)$.   Substituting the new formulation, we obtain
\[
 x_t = \sum_{j=1}^d \phi_j (t) p^{(j)} (B) \, x_{0} + \int_{-\pi}^{\pi}
 \frac{ e^{i \omega t} - \sum_{j=1}^d \phi_j (t) \, p^{(j)} ( e^{-i \omega
 } )}{ \delta (e^{-i \omega}) } \; d \ZZ (\omega),
\]
 where $p^{(j)} (B)$ acts on $x_0$ by shifting the time index $t=0$
 back in time for each power of $B$.  This representation is now
 extremely convenient, because application of any factor of
 $\delta(B)$ will annihilate a corresponding basis function (when
 roots are repeated, some basis functions will also be transformed
 into others that are instead annihilated). 

 Suppose that we left multiply by $\beta^{\prime}$,
 which is a co-integrating vector at frequency $\omega_k$:
\begin{equation}
 \label{eq:co-intRep}
  \beta^{\prime} x_t = \sum_{j=1}^d \phi_j (t) p^{(j)} (B) \, \beta^{\prime} \, x_{0} + \int_{-\pi}^{\pi}
 \frac{ e^{i \omega t} - \sum_{j=1}^d \phi_j (t) \, p^{(j)} ( e^{-i \omega
 } )}{ \delta (e^{-i \omega}) } \; \beta^{\prime} \, d \ZZ
 (\omega).
\end{equation}
  From our previous discussion, we know that the result is a non-stationary
 process with differencing operator $\delta^{(k)} (B)$; this implies
 that there should be a cancelation of $\beta^{\prime} \, d\ZZ
 (\omega)$ with the ${(1 - e^{i \omega_{k}} e^{-i\omega}
 )}^{q_{k}}$ term in $\delta (e^{-i \omega})$.  As a result, we
 have the following spectral formalization of the co-integrating
 relation:
\begin{equation}
\label{eq:co-intRel}
 \beta^{\prime} \, d\ZZ (\omega) = {(1 - e^{i \omega_{k}} e^{-i\omega}
 )}^{q_{k}} \, d\ZZ^{(k)} (\omega),
\end{equation}
 where $d \ZZ^{(k)} (\omega)$ is the orthogonal increments measure
 of another stationary invertible process.  This condition
 (\ref{eq:co-intRel}) is readily satisfied by the latent dynamic
 factor process discussed earlier, which is exemplary of the general
 situation of interest.  The extreme case, where the co-integrating
 vector lies in all the left null spaces of the component processes,
 allows us to factor $\delta (e^{-i \omega})$ completely from
 $\beta^{\prime} d \ZZ (\omega)$, though such a property need not
 hold in practice.  

In order to see the full effect of condition
 (\ref{eq:co-intRel}) on $\beta^{\prime} x_t$, we re-organize terms
 in equation (\ref{eq:co-intRep}).  Let us suppose, without loss of
 generality, that frequency $\omega_k$ has corresponding basis
 functions $\phi_1, \cdots, \phi_{q_k}$, so that the first $q_k$
 basis functions are annihilated by ${(1 - e^{i \omega_k}
 B)}^{q_k}$.  Then we can write
\begin{align*}
 \beta^{\prime} x_t & = \sum_{j= q_k + 1}^d \phi_j (t) p^{(j)} (B) \, \beta^{\prime} \, x_{0}
  + \int_{-\pi}^{\pi} \frac{ e^{i \omega t} - \sum_{j= q_k + 1}^d \phi_j (t) \, p^{(j)} ( e^{-i \omega
 } )}{ \delta^{(k)} (e^{-i \omega}) } \, d \ZZ^{(k)}
 (\omega) \\
 & + \sum_{j=1}^{q_k} \phi_j (t) \,  \left(p^{(j)}
 (B)  \beta^{\prime} \,  x_0 - \int_{-\pi}^{\pi} \frac{ p^{(j)} (e^{-i \omega}) }{
 \delta^{(k)} (e^{-i \omega}) } \, d\ZZ^{(k)} (\omega) \right).
\end{align*}
 The first two terms are immediately recognized as the deterministic
 and stochastic portions respectively of a non-stationary process
 that has $\delta^{(k)} (B)$ for differencing operator.  The third
 term is left over, and consists of deterministic time series that
 are in the null space of ${(1 - e^{i \omega_k}
 B)}^{q_k}$.  To see this, observe that for the third term the expression in parentheses is
stochastic, but does not depend on time $t$, so that the resulting series is predictable.


 It is true that $\delta^{(k)} (B)$
 always divides $p^{(j)} (B)$, and hence the stochastic portion of
 the third term is well-defined.  We cannot prove that the
 coefficients of the $\phi_j (t)$ for $1 \leq j \leq q_k$ must be
 zero, as counter-examples are easy to construct; consider two series that
 have a common stochastic trend with null vector $\beta^{\prime} =
 [1, \, 1]$, but whose underlying linear deterministic trends have
different slopes.  
%(Such a bivariate series might not be considered
% co-integrated, because $\beta^{\prime} x_t$ equals a stationary
% process plus a linear drift term.)
  In our analysis henceforth, we
 will assume that this third term is identically zero.


\section{Error Criterion and Computation}
\label{sec:mdfa-nonstat}
