

<<loading_code,echo=False>>=
path.main <- getwd()
#load_code(paste(path.main,"/Sweave/Sigex",sep=""))
setwd(paste(path.main,"/Sweave/RcodeTSM",sep=""))
# requires packages expm and xtable
library(devtools)
library(expm)
library(xtable)
source("sqrtm.r")
source("var1.psi2par.r")
source("var1.par2psi.r")
source("lpp.var1.r")
source("mdfa.dft.r")
source("mdfa.pergram.r")
source("mdfa.frf.r")
source("mdfa.coeff.r")
source("mdfa.filter.r")
source("mdfa.getconstraints.r")
source("mdfa.unconstrained.r")
source("mdfa.levelconstraint.r")
source("mdfa.tsconstraint.r")
source("mdfa.ltsconstraint.r")
source("sigex.sim.r")
source("sigex.graph.r")
setwd(path.main)
@

\chapter{Multivariate Direct Filter Analysis for Non-stationary Processes}
\label{chap:int}

 We now extend the basic MDFA of Chapter \ref{chap:basic}  by considering
 the method's application to  non-stationary processes.  
 Section \ref{sec:constraint} introduces the idea of filter constraints
arising from time-varying means, a form of non-stationarity.
 This treatment is generalized in Section \ref{sec:non-stat}
  by the definition of non-stationary processes, and theory for the corresponding
   model-based filters is developed.  Finally, the MDFA criterion for
    non-stationary processes is discussed in Section \ref{sec:mdfa-nonstat}.
 

\section{Constrained MDFA}
\label{sec:constraint}

 Various constraints upon the concurrent filter can be envisioned, 
   and imposing such strictures results in  a constrained MDFA. 
   A chief case of interest arises when the 
    data process has a time-varying mean (which is a form of  non-stationarity);
  then it is necessary to impose additional filter constraints -- otherwise
   the filter error will not have mean zero.    To see why, 
   Write $\Delta (L) = \Psi (L) - \widehat{\Psi} (L)$ as the discrepancy filter,
   so that we see  from (\ref{eq:dfa-error})  
   that $\EE [ E_t ] = \Delta (L) \, \EE [ X_t ]$; 
   by Definition \ref{def:lpp}, we require
 that $\EE [ E_t ] = 0$ for any LPP.  
  If $\EE [ X_t] = 0$ then this condition is always satisfied, but
   for most time series of interest the mean will be nonzero, and is typically
    time-varying.  For such cases additional constraints on $\Delta (L)$ must be imposed,
    which implicitly amount to constraints on $\widehat{\Psi} (L)$.
    
\begin{Example}    {\bf Constant Mean.}  \rm
\label{exam:constant.mean}
  If $\EE [ X_t ] = \mu$, some nonzero constant,  then we require $\Delta (1) = 0$.
  This is because the mean of the filter error is
  \[
   \Delta (B) \, \EE [ X_t] = \Delta(B) \, \mu = \sum_j \delta (j) \, \mu =
   \Delta (1) \, \mu,
  \]
  and this is zero only if $\Delta (1) = 0$.  This is called a Level Constraint (LC).
\end{Example}  

\begin{Example}    {\bf Linear Mean.}  \rm
\label{exam:linear.mean}
  Suppose that $\EE [ X_t ] = \mu \, t$, where $\mu$ is a nonzero slope
 of a linear time trend.  Then it is required that  $\partial {\Delta} (1) = 0$
  in addition to the LC,  which is seen as follows:
  \[
   \Delta (B) \, \EE [ X_t] = \Delta(B) \, \mu \, t =   \mu \, \sum_j \delta (j) \, (t-j)
   = \mu \, \left(t \, \sum_j \delta (j) - \sum_j j \,\delta (j) \right)
    = \mu \, t \, \Delta(1) - \mu \, \partial \Delta (1).
  \]
  This mean of the filter error  is zero only if both $\Delta(1)=0$ and
  $\partial \Delta (1)=0$; the latter condition is called the
   Time-Shift Constraint (TSC).  
\end{Example}  

     Hence, for linear means we obtain
 three fundamental types of constraints: LC, TSC, and Level plus 
 Time-Shift Constraint (LTSC), which combines both LC and TSC.
  Using the fact that $\Delta (L) = \Psi (L) - \widehat{\Psi} (L)$,
   these three constaints can be described as follows:
\begin{align*}
 \mbox{LC} : &  \;  \Delta (1) = 0 \quad \mbox{or} \quad \Psi (1) = \widehat{\Psi} (1) \\
 \mbox{TSC} : &  \;   \partial {\Delta} (1) = 0 \quad \mbox{or} \quad 
 \partial {\Psi} (1) = \partial {\widehat{\Psi}} (1)  \\
 \mbox{LTSC} : &  \;  \Delta (1) = 0,  \,  \partial {\Delta} (1) = 0 \quad 
 \mbox{or} \quad \Psi (1) = \widehat{\Psi} (1), \; \partial {\Psi} (1) =
 \partial {\widehat{\Psi}} (1).
\end{align*}
 In the case of  concurrent filters of form  (\ref{eq:conc.filter}), 
 LC is accomplished by demanding that 
  $\sum_{j=0}^{q-1} \widehat{\psi} (j) = \Psi(1)$.   More generally, we consider  linear constraints  formulated via
\begin{equation}
\label{eq:concurrent-constrain}
  \vartheta = R \, \varphi + Q,
\end{equation}
 where $R$ is $n q \times n r$ and $\varphi$ is $n r \times 1$ dimensional, consisting of 
 free parameters; $Q$ is a matrix of constants, and is $n q \times 1$ dimensional.


\begin{Illustration}  {\bf Level Constraint (LC).}   \rm
\label{ill:lc}
 Note that $\sum_{j=0}^{q-1} \widehat{\psi} (j) = \Psi(1)$ implies that
\begin{equation}
\label{eq:lc-gamma0}
 \widehat{\psi} (0) = \Psi(1) - \sum_{j=1}^{q-1} \widehat{\psi} (j).
\end{equation}
 Hence  $ \varphi^{\prime}  = [ \widehat{\psi} (1), \widehat{\psi} (2), \ldots, \widehat{\psi} (q-1) ] $ and
\[
	R  = \left[ \begin{array}{ccc} -1 & \ldots & -1 \\ 1 & 0 & 0 \\
		\vdots & \ddots & \vdots \\ 0 & 0 & 1  \end{array} \right]  \otimes 1_n \qquad
	Q = \left[ \begin{array}{c} \Psi (1) \\ 0 \\ \vdots \\ 0 \end{array} \right].
\]
\end{Illustration}
 
 
\begin{Illustration}  {\bf Time Shift Constraint (TSC).}   \rm
\label{ill:tsc}
   The constraint is $\partial {\Psi} (1) = \partial \widehat{\Psi} (1)
   = \sum_{j=0}^{q-1} j \, \widehat{\psi} (j)$,
 or $\widehat{\psi} (1)  = \partial {\Psi} (1)  -  \sum_{j=2}^{q-1} j \, \widehat{\psi} (j) $.
 Hence  $ \varphi^{\prime}  = [ \widehat{\psi} (0), \widehat{\psi} (2), \ldots, \widehat{\psi} (q-1) ] $ and
\[
	R  = \left[ \begin{array}{cccc} 1 & 0 &  \ldots &  0  \\  0 & -2  &  -3  & \ldots  \\
		0 & 1 & 0 & \ldots \\ 
		\vdots & \ddots & \vdots & \vdots \\ 0 & \ldots & 0 & 1 \end{array} \right] \otimes 1_n \qquad
	Q = \left[ \begin{array}{c} 0 \\ \partial {\Psi} (1) \\ 0 \\ \vdots \\ 0 \end{array} \right].
\]
\end{Illustration}
 
 
\begin{Illustration}  {\bf Level and Time Shift Constraint (LTSC).}  \rm
\label{ill:ltsc}
   Take the Time Shift Constraint formula for $\widehat{\psi} (1)$,
 and plug this into (\ref{eq:lc-gamma0}), to obtain
\begin{align*}
 \widehat{\psi} (0)  & = \Psi (1) - \left( \partial {\Psi} (1)  -  \sum_{j=2}^{q-1} j  \, \widehat{\psi} (j) \right) -  \sum_{j=2}^{q-1} 
 \widehat{\psi} (j)  \\
	& = \Psi (1) -  \partial {\Psi} (1)  +  \sum_{j=2}^{q-1} (j-1)  \, \widehat{\psi} (j).
\end{align*}
 Hence  $ \varphi^{\prime}  = [  \widehat{\psi} (2), \ldots, \widehat{\psi} (q-1)  ] $ and
\[
	R  = \left[ \begin{array}{cccc} 1 & 2  &  3  &   \ldots    \\  -2  & -3  &  -4  & \ldots  \\
		 1  & 0 & \ldots & 0 \\ 
		\vdots & \ddots & \vdots & \vdots \\ 0 & \ldots & 0 & 1 \end{array} \right]  \otimes 1_n \qquad
	Q = \left[ \begin{array}{c} \Psi (1) - \partial {\Psi} (1)  \\  \partial {\Psi} (1) \\ 0 \\ \vdots \\ 0 \end{array} \right].
\]
\end{Illustration}



 More generally, we can envision an LPP involving $m$ linear constraints on 
  $\vartheta$, taking the form
 $   A = [ J \otimes 1_n ] \, \vartheta$, where $J$ is $m \times q$ 
 dimensional ($m < q$) and $A$ is $n m \times 1$ dimensional.
 (The LC, TSC, and LTSC examples all have this form.)  In order to express 
 this constraint in the form 
 (\ref{eq:concurrent-constrain}), we use the Q-R decomposition 
 (Golub and Van Loan, 1996) of $J$, writing
 $J = C \, G \, \Pi$ for an orthogonal matrix $C$ (which is $m \times m$ dimensional),
 a rectangular upper triangular matrix $G$
 (which is $m \times q$ dimensional), and a permuation matrix $\Pi$ 
 (which is $q \times q$ dimensional).  
 Standard matrix software such as $\textsc{R}$ will provide the Q-R decomposition $J$,
 and should produce the rank of $J$ as  a by-product --
 if this is less than $m$, then there are redundancies in the 
 constraints that should first be eliminated. 
 
 \begin{Exercise} {\bf QR Decomposition.} \rm
 \label{exer:qr.constraint}
 HERE
 \end{Exercise}
 
 Hence  proceeding with a full rank $J$, we partition $G$ as $G = [ G_1 \, G_2]$ 
 such that $G_1$ has $m$ columns and $G_2$
 has $q-m$ columns.  This quantity $q-m$ corresponds to the number 
 of free coefficient matrices, and is therefore the same as $r$.
 The Q-R decomposition guarantees that $G_1$ is an upper triangular matrix, 
 and moreover it is invertible.  Therefore
\[
  \left[ G_1^{-1} \, C^{-1} \otimes 1_n \right] \, A  = 
  \left( \left[ 1_m , \, G_1^{-1} \, G_2 \right] \, \Pi \otimes 1_n  \right) \, \vartheta,
\]
 and the action of $\Pi$ (together with the tensor product) amounts 
 to a   permutation of the elements of $\vartheta$.
  Let the output of this permutation be denoted
\[
   \left[ \begin{array}{l} \overline{\vartheta} \\ \underline{\vartheta} \end{array} \right]
   = \left( \Pi \otimes 1_n \right) \, \vartheta,
\]
 where $\overline{\vartheta}$ is $n m \times 1$ dimensional and
 $\underline{\vartheta}$ is $n r \times 1$ dimensional.  
 Then  by substitution we can solve for $\overline{\vartheta}$ in terms 
 of $\underline{\vartheta}$:
\[
   \overline{\vartheta} =  \left[ G_1^{-1} \, C^{-1} \otimes 1_n \right] \, A - 
   \left[  G_1^{-1} \, G_2  \otimes 1_n   \right] \, \underline{\vartheta}.
\]
 Therefore we recognize the free variables $\varphi = \underline{\vartheta}$, 
 and obtain $R$ and $Q$ in (\ref{eq:concurrent-constrain}) via
\begin{align*}
   R & = \Pi^{-1} \, \left[ \begin{array}{c} - G_1^{-1} \, G_2 \\ 
   1_{r} \end{array} \right] \otimes 1_n  \\
  Q & = \left( \Pi^{-1}  \, \left[ \begin{array}{c}  G_1^{-1} \, C^{-1} \\ 0 \end{array} \right] \otimes 1_n  \right) \, A.
\end{align*}
  These formulas allow one to compute the   form (\ref{eq:concurrent-constrain}) 
   from given constraints, and
 an analytical solution to the resulting MDFA criterion  be obtained from the following result.

\begin{Proposition}
\label{prop:mdfa.quadsoln-constrain}
 The minimizer of the  MDFA criterion given by   (\ref{eq:mdfa-criterion})
 with respect to  $\mathcal{G}$ -- consisting of all length $q$ concurrent filters 
 subject to  linear constraints of the form (\ref{eq:concurrent-constrain}) -- is
\begin{equation}
\label{eq:phi.soln-constrained}
 \varphi =  { \left[ R^{\prime} \, B \, R \right] }^{-1} \, R^{\prime} \,
 \left( b - B \, Q \right).
\end{equation}
 The minimal value is  
\begin{equation}
\label{eq:opt.val.mdfa-constrained}
{ \langle \Psi (z) \, G \, { \Psi (z) }^* \rangle }_0 
  + {(b - B \, Q)}^{\prime} \, \left( 
  R \, { \left[ R^{\prime} \, B \, R \right] }^{-1} \, R^{\prime} 
  + B^{-1} \right) \, ( b - B \, Q) -   b^{\prime} \, B^{-1} \, b.
\end{equation}
\end{Proposition}

\paragraph{Proof of Proposition \ref{prop:mdfa.quadsoln-constrain}.}
 Substituting (\ref{eq:concurrent-constrain}) in (\ref{eq:mdfa-crit.linear}) yields
\begin{align*}
  D_{\Psi} (\vartheta, G) &  = \varphi^{\prime} \,  \left[ R^{\prime} \, B \, R \right] \,  \varphi 
  + \left[ Q^{\prime} \, B \, R - b^{\prime} \, R \right] \, \varphi + \varphi^{\prime} \,
   \left[ R^{\prime} \, B \, Q - R^{\prime} \, b \right]  \\
 & + Q^{\prime} \, B \, Q  - Q^{\prime} \, b - b^{\prime} \, Q  + { \langle \Psi (z) \, G \, { \Psi (z) }^* \rangle }_0.
\end{align*}
  Now by applying the method of proof in Proposition \ref{prop:mdfa.quadsoln}, we obtain 
  the formula (\ref{eq:phi.soln-constrained}) for $\varphi$.  Plugging back into $D_{\Psi} (\vartheta, G)$
 yields the minimal value (\ref{eq:opt.val.mdfa-constrained}).  $\quad \Box$


\vspace{.5cm}

For computation, we utilize the same approximations to $B$ and $b$ as discussed 
in  Chapter \ref{chap:basic},
 obtaining the constrained MDFA filter $\vartheta$ via (\ref{eq:phi.soln-constained})
 followed by (\ref{eq:concurrent-constrain}).

\begin{Exercise} {\bf  Constrained MDFA for White Noise with Linear Trend.} \rm
\label{exer:wntrend-mdfa}
This exercise applies the constrained MDFA in the case of an ideal low-pass filter
 (cf. Example \ref{exam:ideal-low}) 
 applied to a white noise process that exhibits a linear trend.
 Simulate a sample of size $T=2500$ from a
  bivariate white noise process with 
  $\Sigma$ equal to the identity, but with a linear trend  given by
  \[
   \left[ \begin{array}{c} 1 \\ 2 \end{array} \right] + t \, 
   \left[ \begin{array}{c} -.002 \\ .001 \end{array} \right].
  \]
   Apply the   ideal low-pass filter (cf. Example \ref{exam:ideal-low}) with 
  $\mu = \pi/6$ to the sample (truncate the filter to $1000$ coefficients on each side).  
 Use the moving average filter  MDFA  with LC, TSC, and LTSC constraints
  (Proposition \ref{prop:mdfa.quadsoln-constrain}),
  as well as unconstrained MDFA  (Proposition \ref{prop:mdfa.quadsoln}), to find the best
 concurrent filter, setting $q= 30$. 
  (Hint: compute the periodogram from OLS residuals obtained by regressing the simulation
   on a constant plus time.)
 Apply this concurrent filter 
 to the simulation, and compare the relevant portions to the ideal trend.
 Also determine the in-sample performance, in comparison to the criterion value
 (\ref{eq:opt.val.mdfa}).   Target the trends for both time series.
\end{Exercise}

<<exercise_wntrend-mdfa,echo=True>>=
# Simulate a Gaussian WN of sample size 2500:
T <- 2500
N <- 2
levels <- c(1,2)
slopes <- c(-2,1)/1000
innovar.matrix <- diag(N)
x.sim <- NULL
for(t in 1:T)
{
	x.next <- levels + slopes*t + t(chol(innovar.matrix)) %*% rnorm(N)
	x.sim <- cbind(x.sim,x.next)
}
x.sim <- ts(t(x.sim))
time.trend <- seq(1,T)
sim.ols <- lm(x.sim ~ time.trend)
x.resid <- sim.ols$residuals

# construct and apply low pass filter
mu <- pi/6
len <- 1000
lp.filter <- c(mu/pi,sin(seq(1,len)*mu)/(pi*seq(1,len)))
lp.filter <- c(rev(lp.filter),lp.filter[-1])
x.trend.ideal <- filter(x.sim,lp.filter,method="convolution",sides=2)[(len+1):(T-len),]

# get MDFA concurrent filter
q <- 30
Grid <- T
m <- floor(Grid/2)
# The Fourier frequencies
freq.ft <- 2*pi*Grid^{-1}*(seq(1,Grid) - (m+1))

# frf for ideal low-pass
frf.psi <- rep(0,Grid)
frf.psi[abs(freq.ft) <= mu] <- 1
frf.psi <- matrix(frf.psi,nrow=1) %x% diag(N) 	  
frf.psi <- array(frf.psi,c(N,N,Grid))
spec.hat <- mdfa.pergram(x.resid,1)	
lp.mdfa.uc <- mdfa.unconstrained(frf.psi,spec.hat,q)
lp.mdfa.lc <- mdfa.levelconstraint(frf.psi,spec.hat,q)
lp.mdfa.tsc <- mdfa.tsconstraint(frf.psi,spec.hat,q)
lp.mdfa.ltsc <- mdfa.ltsconstraint(frf.psi,spec.hat,q)

# case 1: apply the unconstrained MDFA concurrent filter 
x.trend.mdfa11 <- filter(x.sim[,1],lp.mdfa.uc[[1]][1,1,],method="convolution",sides=1)
x.trend.mdfa12 <- filter(x.sim[,2],lp.mdfa.uc[[1]][1,2,],method="convolution",sides=1)
x.trend.mdfa21 <- filter(x.sim[,1],lp.mdfa.uc[[1]][2,1,],method="convolution",sides=1)
x.trend.mdfa22 <- filter(x.sim[,2],lp.mdfa.uc[[1]][2,2,],method="convolution",sides=1)
x.trend.mdfa <- cbind(x.trend.mdfa11 + x.trend.mdfa12,x.trend.mdfa21 + x.trend.mdfa22)
x.trend.mdfa <- x.trend.mdfa[(len+1):(T-len),] 
 
# compare in-sample performance
print(c(mean((x.trend.ideal[,1] - x.trend.mdfa[,1])^2),
	mean((x.trend.ideal[,2] - x.trend.mdfa[,2])^2)))

# compare to criterion value
diag(lp.mdfa.uc[[2]])

# case 2: apply the lc MDFA concurrent filter 
x.trend.mdfa11 <- filter(x.sim[,1],lp.mdfa.lc[[1]][1,1,],method="convolution",sides=1)
x.trend.mdfa12 <- filter(x.sim[,2],lp.mdfa.lc[[1]][1,2,],method="convolution",sides=1)
x.trend.mdfa21 <- filter(x.sim[,1],lp.mdfa.lc[[1]][2,1,],method="convolution",sides=1)
x.trend.mdfa22 <- filter(x.sim[,2],lp.mdfa.lc[[1]][2,2,],method="convolution",sides=1)
x.trend.mdfa <- cbind(x.trend.mdfa11 + x.trend.mdfa12,x.trend.mdfa21 + x.trend.mdfa22)
x.trend.mdfa <- x.trend.mdfa[(len+1):(T-len),] 
 
# compare in-sample performance
print(c(mean((x.trend.ideal[,1] - x.trend.mdfa[,1])^2),
	mean((x.trend.ideal[,2] - x.trend.mdfa[,2])^2)))

# compare to criterion value
diag(lp.mdfa.lc[[2]])

# case 3: apply the tsc MDFA concurrent filter 
x.trend.mdfa11 <- filter(x.sim[,1],lp.mdfa.tsc[[1]][1,1,],method="convolution",sides=1)
x.trend.mdfa12 <- filter(x.sim[,2],lp.mdfa.tsc[[1]][1,2,],method="convolution",sides=1)
x.trend.mdfa21 <- filter(x.sim[,1],lp.mdfa.tsc[[1]][2,1,],method="convolution",sides=1)
x.trend.mdfa22 <- filter(x.sim[,2],lp.mdfa.tsc[[1]][2,2,],method="convolution",sides=1)
x.trend.mdfa <- cbind(x.trend.mdfa11 + x.trend.mdfa12,x.trend.mdfa21 + x.trend.mdfa22)
x.trend.mdfa <- x.trend.mdfa[(len+1):(T-len),] 
 
# compare in-sample performance
print(c(mean((x.trend.ideal[,1] - x.trend.mdfa[,1])^2),
	mean((x.trend.ideal[,2] - x.trend.mdfa[,2])^2)))

# compare to criterion value
diag(lp.mdfa.tsc[[2]])

# case 4: apply the ltsc MDFA concurrent filter 
x.trend.mdfa11 <- filter(x.sim[,1],lp.mdfa.ltsc[[1]][1,1,],method="convolution",sides=1)
x.trend.mdfa12 <- filter(x.sim[,2],lp.mdfa.ltsc[[1]][1,2,],method="convolution",sides=1)
x.trend.mdfa21 <- filter(x.sim[,1],lp.mdfa.ltsc[[1]][2,1,],method="convolution",sides=1)
x.trend.mdfa22 <- filter(x.sim[,2],lp.mdfa.ltsc[[1]][2,2,],method="convolution",sides=1)
x.trend.mdfa <- cbind(x.trend.mdfa11 + x.trend.mdfa12,x.trend.mdfa21 + x.trend.mdfa22)
x.trend.mdfa <- x.trend.mdfa[(len+1):(T-len),] 
 
# compare in-sample performance
print(c(mean((x.trend.ideal[,1] - x.trend.mdfa[,1])^2),
	mean((x.trend.ideal[,2] - x.trend.mdfa[,2])^2)))

# compare to criterion value
diag(lp.mdfa.ltsc[[2]])
@
 
 




HERE  add exercise on ideal low-pass with various constraints 
  repeat with VAR(1) with linear trend

% 
% We suppose the true process is a VAR($1$), and apply the MB trend filter defined in Example \ref{exam:trend-i1},
%   where the parameters are given by
% \[
%  \Sigma_{\mu} = \left[ \begin{array}{ll} 
%    2.32 \cdot 10^{-4} &  5.04 \cdot 10^{-4} \\
%    5.04 \cdot 10^{-4}  & 34.73 \cdot 10^{-4}  \end{array}  \right]
%  \qquad  \Sigma_{\iota} = \left[ \begin{array}{ll}
%         110.44 \cdot 10^{-5} &  7.17 \cdot 10^{-5}  \\
%         7.17 \cdot 10^{-5} & 128.57 \cdot 10^{-5}   \end{array} \right].
% \]
%   Because the trend variance for the second component is $15$ times larger than that of the first component,
%  the correspoding trend filter  does less smoothing.  
%  
% \begin{figure}[htb!]
% \centering
% \includegraphics[]{petrolVAR1trends}
% \caption{\baselineskip=10pt Bivariate  trend  filter applied to VAR($1$) simulation (grey), with trends in black. }
% \label{fig:petrol.var1trends}
% \end{figure}
% 
% 
%   We seek to solve the corresponding trend extraction LPP.    First, we can use the optimal solution  (\ref{eq:var1.lpp-opt})
% given in   Illustration \ref{ill:var1}, supposing that we know  that the VAR($1$) is correctly specified.  
%  Second, we can use MDFA, proceeding as if we do not know the true process is a VAR($1$), as we would in practice, 
% and hence use the periodogram;  MDFA should be able to replicate the optimal solution, so long as the filter class
%  $\mathcal{G}$ is sufficiently rich.    The VAR($1$)  is defined by
% \[
%   X_t =  \left[ \begin{array}{ll}  1.0  & 0.5 \\    -0.2  &  0.3  \end{array} \right] \, X_{t-1} + \epsilon_t,
% \]
%  with stationary initialization, and $\{ \epsilon_t \}$ a Gaussian white noise of identity innovation variance.
%   Operationally, we simulate this process with sample size $4500$.  Then the 
%   ideal trends $\Psi (B) X_t$ are produced by truncating the  MB  filter to length $4001$ (it is symmetric, so the indices
%  range between $-2000$ and $2000$) and applying to the  simulation, only retaining the central $500$ data points,
%  as displayed in Figure \ref{fig:petrol.var1trends}.   (In this way we can dispense with edge effects, and the extra
%  $4000$ observations are not used in the MDFA.)     The grey lines of Figure \ref{fig:petrol.var1trends}
%  are the central $500$ observations of the VAR($1$)
%  simulation, and the  black  line is the target.     We wish to use MDFA (setting $q=30$) 
%  with various constraints (LC, TSC, LTSC) 
% to obtain a   real-time estimate,  comparing  the result  to the optimal solution given by implementing  Illustration \ref{ill:var1}.  
%   In that case  we find that
% \[
%   A_{\Psi} (\Phi) = \left[ \begin{array}{ll}  0.317  &  0.218 \\    -0.054  &  0.027  \end{array} \right]
% \]
%  by direct calculation, 
% and hence the optimal filter is easily computed.  The in-sample MSEs of the various methods are displayed in Table \ref{tab:petrol.var1.mdfa}.
%   Note that the basic MDFA (no constraints) replicates the optimal filter, as their MSE is the same up to negligible   error.
%   When imposing a level constraint (LC and LTSC) there is a loss to the MDFA performance, which makes sense given that the optimal
%  filter does {\it not} impose a level constraint -- in fact, the value of the optimal concurrent filter at frequency zero is
% \[
%   \widehat{\Psi} (1) =  \left[ \begin{array}{ll}  0.914 &  0.251 \\    -0.030  &  0.842  \end{array} \right],
% \]
%  which is quite different from $1_2$.  On the other hand, the time shift constraint alone (TSC) has little impact on the performance of MDFA,
%  because $\partial \widehat{\Psi} (1) \approx 0 \cdot 1_2$, i.e., the optimal filter already has this property of zero time shift.
% 
%  
% 
% \begin{table}[!htb]
% \centering
% \begin{tabular}{clllll}
% \hline
%   Series  &  LPP Opt  &  MDFA Basic &  MDFA LC &  MDFA TSC   &  MDFA LTSC \\
%   1 & .2404  &  .2388  &  .2877  &   .2363  & .4367  \\
%   2 & .0224 &   .0217 &   .0229  &   .0216  & .0264 \\
% \hline
% \end{tabular}
% \caption{\baselineskip=10pt  LPP MSE for bivariate VAR($1$) process --  with target trend
%  given by the LLM MB trend -- for various concurrent filters: LPP Opt is the optimal filter,
%  whereas the MDFA filters are labeled according to the constraints imposed. }
% \label{tab:petrol.var1.mdfa}
% \end{table}
%  




\section{Background on Non-stationary Vector Time Series }
\label{sec:non-stat}

We next consider processes that when differenced are 
stationary, which are the most common type occuring in econometrics and finance.  
 This type of non-stationary process substantially broadens
  the possible types of applications over the stationary processes
   considered in Chapters \ref{chap:lpp} and \ref{chap:basic}.
  Also, as such processes typically can have a time-varying mean,
  they also necessitate the use of filter constraints such as those
   considered in Section \ref{sec:constraint}.
  
 We suppose that there exists a polynomial $\delta (L)$
  that reduces each component series of $\{ X_t \}$ to a stationary
   time series (which is allowed to have a non-zero constant mean),
   and suppose this is the minimal degree polynomial that accomplishes
    this reduction.  We write $\partial X_t = \delta (L) \, X_t$ for
  the stationary, differenced time series.
    
HERE :  do examples with $1-L$  and $1-L^2$, derive time-varying
  spectral rep (from old notes on evol spectra, and DFA paper).
  
  
  HERE : state the following as a proposition
    
So we can write
\[
 X_t = \sum_{j=1}^d A_{j, t+d} \, X_{j-d} + \int_{-\pi}^{\pi}
 \frac{ e^{i \omega t} - \sum_{j=1}^d A_{j,t+d} \, e^{-i \omega
 (d-j)} }{ \delta (e^{-i \omega}) } \; d \ZZ (\omega),
\]
 where $d = \sum_{\ell=1}^m q_{\ell}$ and each $A_{j, t+d}$ is a
 time varying function for each $j$, and the $x_{j-d}$ are initial
 values.  This representation is chiefly useful when $t > 1$, though
 it is still valid when $t \leq 0$.  The $\ZZ (\omega)$ is the
 orthogonal increments process in the spectral representation of
 $\partial X_t$. 

 Each of the time-varying functions is in the null
 space of $\delta (B)$, i.e., $\delta (B) A_{j, t+d} = 0$ for $1 \leq
 j \leq d$, where the backshift operator works on the $t+d$ index.
 As a consequence, we can rewrite each $A_{j,t+d}$ as a linear
 combination of the basis functions of the null space of $\delta
 (B)$, which yields a more convenient representation.  Let the basis
 functions be $\phi_j (t) $ for $1 \leq j \leq d$; the existence and
 form of such functions are a basic staple of difference equation
 theory, treated briefly in Brockwell and Davis (1991).  Then we can
 write $A_{j,t+d} = \sum_{k=1}^d G_{jk} \phi_k (t)$ for each $1 \leq
 j \leq d$, for some coefficients $G_{jk}$.  It follows that
\[
 \sum_{j=1}^d A_{j,t+d} B^{d-j} = \sum_{k=1}^d \phi_k (t)  \; \left(
 \sum_{j=1}^d G_{jk} B^{d-j} \right).
\]
 Each expression in parentheses on the right hand side is a degree
 $d-1$ polynomial in $B$, and will henceforth be denoted as $p^{(k)}
 (B)$.   Substituting the new formulation, we obtain
\[
 x_t = \sum_{j=1}^d \phi_j (t) p^{(j)} (B) \, x_{0} + \int_{-\pi}^{\pi}
 \frac{ e^{i \omega t} - \sum_{j=1}^d \phi_j (t) \, p^{(j)} ( e^{-i \omega
 } )}{ \delta (e^{-i \omega}) } \; d \ZZ (\omega),
\]
 where $p^{(j)} (B)$ acts on $x_0$ by shifting the time index $t=0$
 back in time for each power of $B$.  This representation is now
 extremely convenient, because application of any factor of
 $\delta(B)$ will annihilate a corresponding basis function (when
 roots are repeated, some basis functions will also be transformed
 into others that are instead annihilated). 

   
  HERE material on WK : signal and noise spectra non-stat case, and basic formulas
    in freq domain
    
  HERE illustrations by LLM, STM, and Example 5 of E and S paper
  
  

\section{Error Criterion and Computation}
\label{sec:mdfa-nonstat}

HERE material from E and S paper section 4.3

% We here consider difference-stationary vector time series, which means there exists a scalar differencing polynomial $\delta (L)$ such
%  that $\partial X_t = \delta (L) X_t$ is mean zero and covariance stationary.  
%  Examination of (\ref{eq:dfa-error}) indicates that the error process is not stationary unless we make certain assumptions
%  about $\Delta (L) = \Psi (L) - \widehat{\Psi} (L)$.     It is necessary that we can factor $\delta (L)$ from $\Delta (L)$, i.e., there exists
%  $\widetilde{\Delta } (L)$ such that
% \begin{equation}
%  \label{eq:delta.factor}
%   \Delta (L) = \widetilde{\Delta } (L) \, \delta (L),
% \end{equation}
%  as otherwise we cannot guarantee that $\{ E_t \}$ will be stationary.  However, (\ref{eq:delta.factor}) is sufficient to guarantee
%  that the filter error be stationary, because
% \[
%   E_t = \widetilde{\Delta} (L) \, \partial X_t
% \]
%  in such a case.   We next discuss a set of filter constraints that guarantee (\ref{eq:delta.factor}), beginning with a lemma
%  that discusses factorization of filters.  We say a filter $\Psi (L)$ is absolutely convergent if $\sum_{j \in \ZZ} \| \psi (j) \| < \infty$
%  for a given matrix norm $\| \cdot \|$.
% 
% \begin{Proposition}
% \label{prop:filter-decompose}
%  Any linear filter $\Psi (L)$ can be expressed as
% \[
%   \Psi (L) = \Psi (\zeta) + (L - \zeta) \, \Psi^{\sharp} (L)
% \]
%  for any $\zeta \in \CC$  such that $| \zeta | = 1$, 
%   and an absolutely convergent filter $\Psi^{\sharp} (L)$, so long as  $\partial \Psi (L) $ is absolutely convergent.
%  If in addition $ \partial \partial \Psi (L) = \sum_{ j \in \ZZ} j (j-1) \, \psi (j) \, L^j$
%    is absolutely convergent, then there also exists an absolutely convergent filter $\Psi^{\flat} (L)$ 
%  such that
% \[
%  \Psi (L) = \Psi (\zeta) + \partial \Psi (\zeta) \, (L- \zeta) \, \overline{\zeta} + {(L - \zeta)}^2 \, \Psi^{\flat} (L).
% \]
% \end{Proposition}
% 
%  Note that if $\Psi (\zeta) = 0$, it follows from Proposition \ref{prop:filter-decompose} that $L-\zeta$ can be factored from
%   $\Psi (L)$.  Similarly, ${(L- \zeta)}^2$ can be factored from $\Psi (L)$ is $\Psi(\zeta) = \partial \Psi (\zeta) =0$.
% 
% \begin{Definition} \rm
% \label{def:filter-noise}
%  For $\omega \in [-\pi, \pi]$, a filter $\Psi (L)$ annihilates $\omega$-noise of order $1$ if $\Psi (e^{-i \omega}) = 0$,
%  and annihilates $\omega$-noise of order $2$ if in addition $\partial \Psi (e^{-i \omega}) = 0$.
% \end{Definition}
% 
% 
% Hence, we have the following immediate corollary of Proposition \ref{prop:filter-decompose}.
% 
% \begin{Corollary}
%  \label{cor:filter-noise}
%   If a filter $\Psi (L)$ annihilates $\omega$-noise of order $1$ and $\partial \Psi (L)$ is absolutely convergent, then
% \[
%   \Psi (L) = (L- e^{-i \omega}) \, \Psi^{\sharp} (L).
% \]
%  If a filter $\Psi (L)$ annihilate $\omega$-noise of order $2$,  and $\partial \partial \Psi (L)$ is absolutely convergent, then
% \[
%   \Psi (L) = {(L- e^{-i \omega}) }^2 \, \Psi^{\flat} (L).
% \]
% \end{Corollary}
% 
%  We can apply Corollary \ref{cor:filter-noise} to factor a noise-differencing polynomial $\delta^N (L)$ from $\Delta (L)$:
%  for each $\omega$ such that the target filter $\Psi (L)$ annihilate $\omega$-noise of order $d$, we impose the constraint
%  that $\widehat{\Psi} (L)$ shall have the same property, and hence ${(L- e^{-i \omega})}^d$ can be factored from both
%  filters.   For instance, if noise frequencies are $\omega_{\ell}$ with multiplicities $d_{\ell}$, then repeated application of 
%  Corollary \ref{cor:filter-noise} yields
% \[
%  \Psi (L) = \prod_{\ell} {(L -  e^{-i \omega_{\ell}})}^{d_{\ell}} \, \Psi^{\natural} (L)
%    = \delta^N (L) \, \Psi^{\star} (L)
% \]
%  for some residual filter $\Psi^{\natural} (L)$, where $\Psi^{\star} (L) = \prod_{\ell} -e^{-i \omega_{\ell} d_{\ell}} \, \Psi^{\natural} (L)$
%  and $\delta^N (L) = \prod_{\ell} (1 - e^{i \omega_{\ell}} \, L)$.
%  By imposing the same linear constraints on $\widehat{\Psi} (L)$, we likewise obtain $\widehat{\Psi} (L) = \delta^N (L) \, \widehat{\Psi}^{\star} (L)$,
%  and hence 
% \begin{equation}
%  \label{eq:delta-noise}
% \Delta (L) = \left(  {\Psi}^{\star} (L) - \widehat{\Psi}^{\star} (L) \right) \, \delta^N (L).
% \end{equation}
%   So if $\delta (L) = \delta^N (L)$, then (\ref{eq:delta.factor}) holds at once.  More generally, a given process' differencing polynomial
%  may be factored into relatively prime polynomials $\delta^N (z)$ and $\delta^S (z)$, which correspond to noise and signal dynamics
%  respectively -- see Bell (1984) and McElroy (2008a).  Many  signal extraction filters $\Psi (L)$   have the property that they
%  annihilate $\omega$-noise of the appropriate order, such that $\delta^N (L)$ can be factored; in addition, the noise filter $1_N - \Psi (L)$
%  has the same property with respect to the signal frequencies, i.e., $\delta^S (L)$ can be factored from $1_N - \Psi (L)$ in the same manner.
%  Hence  $1_N -  \Psi (L) =   \delta^S (L) \, \Psi^{\diamond} (L)$ for some factor $\Psi^{\diamond} (L)$,
%  and imposing the same constraints on the concurrent filter yields
% \[
%   \Delta (L) = (1_N - \widehat{\Psi} (L)) - (1_N - \Psi (L)) = \left(  \widehat{\Psi}^{\diamond} (L) - \Psi^{\diamond} (L)  \right) \, \delta^S (L).
% \]
%   However, (\ref{eq:delta-noise}) also holds, and the roots of $\delta^S (z)$ and $\delta^N (z)$ are distinct (because the polynomials
%  are relatively prime by assumption), and hence $\delta (L) = \delta^N (L) \, \delta^S (L)$ must be a factor.  Therefore,
%  $\widetilde{\Delta} (L) =  (  \widehat{\Psi}^{\diamond} (L) - \Psi^{\diamond} (L)   )/ \delta^N (L)$, and (\ref{eq:delta.factor}) holds.
% 
% In summary, given a factorization of $\delta (z)$ into signal and noise differencing polynomials, the noise constraints and signal constraints
%  on $\Psi (L)$ must also be imposed upon $\widehat{\Psi} (L)$, and this ensures that $\{ E_t \}$ will be stationary with mean zero.  
%  If $\omega$ satisfies $\delta^N (e^{-i \omega}) = 0$, then we impose that $\widehat{\Psi} (L)$ annihilates $\omega$-noise of order
%  given by the multiplicity of the root in $\delta^N (z)$.  Otherwise, if $\omega$ satisfies $\delta^S (e^{-i \omega})$ then we impose
%  that $\widehat{\Psi} (e^{-i \omega}) = \Psi (e^{-i \omega})$ (if the root is simple -- if a double root, then also impose that
%  $\partial \widehat{\Psi} (e^{-i \omega}) = \partial \Psi (e^{-i \omega})$).  In practice, we must determine the real and imaginary  parts of each such constraint, and write the corresponding constraints on $\widehat{\Psi} (L)$ in the form $A = [J \otimes 1_N] \, \vartheta$ for
%   filters of form (\ref{eq:conc.filter}), applying the methodology of the previous subsection.  
%   With these constraints in play, the formula (\ref{eq:dfa-mvar}) holds with $\Psi (z) - \widehat{\Psi} (z)$ replaced by $\widetilde{\Delta} (z)$
%  and $F$ being the spectral density of $\{ \partial X_t \}$, i.e., we define the nonstationary MDFA criterion 
%  function as $\det D_{\Psi } (\vartheta, G)$ for
% \begin{equation}
% \label{eq:mdfa-criterion-nonstat}
%  D_{\Psi} (\vartheta, G) =     { \langle  \widetilde{\Delta} (z)   \,   G \,  {\widetilde{\Delta} (z) }^*   \rangle }_0
%  = { \langle  \left[ \Psi (z) -   \widehat{\Psi}_{\vartheta} (z) \right] \,   G \, {|\delta (z) |}^{-2} \,
%   {  \left[ \Psi (z) -  \widehat{\Psi}_{\vartheta} (z) \right] }^{*} \rangle }_0.
% \end{equation}
%   The second expression in (\ref{eq:mdfa-criterion-nonstat}) utilizes (\ref{eq:delta.factor}), and employs the understanding
%  that poles in ${\delta (z) }^{-1}$ are exactly canceled out by the corresponding zeros in $\Psi (z) - \widehat{\Psi} (z)$.
%   Moreover, the ratio $(\Psi (z) - \widehat{\Psi} (z))/\delta (z) = \widetilde{\Delta} (z)$ is bounded in $\omega$ for $z = e^{-i \omega}$,
%  as the previous discussion guarantees.  As a matter of convenience, given that the frequencies of singularity in
%  ${|\delta (z) |}^{-2}$ are a set of Lebesgue measure zero, calculation of $D_{\Psi} (\vartheta, G)$ can proceed by using
%  the second expression, computing the numerical integration over only those frequencies where $\delta (z)$ is nonzero.
%   Whereas the theoretical filter error MSE is given by $D_{\Psi, F}$, with $F$ being the spectral density of $\{ \partial X_t \}$,
%  for estimation we approximate the integral over Fourier frequencies, and utilize the periodogram of the differenced data for $G$.
%  Again, we omit any contributions to the sum arising from Fourier frequencies that correspond to zeros of $\delta (z)$, as such an omission
%  only results in a loss of order $T^{-1}$.  (The alternative is to compute the quantities $\widetilde{\Delta} (z)$ at Fourier frequencies,
%  using the factorization results of Corollary  \ref{cor:filter-noise}; this is not worth the effort in practical applications.)

HERE revisit section 1 exercises now with RW and diff - VAR(1), using LTSC.  Petrol ex


% Exercise 1
% 
% # Simulate a Gaussian VAR(1)  
% T.sim <- 500 + 2*len
% N <- 2
% phi.matrix <- rbind(c(1,.5),c(-.2,.3))
% innovar.matrix <- diag(N)
% true.psidelta <- var1.par2psi(phi.matrix,100)
% gamma.0 <- matrix(solve(diag(N^2) - phi.matrix %x% phi.matrix) %*% 
% 	matrix(innovar.matrix,ncol=1),nrow=N)
% x.init <- t(chol(gamma.0)) %*% rnorm(N)
% x.next <- x.init
% x.sim <- NULL
% for(t in 1:T.sim)
% {
% 	x.next <- phi.matrix %*% x.next + rnorm(N)
% 	x.sim <- cbind(x.sim,x.next)
% }
% x.sim <- ts(t(x.sim))
% x.acf <- acf(x.sim,type="covariance",plot=FALSE,lag.max=T.sim)[[1]]
% x.acf <- aperm(aperm(x.acf,c(3,2,1)),c(2,1,3))
% 
% # construct and apply BW filter
% bw.filter <- wk.trend[[1]]
% x.trend.bw11 <- filter(x.sim[,1],bw.filter[1,1,],method="convolution",sides=2)
% x.trend.bw12 <- filter(x.sim[,2],bw.filter[1,2,],method="convolution",sides=2)
% x.trend.bw21 <- filter(x.sim[,1],bw.filter[2,1,],method="convolution",sides=2)
% x.trend.bw22 <- filter(x.sim[,2],bw.filter[2,2,],method="convolution",sides=2)
% x.trend.bw <- cbind(x.trend.bw11 + x.trend.bw12,x.trend.bw21 + x.trend.bw22)
% x.trend.bw <- x.trend.bw[(len+1):(T.sim-len),] 
%  
% 
% # visualize
% pdf(file="petrolVAR1trends.pdf",width=4, height=4.6)
% par(oma=c(2,0,0,0),mar=c(2,4,2,2)+0.1,mfrow=c(2,1),cex.lab=.8)
% plot(ts(x.sim[(len+1):(T.sim-len),1]),ylab="Series 1",xlab="",yaxt="n",xaxt="n",col=grey(.7))
% axis(1,cex.axis=.5)
% axis(2,cex.axis=.5)
% lines(x.trend.bw[,1],col=1,lwd=2)
% #lines(ts(x.sim[(len+1):(T.sim-len),1]))
% plot(ts(x.sim[(len+1):(T.sim-len),2]),ylab="Series 2",xlab="",yaxt="n",xaxt="n",col=grey(.7))
% axis(1,cex.axis=.5)
% axis(2,cex.axis=.5)
% lines(x.trend.bw[,2],col=1,lwd=2)
% #lines(ts(x.sim[(len+1):(T.sim-len),2]))
% mtext("Time", side = 1, line = 1,outer=TRUE)
% dev.off()
% 
% # get LPP soln using true VAR(1)
% A.next <- diag(2)
% A.phi <- matrix(0,2,2)
% for(j in 1:len)
% {
% 	A.next <- A.next %*% phi.matrix
% 	A.phi <- A.phi + bw.filter[,,(len+1-j)] %*% A.next
% }
% bw.lpp <- bw.filter[,,(len+1):(2*len+1)]
% bw.lpp[,,1] <- bw.lpp[,,1] + A.phi
% 
% 
% # get MDFA concurrent filter
% q <- 30
% Grid <- T.sim
% frf.trend <- mdfa.frf(wk.trend[[1]],len,Grid)
% spec.hat <- mdfa.pergram(x.sim,1)
% # choose one of the four constraint scenarios for MDFA	
% bw.mdfa <- mdfa.unconstrained(frf.trend,spec.hat,q)
% bw.mdfa <- mdfa.levelconstraint(frf.trend,spec.hat,q)
% bw.mdfa <- mdfa.tsconstraint(frf.trend,spec.hat,q)
% bw.mdfa <- mdfa.ltsconstraint(frf.trend,spec.hat,q)
%  
% # apply the MDFA concurrent filter
% x.trend.mdfa11 <- filter(x.sim[,1],bw.mdfa[[1]][1,1,],method="convolution",sides=1)
% x.trend.mdfa12 <- filter(x.sim[,2],bw.mdfa[[1]][1,2,],method="convolution",sides=1)
% x.trend.mdfa21 <- filter(x.sim[,1],bw.mdfa[[1]][2,1,],method="convolution",sides=1)
% x.trend.mdfa22 <- filter(x.sim[,2],bw.mdfa[[1]][2,2,],method="convolution",sides=1)
% x.trend.mdfa <- cbind(x.trend.mdfa11 + x.trend.mdfa12,x.trend.mdfa21 + x.trend.mdfa22)
% x.trend.mdfa <- x.trend.mdfa[(len+1):(T.sim-len),] 
% 
% # construct and apply concurrent BW filter
% conc.filter <- conc.trend
% x.trend.conc11 <- filter(x.sim[,1],rev(conc.filter[1,1,]),method="convolution",sides=1)
% x.trend.conc12 <- filter(x.sim[,2],rev(conc.filter[1,2,]),method="convolution",sides=1)
% x.trend.conc21 <- filter(x.sim[,1],rev(conc.filter[2,1,]),method="convolution",sides=1)
% x.trend.conc22 <- filter(x.sim[,2],rev(conc.filter[2,2,]),method="convolution",sides=1)
% x.trend.conc <- cbind(x.trend.conc11 + x.trend.conc12,x.trend.conc21 + x.trend.conc22)
% x.trend.conc <- x.trend.conc[(len+1):(T.sim-len),] 
% 
% # construct and apply optimal LPP filter
% x.trend.lpp11 <- filter(x.sim[,1],bw.lpp[1,1,],method="convolution",sides=1)
% x.trend.lpp12 <- filter(x.sim[,2],bw.lpp[1,2,],method="convolution",sides=1)
% x.trend.lpp21 <- filter(x.sim[,1],bw.lpp[2,1,],method="convolution",sides=1)
% x.trend.lpp22 <- filter(x.sim[,2],bw.lpp[2,2,],method="convolution",sides=1)
% x.trend.lpp <- cbind(x.trend.lpp11 + x.trend.lpp12,x.trend.lpp21 + x.trend.lpp22)
% x.trend.lpp <- x.trend.lpp[(len+1):(T.sim-len),] 
% 
% 
% 
% # visualize
% par(oma=c(2,0,0,0),mar=c(2,4,2,2)+0.1,mfrow=c(2,1),cex.lab=.8)
% plot(ts(x.trend.bw[,1]),ylab="",xlab="",yaxt="n",xaxt="n",col=1,lwd=1,
% 	ylim=c(min(x.trend.bw[,1])-4,max(x.trend.bw[,1])))
% axis(1,cex.axis=.5)
% axis(2,cex.axis=.5)
% lines(x.trend.mdfa[,1]-2,col=grey(.4),lwd=1,lty=1)
% lines(x.trend.lpp[,1]-4,col=grey(.6),lwd=1,lty=1)
% plot(ts(x.trend.bw[,2]),ylab="",xlab="",yaxt="n",xaxt="n",col=1,lwd=1,
% 	ylim=c(min(x.trend.bw[,2])-4,max(x.trend.bw[,2])))
% axis(1,cex.axis=.5)
% axis(2,cex.axis=.5)
% lines(x.trend.mdfa[,2]-2,col=grey(.4),lwd=1,lty=1)
% lines(x.trend.lpp[,2]-4,col=grey(.6),lwd=1,lty=1)
% mtext("Time", side = 1, line = 1,outer=TRUE)
%  
% # compare in-sample performance: MDFA for series, and then Conc for series
% print(c(mean((x.trend.bw[,1] - x.trend.mdfa[,1])^2),
% 	mean((x.trend.bw[,2] - x.trend.mdfa[,2])^2)))
% print(c(mean((x.trend.bw[,1] - x.trend.conc[,1])^2),
% 	mean((x.trend.bw[,2] - x.trend.conc[,2])^2)))
% print(c(mean((x.trend.bw[,1] - x.trend.lpp[,1])^2),
% 	mean((x.trend.bw[,2] - x.trend.lpp[,2])^2)))
% 
% # compare to criterion value
% print(diag(bw.mdfa[[2]]))
% 
% 

% 
% 
% ##########################################################
% ### Exercise 2: replication - simulate bivariate LLM and apply MDFA
% 
% # Simulate a Gaussian LLM  
% psi.sim <- c(2.17150287559847+1i, -8.36795922528+1i, -6.04133725367594+1i, 
% 0.0648981656699+1i, -6.80849700177184+1i, -6.66004335288479+1i, 
% -0.00016098322952+1i, 0.00051984185863+1i)
% psi.sim[7:8] <- c(0+1i,0+1i)
% psi.sim <- sigex.par2psi(param.sim,flag.default,mdl)
% T.sim <- 500 + 2*len
% N <- 2
% burnin <- 0
% init <- matrix(rnorm(2*N),ncol=N)
% 
%  
% x.sim <- sigex.sim(psi.sim,mdl,T.sim,burnin,Inf,init)
% plot(ts(x.sim))
% 
% # construct and apply BW filter
% bw.filter <- wk.trend[[1]]
% x.trend.bw11 <- filter(x.sim[,1],bw.filter[1,1,],method="convolution",sides=2)
% x.trend.bw12 <- filter(x.sim[,2],bw.filter[1,2,],method="convolution",sides=2)
% x.trend.bw21 <- filter(x.sim[,1],bw.filter[2,1,],method="convolution",sides=2)
% x.trend.bw22 <- filter(x.sim[,2],bw.filter[2,2,],method="convolution",sides=2)
% x.trend.bw <- cbind(x.trend.bw11 + x.trend.bw12,x.trend.bw21 + x.trend.bw22)
% x.trend.bw <- x.trend.bw[(len+1):(T.sim-len),] 
% 
% # visualize
% pdf(file="petrolLLMtrendsNull.pdf",width=4, height=4.6)
% par(oma=c(2,0,0,0),mar=c(2,4,2,2)+0.1,mfrow=c(2,1),cex.lab=.8)
% plot(ts(x.sim[(len+1):(T.sim-len),1]),ylab="Consumption",xlab="",yaxt="n",xaxt="n",col=grey(.7))
% axis(1,cex.axis=.5)
% axis(2,cex.axis=.5)
% lines(x.trend.bw[,1],col=1,lwd=1)
% plot(ts(x.sim[(len+1):(T.sim-len),2]),ylab="Imports",xlab="",yaxt="n",xaxt="n",col=grey(.7))
% axis(1,cex.axis=.5)
% axis(2,cex.axis=.5)
% lines(x.trend.bw[,2],col=1,lwd=1)
% mtext("Time", side = 1, line = 1,outer=TRUE)
% dev.off()
% 
% # get MDFA concurrent filter for I(1) case
% q <- 30
% x.diff <- diff(x.sim)
% spec.hat <- mdfa.pergram(x.diff,c(1,-1))
% Grid <- T.sim - 1
% frf.trend <- mdfa.frf(wk.trend[[1]],len,Grid)
% #bw.mdfa <- mdfa.levelconstraint(frf.trend,spec.hat,q)[[1]]
% constraints.mdfa <- mdfa.getconstraints(frf.trend,0,NULL,q)
% bw.mdfa <- mdfa.filter(frf.trend,spec.hat,constraints.mdfa[[1]],constraints.mdfa[[2]])[[1]]
% 
% # apply the MDFA concurrent filter
% x.trend.mdfa11 <- filter(x.sim[,1],bw.mdfa[1,1,],method="convolution",sides=1)
% x.trend.mdfa12 <- filter(x.sim[,2],bw.mdfa[1,2,],method="convolution",sides=1)
% x.trend.mdfa21 <- filter(x.sim[,1],bw.mdfa[2,1,],method="convolution",sides=1)
% x.trend.mdfa22 <- filter(x.sim[,2],bw.mdfa[2,2,],method="convolution",sides=1)
% x.trend.mdfa <- cbind(x.trend.mdfa11 + x.trend.mdfa12,x.trend.mdfa21 + x.trend.mdfa22)
% x.trend.mdfa <- x.trend.mdfa[(len+1):(T.sim-len),] 
% 
% # construct and apply concurrent BW filter
% conc.filter <- conc.trend
% x.trend.conc11 <- filter(x.sim[,1],rev(conc.filter[1,1,]),method="convolution",sides=1)
% x.trend.conc12 <- filter(x.sim[,2],rev(conc.filter[1,2,]),method="convolution",sides=1)
% x.trend.conc21 <- filter(x.sim[,1],rev(conc.filter[2,1,]),method="convolution",sides=1)
% x.trend.conc22 <- filter(x.sim[,2],rev(conc.filter[2,2,]),method="convolution",sides=1)
% x.trend.conc <- cbind(x.trend.conc11 + x.trend.conc12,x.trend.conc21 + x.trend.conc22)
% x.trend.conc <- x.trend.conc[(len+1):(T.sim-len),] 
% 
% # visualize
% pdf(file="petrolLLMrealtimeNull.pdf",width=4, height=4.6)
% par(oma=c(2,0,0,0),mar=c(2,4,2,2)+0.1,mfrow=c(2,1),cex.lab=.8)
% plot(ts(x.trend.bw[,1]),ylab="Consumption",xlab="",yaxt="n",xaxt="n",col=1,lwd=1,
% 	ylim=c(min(x.trend.bw[,1])-2/2,max(x.trend.bw[,1])))
% axis(1,cex.axis=.5)
% axis(2,cex.axis=.5)
% lines(x.trend.mdfa[,1]-1/2,col=grey(.4),lwd=1,lty=1)
% lines(x.trend.conc[,1]-2/2,col=grey(.6),lwd=1,lty=1)
% plot(ts(x.trend.bw[,2]),ylab="Imports",xlab="",yaxt="n",xaxt="n",col=1,lwd=1,
% 	ylim=c(min(x.trend.bw[,2])-2,max(x.trend.bw[,2])))
% axis(1,cex.axis=.5)
% axis(2,cex.axis=.5)
% lines(x.trend.mdfa[,2]-1,col=grey(.4),lwd=1,lty=1)
% lines(x.trend.conc[,2]-2,col=grey(.6),lwd=1,lty=1)
% mtext("Time", side = 1, line = 1,outer=TRUE)
% dev.off()
%  
% # compare in-sample performance: MDFA for series, and then Conc for series
% print(c(mean((x.trend.bw[,1] - x.trend.mdfa[,1])^2),
% 	mean((x.trend.bw[,2] - x.trend.mdfa[,2])^2)))
% print(c(mean((x.trend.bw[,1] - x.trend.conc[,1])^2),
% 	mean((x.trend.bw[,2] - x.trend.conc[,2])^2)))
% 
% 



HERE  HP exercise with connection to STM, and MB replication.  Ndc ex

HERE CF apply to RW

HERE  Starts exercise
