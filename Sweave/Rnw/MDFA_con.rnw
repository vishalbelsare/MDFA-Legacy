

<<loading_code,echo=False>>=
path.main <- getwd()
#load_code(paste(path.main,"/Sweave/Sigex",sep=""))
setwd(paste(path.main,"/Sweave/RcodeTSM",sep=""))
# requires packages expm and xtable
library(devtools)
library(expm)
library(xtable)
source("sqrtm.r")
source("var1.psi2par.r")
source("var1.par2psi.r")
source("lpp.var1.r")
source("mdfa.dft.r")
source("mdfa.pergram.r")
source("mdfa.frf.r")
source("mdfa.coeff.r")
source("mdfa.filter.r")
source("mdfa.getconstraints.r")
source("mdfa.unconstrained.r")
source("mdfa.levelconstraint.r")
source("mdfa.tsconstraint.r")
source("mdfa.ltsconstraint.r")
source("sigex.sim.r")
source("sigex.graph.r")
setwd(path.main)
@

\chapter{Multivariate Direct Filter Analysis for Non-stationary Processes}
\label{chap:int}

 We now extend the basic MDFA of Chapter \ref{chap:basic}  by considering
 the method's application to  non-stationary processes.  
 Section \ref{sec:constraint} introduces the idea of filter constraints
arising from time-varying means, a form of non-stationarity.
 This treatment is generalized in Section \ref{sec:non-stat}
  by the definition of non-stationary processes, and theory for the corresponding
   model-based filters is developed.  Finally, the MDFA criterion for
    non-stationary processes is discussed in Section \ref{sec:mdfa-nonstat}.
 

\section{Constrained MDFA}
\label{sec:constraint}

 Various constraints upon the concurrent filter can be envisioned, 
   and imposing such strictures results in  a constrained MDFA. 
   A chief case of interest arises when the 
    data process has a time-varying mean (which is a form of  non-stationarity);
  then it is necessary to impose additional filter constraints -- otherwise
   the filter error will not have mean zero.    To see why, 
   Write $\Delta (L) = \Psi (L) - \widehat{\Psi} (L)$ as the discrepancy filter,
   so that we see  from (\ref{eq:dfa-error})  
   that $\EE [ E_t ] = \Delta (L) \, \EE [ X_t ]$; 
   by Definition \ref{def:lpp}, we require
 that $\EE [ E_t ] = 0$ for any LPP.  
  If $\EE [ X_t] = 0$ then this condition is always satisfied, but
   for most time series of interest the mean will be nonzero, and is typically
    time-varying.  For such cases additional constraints on $\Delta (L)$ must be imposed,
    which implicitly amount to constraints on $\widehat{\Psi} (L)$.
    
\begin{Example}    {\bf Constant Mean.}  \rm
\label{exam:constant.mean}
  If $\EE [ X_t ] = \mu$, some nonzero constant,  then we require $\Delta (1) = 0$.
  This is because the mean of the filter error is
  \[
   \Delta (B) \, \EE [ X_t] = \Delta(B) \, \mu = \sum_j \delta (j) \, \mu =
   \Delta (1) \, \mu,
  \]
  and this is zero only if $\Delta (1) = 0$.  This is called a Level Constraint (LC).
\end{Example}  

\begin{Example}    {\bf Linear Mean.}  \rm
\label{exam:linear.mean}
  Suppose that $\EE [ X_t ] = \mu \, t$, where $\mu$ is a nonzero slope
 of a linear time trend.  Then it is required that  $\partial {\Delta} (1) = 0$
  in addition to the LC,  which is seen as follows:
  \[
   \Delta (B) \, \EE [ X_t] = \Delta(B) \, \mu \, t =   \mu \, \sum_j \delta (j) \, (t-j)
   = \mu \, \left(t \, \sum_j \delta (j) - \sum_j j \,\delta (j) \right)
    = \mu \, t \, \Delta(1) - \mu \, \partial \Delta (1).
  \]
  This mean of the filter error  is zero only if both $\Delta(1)=0$ and
  $\partial \Delta (1)=0$; the latter condition is called the
   Time-Shift Constraint (TSC).  
\end{Example}  

     Hence, for linear means we obtain
 three fundamental types of constraints: LC, TSC, and Level plus 
 Time-Shift Constraint (LTSC), which combines both LC and TSC.
  Using the fact that $\Delta (L) = \Psi (L) - \widehat{\Psi} (L)$,
   these three constaints can be described as follows:
\begin{align*}
 \mbox{LC} : &  \;  \Delta (1) = 0 \quad \mbox{or} \quad \Psi (1) = \widehat{\Psi} (1) \\
 \mbox{TSC} : &  \;   \partial {\Delta} (1) = 0 \quad \mbox{or} \quad 
 \partial {\Psi} (1) = \partial {\widehat{\Psi}} (1)  \\
 \mbox{LTSC} : &  \;  \Delta (1) = 0,  \,  \partial {\Delta} (1) = 0 \quad 
 \mbox{or} \quad \Psi (1) = \widehat{\Psi} (1), \; \partial {\Psi} (1) =
 \partial {\widehat{\Psi}} (1).
\end{align*}
 In the case of  concurrent filters of form  (\ref{eq:conc.filter}), 
 LC is accomplished by demanding that 
  $\sum_{j=0}^{q-1} \widehat{\psi} (j) = \Psi(1)$.   More generally, we consider  linear constraints  formulated via
\begin{equation}
\label{eq:concurrent-constrain}
  \vartheta = R \, \varphi + Q,
\end{equation}
 where $R$ is $n q \times n r$ and $\varphi$ is $n r \times 1$ dimensional, consisting of 
 free parameters; $Q$ is a matrix of constants, and is $n q \times 1$ dimensional.


\begin{Illustration}  {\bf Level Constraint (LC).}   \rm
\label{ill:lc}
 Note that $\sum_{j=0}^{q-1} \widehat{\psi} (j) = \Psi(1)$ implies that
\begin{equation}
\label{eq:lc-gamma0}
 \widehat{\psi} (0) = \Psi(1) - \sum_{j=1}^{q-1} \widehat{\psi} (j).
\end{equation}
 Hence  $ \varphi^{\prime}  = [ \widehat{\psi} (1), \widehat{\psi} (2), \ldots, \widehat{\psi} (q-1) ] $ and
\[
	R  = \left[ \begin{array}{ccc} -1 & \ldots & -1 \\ 1 & 0 & 0 \\
		\vdots & \ddots & \vdots \\ 0 & 0 & 1  \end{array} \right]  \otimes 1_n \qquad
	Q = \left[ \begin{array}{c} \Psi (1) \\ 0 \\ \vdots \\ 0 \end{array} \right].
\]
\end{Illustration}
 
 
\begin{Illustration}  {\bf Time Shift Constraint (TSC).}   \rm
\label{ill:tsc}
   The constraint is $\partial {\Psi} (1) = \partial \widehat{\Psi} (1)
   = \sum_{j=0}^{q-1} j \, \widehat{\psi} (j)$,
 or $\widehat{\psi} (1)  = \partial {\Psi} (1)  -  \sum_{j=2}^{q-1} j \, \widehat{\psi} (j) $.
 Hence  $ \varphi^{\prime}  = [ \widehat{\psi} (0), \widehat{\psi} (2), \ldots, \widehat{\psi} (q-1) ] $ and
\[
	R  = \left[ \begin{array}{cccc} 1 & 0 &  \ldots &  0  \\  0 & -2  &  -3  & \ldots  \\
		0 & 1 & 0 & \ldots \\ 
		\vdots & \ddots & \vdots & \vdots \\ 0 & \ldots & 0 & 1 \end{array} \right] \otimes 1_n \qquad
	Q = \left[ \begin{array}{c} 0 \\ \partial {\Psi} (1) \\ 0 \\ \vdots \\ 0 \end{array} \right].
\]
\end{Illustration}
 
 
\begin{Illustration}  {\bf Level and Time Shift Constraint (LTSC).}  \rm
\label{ill:ltsc}
   Take the Time Shift Constraint formula for $\widehat{\psi} (1)$,
 and plug this into (\ref{eq:lc-gamma0}), to obtain
\begin{align*}
 \widehat{\psi} (0)  & = \Psi (1) - \left( \partial {\Psi} (1)  -  \sum_{j=2}^{q-1} j  \, \widehat{\psi} (j) \right) -  \sum_{j=2}^{q-1} 
 \widehat{\psi} (j)  \\
	& = \Psi (1) -  \partial {\Psi} (1)  +  \sum_{j=2}^{q-1} (j-1)  \, \widehat{\psi} (j).
\end{align*}
 Hence  $ \varphi^{\prime}  = [  \widehat{\psi} (2), \ldots, \widehat{\psi} (q-1)  ] $ and
\[
	R  = \left[ \begin{array}{cccc} 1 & 2  &  3  &   \ldots    \\  -2  & -3  &  -4  & \ldots  \\
		 1  & 0 & \ldots & 0 \\ 
		\vdots & \ddots & \vdots & \vdots \\ 0 & \ldots & 0 & 1 \end{array} \right]  \otimes 1_n \qquad
	Q = \left[ \begin{array}{c} \Psi (1) - \partial {\Psi} (1)  \\  \partial {\Psi} (1) \\ 0 \\ \vdots \\ 0 \end{array} \right].
\]
\end{Illustration}



 More generally, we can envision an LPP involving $m$ linear constraints on 
  $\vartheta$, taking the form
 $   A = [ J \otimes 1_n ] \, \vartheta$, where $J$ is $m \times q$ 
 dimensional ($m < q$) and $A$ is $n m \times 1$ dimensional.
 (The LC, TSC, and LTSC examples all have this form.)  In order to express 
 this constraint in the form 
 (\ref{eq:concurrent-constrain}), we use the Q-R decomposition 
 (Golub and Van Loan, 1996) of $J$, writing
 $J = C \, G \, \Pi$ for an orthogonal matrix $C$ (which is $m \times m$ dimensional),
 a rectangular upper triangular matrix $G$
 (which is $m \times q$ dimensional), and a permuation matrix $\Pi$ 
 (which is $q \times q$ dimensional).  
 Standard matrix software such as $\textsc{R}$ will provide the Q-R decomposition $J$,
 and should produce the rank of $J$ as  a by-product --
 if this is less than $m$, then there are redundancies in the 
 constraints that should first be eliminated. 
 

 Hence  proceeding with a full rank $J$, we partition $G$ as $G = [ G_1 \, G_2]$ 
 such that $G_1$ has $m$ columns and $G_2$
 has $q-m$ columns.  This quantity $q-m$ corresponds to the number 
 of free coefficient matrices, and is therefore the same as $r$.
 The Q-R decomposition guarantees that $G_1$ is an upper triangular matrix, 
 and moreover it is invertible.  Therefore
\[
  \left[ G_1^{-1} \, C^{-1} \otimes 1_n \right] \, A  = 
  \left( \left[ 1_m , \, G_1^{-1} \, G_2 \right] \, \Pi \otimes 1_n  \right) \, \vartheta,
\]
 and the action of $\Pi$ (together with the tensor product) amounts 
 to a   permutation of the elements of $\vartheta$.
  Let the output of this permutation be denoted
\[
   \left[ \begin{array}{l} \overline{\vartheta} \\ \underline{\vartheta} \end{array} \right]
   = \left( \Pi \otimes 1_n \right) \, \vartheta,
\]
 where $\overline{\vartheta}$ is $n m \times 1$ dimensional and
 $\underline{\vartheta}$ is $n r \times 1$ dimensional.  
 Then  by substitution we can solve for $\overline{\vartheta}$ in terms 
 of $\underline{\vartheta}$:
\[
   \overline{\vartheta} =  \left[ G_1^{-1} \, C^{-1} \otimes 1_n \right] \, A - 
   \left[  G_1^{-1} \, G_2  \otimes 1_n   \right] \, \underline{\vartheta}.
\]
 Therefore we recognize the free variables $\varphi = \underline{\vartheta}$, 
 and obtain $R$ and $Q$ in (\ref{eq:concurrent-constrain}) via
\begin{align*}
   R & = \Pi^{-1} \, \left[ \begin{array}{c} - G_1^{-1} \, G_2 \\ 
   1_{r} \end{array} \right] \otimes 1_n  \\
  Q & = \left( \Pi^{-1}  \, \left[ \begin{array}{c}  G_1^{-1} \, C^{-1} \\ 0 \end{array} \right] \otimes 1_n  \right) \, A.
\end{align*}


 \begin{Exercise} {\bf QR Decomposition.} \rm
 \label{exer:qr.constraint}
  Consider an arbitrary set of constraints $J$ on $\vartheta$, such that
    $   A = [ J \otimes 1_n ] \, \vartheta$ for a given vector $A$.  
    Encode the procedure that obtains $R$ and $Q$, and apply this
    to the cases of the LC, TSC, and LTSC scenarios with $n=1$, verifying the results
    given in Illustrations \ref{ill:lc}, \ref:tsc}, and \ref:ltsc}.
    Use one-step ahead forecasting as the target filter.
 \end{Exercise}
 
 <<exer_qr-constraint,Echo=True>>=
  N <- 1
  q <- 10

  ## level constraint case
	constraint.mat <- matrix(rep(1,q),nrow=1)
	constraint.vec <- diag(N)
	constraint.qr <- qr(constraint.mat)
	constraint.q <- qr.Q(constraint.qr)
	constraint.r <- qr.R(constraint.qr)
	constraint.pivot <- constraint.qr$pivot
	constraint.ipivot <- sort.list(constraint.pivot)
	M <- q - dim(constraint.r)[2] + dim(constraint.q)[2]
	R.mat <- rbind(-solve(constraint.r[,1:(dim(constraint.q)[2]),drop=FALSE],
		constraint.r[,(dim(constraint.q)[2]+1):q,drop=FALSE]),diag(q-M))
	R.mat <- R.mat[constraint.ipivot,] %x% diag(N)
	Q.mat <- rbind(solve(constraint.r[,1:(dim(constraint.q)[2]),drop=FALSE]) %*% 
		solve(constraint.q),matrix(0,q-M,M)) 
	Q.mat <- (Q.mat[constraint.ipivot,] %x% diag(N)) %*% constraint.vec
  print(R.mat)
	
  ## time shift constraint case
	constraint.mat <- matrix(seq(0,q-1),nrow=1)
	constraint.vec <- -diag(N)
	constraint.qr <- qr(constraint.mat)
	constraint.q <- qr.Q(constraint.qr)
	constraint.r <- qr.R(constraint.qr)
	constraint.pivot <- constraint.qr$pivot
	constraint.ipivot <- sort.list(constraint.pivot)
	M <- q - dim(constraint.r)[2] + dim(constraint.q)[2]
	R.mat <- rbind(-solve(constraint.r[,1:(dim(constraint.q)[2]),drop=FALSE],
		constraint.r[,(dim(constraint.q)[2]+1):q,drop=FALSE]),diag(q-M))
	R.mat <- R.mat[constraint.ipivot,] %x% diag(N)
	Q.mat <- rbind(solve(constraint.r[,1:(dim(constraint.q)[2]),drop=FALSE]) %*% 
		solve(constraint.q),matrix(0,q-M,M)) 
	Q.mat <- (Q.mat[constraint.ipivot,] %x% diag(N)) %*% constraint.vec
  print(R.mat)
  
	## level and time shift constraint case
	constraint.mat <- rbind(rep(1,q),seq(0,q-1))
	constraint.vec <- rbind(diag(N),-diag(N))
	constraint.qr <- qr(constraint.mat)
	constraint.q <- qr.Q(constraint.qr)
	constraint.r <- qr.R(constraint.qr)
	constraint.pivot <- constraint.qr$pivot
	constraint.ipivot <- sort.list(constraint.pivot)
	M <- q - dim(constraint.r)[2] + dim(constraint.q)[2]
	R.mat <- rbind(-solve(constraint.r[,1:(dim(constraint.q)[2]),drop=FALSE],
		constraint.r[,(dim(constraint.q)[2]+1):q,drop=FALSE]),diag(q-M))
	R.mat <- R.mat[constraint.ipivot,] %x% diag(N)
	Q.mat <- rbind(solve(constraint.r[,1:(dim(constraint.q)[2]),drop=FALSE]) %*% 
		solve(constraint.q),matrix(0,q-M,M)) 
	Q.mat <- (Q.mat[constraint.ipivot,] %x% diag(N)) %*% constraint.vec
  print(R.mat)
@


  These formulas allow one to compute the   form (\ref{eq:concurrent-constrain}) 
   from given constraints, and
 an analytical solution to the resulting MDFA criterion  be obtained from the following result.

\begin{Proposition}
\label{prop:mdfa.quadsoln-constrain}
 The minimizer of the  MDFA criterion given by   (\ref{eq:mdfa-criterion})
 with respect to  $\mathcal{G}$ -- consisting of all length $q$ concurrent filters 
 subject to  linear constraints of the form (\ref{eq:concurrent-constrain}) -- is
\begin{equation}
\label{eq:phi.soln-constrained}
 \varphi =  { \left[ R^{\prime} \, B \, R \right] }^{-1} \, R^{\prime} \,
 \left( b - B \, Q \right).
\end{equation}
 The minimal value is  
\begin{equation}
\label{eq:opt.val.mdfa-constrained}
{ \langle \Psi (z) \, G \, { \Psi (z) }^* \rangle }_0 
  + {(b - B \, Q)}^{\prime} \, \left( 
  R \, { \left[ R^{\prime} \, B \, R \right] }^{-1} \, R^{\prime} 
  + B^{-1} \right) \, ( b - B \, Q) -   b^{\prime} \, B^{-1} \, b.
\end{equation}
\end{Proposition}

\paragraph{Proof of Proposition \ref{prop:mdfa.quadsoln-constrain}.}
 Substituting (\ref{eq:concurrent-constrain}) in (\ref{eq:mdfa-crit.linear}) yields
\begin{align*}
  D_{\Psi} (\vartheta, G) &  = \varphi^{\prime} \,  \left[ R^{\prime} \, B \, R \right] \,  \varphi 
  + \left[ Q^{\prime} \, B \, R - b^{\prime} \, R \right] \, \varphi + \varphi^{\prime} \,
   \left[ R^{\prime} \, B \, Q - R^{\prime} \, b \right]  \\
 & + Q^{\prime} \, B \, Q  - Q^{\prime} \, b - b^{\prime} \, Q  + { \langle \Psi (z) \, G \, { \Psi (z) }^* \rangle }_0.
\end{align*}
  Now by applying the method of proof in Proposition \ref{prop:mdfa.quadsoln}, we obtain 
  the formula (\ref{eq:phi.soln-constrained}) for $\varphi$.  Plugging back into $D_{\Psi} (\vartheta, G)$
 yields the minimal value (\ref{eq:opt.val.mdfa-constrained}).  $\quad \Box$


\vspace{.5cm}

For computation, we utilize the same approximations to $B$ and $b$ as discussed 
in  Chapter \ref{chap:basic},
 obtaining the constrained MDFA filter $\vartheta$ via (\ref{eq:phi.soln-constained})
 followed by (\ref{eq:concurrent-constrain}).

\begin{Exercise} {\bf  Constrained MDFA for White Noise with Linear Trend.} \rm
\label{exer:wntrend-mdfa}
This exercise applies the constrained MDFA in the case of an ideal low-pass filter
 (cf. Example \ref{exam:ideal-low}) 
 applied to a white noise process that exhibits a linear trend.
 Simulate a sample of size $T=5000$ from a
  bivariate white noise process with 
  $\Sigma$ equal to the identity, but with a linear trend  given by
  \begin{equation}
  \label{eq:wntrend-lin.trend}
   \left[ \begin{array}{c} 1 \\ 2 \end{array} \right] + t \, 
   \left[ \begin{array}{c} -.002 \\ .001 \end{array} \right].
  \end{equation}
   Apply the   ideal low-pass filter (cf. Example \ref{exam:ideal-low}) with 
  $\mu = \pi/6$ to the sample (truncate the filter to $1000$ coefficients on each side).  
 Use the moving average filter  MDFA  with LC, TSC, and LTSC constraints
  (Proposition \ref{prop:mdfa.quadsoln-constrain}),
  as well as unconstrained MDFA  (Proposition \ref{prop:mdfa.quadsoln}), to find the best
 concurrent filter, setting $q= 30$. 
  (Hint: compute the periodogram from OLS residuals obtained by regressing the simulation
   on a constant plus time.)
 Apply this concurrent filter 
 to the simulation, and compare the relevant portions to the ideal trend.
 Also determine the in-sample performance, in comparison to the criterion value
 (\ref{eq:opt.val.mdfa}).   Target the trends for both time series.
\end{Exercise}

<<exercise_wntrend-mdfa,echo=True>>=
# Simulate a Gaussian WN of sample size 2500:
set.seed(123)
T <- 5000
N <- 2
levels <- c(1,2)
slopes <- c(-2,1)/1000
innovar.matrix <- diag(N)
x.sim <- NULL
for(t in 1:T)
{
	x.next <- levels + slopes*t + t(chol(innovar.matrix)) %*% rnorm(N)
	x.sim <- cbind(x.sim,x.next)
}
x.sim <- ts(t(x.sim))
time.trend <- seq(1,T)
sim.ols <- lm(x.sim ~ time.trend)
x.resid <- sim.ols$residuals

# construct and apply low pass filter
mu <- pi/6
len <- 1000
lp.filter <- c(mu/pi,sin(seq(1,len)*mu)/(pi*seq(1,len)))
lp.filter <- c(rev(lp.filter),lp.filter[-1])
x.trend.ideal <- filter(x.sim,lp.filter,method="convolution",sides=2)[(len+1):(T-len),]

# get MDFA concurrent filter
q <- 30
Grid <- T
m <- floor(Grid/2)
# The Fourier frequencies
freq.ft <- 2*pi*Grid^{-1}*(seq(1,Grid) - (m+1))

# frf for ideal low-pass
frf.psi <- rep(0,Grid)
frf.psi[abs(freq.ft) <= mu] <- 1
frf.psi <- matrix(frf.psi,nrow=1) %x% diag(N) 	  
frf.psi <- array(frf.psi,c(N,N,Grid))
spec.hat <- mdfa.pergram(x.resid,1)	
lp.mdfa.uc <- mdfa.unconstrained(frf.psi,spec.hat,q)
lp.mdfa.lc <- mdfa.levelconstraint(frf.psi,spec.hat,q)
lp.mdfa.tsc <- mdfa.tsconstraint(frf.psi,spec.hat,q)
lp.mdfa.ltsc <- mdfa.ltsconstraint(frf.psi,spec.hat,q)

# case 1: apply the unconstrained MDFA concurrent filter 
x.trend.mdfa11 <- filter(x.sim[,1],lp.mdfa.uc[[1]][1,1,],method="convolution",sides=1)
x.trend.mdfa12 <- filter(x.sim[,2],lp.mdfa.uc[[1]][1,2,],method="convolution",sides=1)
x.trend.mdfa21 <- filter(x.sim[,1],lp.mdfa.uc[[1]][2,1,],method="convolution",sides=1)
x.trend.mdfa22 <- filter(x.sim[,2],lp.mdfa.uc[[1]][2,2,],method="convolution",sides=1)
x.trend.mdfa <- cbind(x.trend.mdfa11 + x.trend.mdfa12,x.trend.mdfa21 + x.trend.mdfa22)
x.trend.mdfa <- x.trend.mdfa[(len+1):(T-len),] 
 
# compare in-sample performance
print(c(mean((x.trend.ideal[,1] - x.trend.mdfa[,1])^2),
	mean((x.trend.ideal[,2] - x.trend.mdfa[,2])^2)))

# compare to criterion value
diag(lp.mdfa.uc[[2]])

# case 2: apply the lc MDFA concurrent filter 
x.trend.mdfa11 <- filter(x.sim[,1],lp.mdfa.lc[[1]][1,1,],method="convolution",sides=1)
x.trend.mdfa12 <- filter(x.sim[,2],lp.mdfa.lc[[1]][1,2,],method="convolution",sides=1)
x.trend.mdfa21 <- filter(x.sim[,1],lp.mdfa.lc[[1]][2,1,],method="convolution",sides=1)
x.trend.mdfa22 <- filter(x.sim[,2],lp.mdfa.lc[[1]][2,2,],method="convolution",sides=1)
x.trend.mdfa <- cbind(x.trend.mdfa11 + x.trend.mdfa12,x.trend.mdfa21 + x.trend.mdfa22)
x.trend.mdfa <- x.trend.mdfa[(len+1):(T-len),] 
 
# compare in-sample performance
print(c(mean((x.trend.ideal[,1] - x.trend.mdfa[,1])^2),
	mean((x.trend.ideal[,2] - x.trend.mdfa[,2])^2)))

# compare to criterion value
diag(lp.mdfa.lc[[2]])

# case 3: apply the tsc MDFA concurrent filter 
x.trend.mdfa11 <- filter(x.sim[,1],lp.mdfa.tsc[[1]][1,1,],method="convolution",sides=1)
x.trend.mdfa12 <- filter(x.sim[,2],lp.mdfa.tsc[[1]][1,2,],method="convolution",sides=1)
x.trend.mdfa21 <- filter(x.sim[,1],lp.mdfa.tsc[[1]][2,1,],method="convolution",sides=1)
x.trend.mdfa22 <- filter(x.sim[,2],lp.mdfa.tsc[[1]][2,2,],method="convolution",sides=1)
x.trend.mdfa <- cbind(x.trend.mdfa11 + x.trend.mdfa12,x.trend.mdfa21 + x.trend.mdfa22)
x.trend.mdfa <- x.trend.mdfa[(len+1):(T-len),] 
 
# compare in-sample performance
print(c(mean((x.trend.ideal[,1] - x.trend.mdfa[,1])^2),
	mean((x.trend.ideal[,2] - x.trend.mdfa[,2])^2)))

# compare to criterion value
diag(lp.mdfa.tsc[[2]])

# case 4: apply the ltsc MDFA concurrent filter 
x.trend.mdfa11 <- filter(x.sim[,1],lp.mdfa.ltsc[[1]][1,1,],method="convolution",sides=1)
x.trend.mdfa12 <- filter(x.sim[,2],lp.mdfa.ltsc[[1]][1,2,],method="convolution",sides=1)
x.trend.mdfa21 <- filter(x.sim[,1],lp.mdfa.ltsc[[1]][2,1,],method="convolution",sides=1)
x.trend.mdfa22 <- filter(x.sim[,2],lp.mdfa.ltsc[[1]][2,2,],method="convolution",sides=1)
x.trend.mdfa <- cbind(x.trend.mdfa11 + x.trend.mdfa12,x.trend.mdfa21 + x.trend.mdfa22)
x.trend.mdfa <- x.trend.mdfa[(len+1):(T-len),] 
 
# compare in-sample performance
print(c(mean((x.trend.ideal[,1] - x.trend.mdfa[,1])^2),
	mean((x.trend.ideal[,2] - x.trend.mdfa[,2])^2)))

# compare to criterion value
diag(lp.mdfa.ltsc[[2]])
@
 
 

\begin{Exercise} {\bf  Constrained MDFA for VAR(1) with Linear Trend.} \rm
\label{exer:var1trend-mdfa}
This exercise applies the constrained MDFA in the case of an ideal low-pass filter
 (cf. Example \ref{exam:ideal-low}) 
 applied to a VAR(1) process that exhibits a linear trend.
 Simulate a sample of size $T=5000$ of a
   bivariate VAR(1) process with linear trend  given by
   (\ref{eq:wntrend-lin.trend}), such that the demeaned process satisfies
\[
  X_t =  \left[ \begin{array}{cc}  1  & 1/2 \\    -1/5  &  3/10
    \end{array} \right] \, X_{t-1} + \epsilon_t,
\]
 with stationary initialization, and $\{ \epsilon_t \}$ a Gaussian white noise of identity innovation variance.   Apply the   ideal low-pass filter
  (cf. Example \ref{exam:ideal-low}) with 
  $\mu = \pi/6$ to the sample (truncate the filter to $1000$ coefficients on each side).  
 Use the moving average filter  MDFA  with LC, TSC, and LTSC constraints
  (Proposition \ref{prop:mdfa.quadsoln-constrain}),
  as well as unconstrained MDFA  (Proposition \ref{prop:mdfa.quadsoln}), to find the best
 concurrent filter, setting $q= 30$. 
  (Hint: compute the periodogram from OLS residuals obtained by regressing the simulation
   on a constant plus time.)
 Apply this concurrent filter 
 to the simulation, and compare the relevant portions to the ideal trend.
 Also determine the in-sample performance, in comparison to the criterion value
 (\ref{eq:opt.val.mdfa}).   Target the trends for both time series.
\end{Exercise}


<<exercise_var1trend-mdfa,echo=True>>=
# Simulate a VAR(1) of sample size 2500:
set.seed(123)
T <- 5000
N <- 2
levels <- c(1,2)
slopes <- c(-2,1)/1000
phi.matrix <- rbind(c(1,.5),c(-.2,.3))
innovar.matrix <- diag(N)
true.psidelta <- var1.par2psi(phi.matrix,100)
gamma.0 <- matrix(solve(diag(N^2) - phi.matrix %x% phi.matrix) %*%
 	matrix(innovar.matrix,ncol=1),nrow=N)
x.init <- t(chol(gamma.0)) %*% rnorm(N)
x.next <- x.init
x.sim <- NULL
for(t in 1:T)
{
 	x.next <- phi.matrix %*% x.next + t(chol(innovar.matrix)) %*% rnorm(N)
 	x.sim <- cbind(x.sim,x.next)
}
x.sim <- ts(t(x.sim))
time.trend <- seq(1,T)
x.sim <- t(levels) %x% rep(1,T) + t(slopes) %x% seq(1,T) + x.sim
sim.ols <- lm(x.sim ~ time.trend)
x.resid <- sim.ols$residuals

# construct and apply low pass filter
mu <- pi/6
len <- 1000
lp.filter <- c(mu/pi,sin(seq(1,len)*mu)/(pi*seq(1,len)))
lp.filter <- c(rev(lp.filter),lp.filter[-1])
x.trend.ideal <- filter(x.sim,lp.filter,method="convolution",sides=2)[(len+1):(T-len),]

# get MDFA concurrent filter
q <- 30
Grid <- T
m <- floor(Grid/2)
# The Fourier frequencies
freq.ft <- 2*pi*Grid^{-1}*(seq(1,Grid) - (m+1))

# frf for ideal low-pass
frf.psi <- rep(0,Grid)
frf.psi[abs(freq.ft) <= mu] <- 1
frf.psi <- matrix(frf.psi,nrow=1) %x% diag(N) 	  
frf.psi <- array(frf.psi,c(N,N,Grid))
spec.hat <- mdfa.pergram(x.resid,1)	
lp.mdfa.uc <- mdfa.unconstrained(frf.psi,spec.hat,q)
lp.mdfa.lc <- mdfa.levelconstraint(frf.psi,spec.hat,q)
lp.mdfa.tsc <- mdfa.tsconstraint(frf.psi,spec.hat,q)
lp.mdfa.ltsc <- mdfa.ltsconstraint(frf.psi,spec.hat,q)

# case 1: apply the unconstrained MDFA concurrent filter 
x.trend.mdfa11 <- filter(x.sim[,1],lp.mdfa.uc[[1]][1,1,],method="convolution",sides=1)
x.trend.mdfa12 <- filter(x.sim[,2],lp.mdfa.uc[[1]][1,2,],method="convolution",sides=1)
x.trend.mdfa21 <- filter(x.sim[,1],lp.mdfa.uc[[1]][2,1,],method="convolution",sides=1)
x.trend.mdfa22 <- filter(x.sim[,2],lp.mdfa.uc[[1]][2,2,],method="convolution",sides=1)
x.trend.mdfa <- cbind(x.trend.mdfa11 + x.trend.mdfa12,x.trend.mdfa21 + x.trend.mdfa22)
x.trend.mdfa <- x.trend.mdfa[(len+1):(T-len),] 
 
# compare in-sample performance
print(c(mean((x.trend.ideal[,1] - x.trend.mdfa[,1])^2),
	mean((x.trend.ideal[,2] - x.trend.mdfa[,2])^2)))

# compare to criterion value
diag(lp.mdfa.uc[[2]])

# case 2: apply the lc MDFA concurrent filter 
x.trend.mdfa11 <- filter(x.sim[,1],lp.mdfa.lc[[1]][1,1,],method="convolution",sides=1)
x.trend.mdfa12 <- filter(x.sim[,2],lp.mdfa.lc[[1]][1,2,],method="convolution",sides=1)
x.trend.mdfa21 <- filter(x.sim[,1],lp.mdfa.lc[[1]][2,1,],method="convolution",sides=1)
x.trend.mdfa22 <- filter(x.sim[,2],lp.mdfa.lc[[1]][2,2,],method="convolution",sides=1)
x.trend.mdfa <- cbind(x.trend.mdfa11 + x.trend.mdfa12,x.trend.mdfa21 + x.trend.mdfa22)
x.trend.mdfa <- x.trend.mdfa[(len+1):(T-len),] 
 
# compare in-sample performance
print(c(mean((x.trend.ideal[,1] - x.trend.mdfa[,1])^2),
	mean((x.trend.ideal[,2] - x.trend.mdfa[,2])^2)))

# compare to criterion value
diag(lp.mdfa.lc[[2]])

# case 3: apply the tsc MDFA concurrent filter 
x.trend.mdfa11 <- filter(x.sim[,1],lp.mdfa.tsc[[1]][1,1,],method="convolution",sides=1)
x.trend.mdfa12 <- filter(x.sim[,2],lp.mdfa.tsc[[1]][1,2,],method="convolution",sides=1)
x.trend.mdfa21 <- filter(x.sim[,1],lp.mdfa.tsc[[1]][2,1,],method="convolution",sides=1)
x.trend.mdfa22 <- filter(x.sim[,2],lp.mdfa.tsc[[1]][2,2,],method="convolution",sides=1)
x.trend.mdfa <- cbind(x.trend.mdfa11 + x.trend.mdfa12,x.trend.mdfa21 + x.trend.mdfa22)
x.trend.mdfa <- x.trend.mdfa[(len+1):(T-len),] 
 
# compare in-sample performance
print(c(mean((x.trend.ideal[,1] - x.trend.mdfa[,1])^2),
	mean((x.trend.ideal[,2] - x.trend.mdfa[,2])^2)))

# compare to criterion value
diag(lp.mdfa.tsc[[2]])

# case 4: apply the ltsc MDFA concurrent filter 
x.trend.mdfa11 <- filter(x.sim[,1],lp.mdfa.ltsc[[1]][1,1,],method="convolution",sides=1)
x.trend.mdfa12 <- filter(x.sim[,2],lp.mdfa.ltsc[[1]][1,2,],method="convolution",sides=1)
x.trend.mdfa21 <- filter(x.sim[,1],lp.mdfa.ltsc[[1]][2,1,],method="convolution",sides=1)
x.trend.mdfa22 <- filter(x.sim[,2],lp.mdfa.ltsc[[1]][2,2,],method="convolution",sides=1)
x.trend.mdfa <- cbind(x.trend.mdfa11 + x.trend.mdfa12,x.trend.mdfa21 + x.trend.mdfa22)
x.trend.mdfa <- x.trend.mdfa[(len+1):(T-len),] 
 
# compare in-sample performance
print(c(mean((x.trend.ideal[,1] - x.trend.mdfa[,1])^2),
	mean((x.trend.ideal[,2] - x.trend.mdfa[,2])^2)))

# compare to criterion value
diag(lp.mdfa.ltsc[[2]])
@
  




\section{Background on Non-stationary Vector Time Series }
\label{sec:non-stat}

We next consider processes that when differenced are 
stationary, which are the most common type occuring in econometrics and finance.  
 This type of non-stationary process substantially broadens
  the possible types of applications over the stationary processes
   considered in Chapters \ref{chap:lpp} and \ref{chap:basic}.
  Also, as such processes typically can have a time-varying mean,
  they also necessitate the use of filter constraints such as those
   considered in Section \ref{sec:constraint}.
  
 We suppose that there exists a degree $d$ polynomial $\delta (L)$
  that reduces each component series of $\{ X_t \}$ to a stationary
   time series (which is allowed to have a non-zero constant mean $\mu$),
   and suppose this is the minimal degree polynomial that accomplishes
    this reduction.  For convenience, and without loss of generality,
    we suppose that $\delta_0 = 1$, or $\delta (0) = 1$.
    We write $\partial X_t = \delta (L) \, X_t$ for
  the stationary, differenced time series,
   where $\partial X_t = \mu + Z_t$ and $\{ Z_t \}$ has a spectral
    representation (\ref{eq:specRep}).  Then it is possible
   to give time-domain and frequency-domain representations of
    the original process $\{ X_t \}$ in terms of the stationary
  ``increments" $\{ \partial X_t \}$, together with deterministic functions
  of time that depend on ``initial values" of the process.
   These deterministic functions can be obtained from 
   the theory of Ordinary Difference Equations (ODE): 
  all solutions to   $\delta (L) X_t = \partial X_t$
   must include a homogeneous solution, i.e., solutions to
    $\delta (L) X_t = 0$,   which include all functions of 
    $t$ that are annihilated by $\delta (L)$.  Below we develop
  a general method of solution, but We first provide a few
   illustrations through specific cases.
 
 \begin{Example} {\bf Representation for an $I(1)$ Process.}  \rm
 \label{exam:I1-rep}
    Letting $\delta (L)= 1-L$, we obtain a once-integrated process,
  denoted as $I(1)$ for short.    
  Because  the constant function (which up to proportionality, is the function $1$)
  is annihilated by $1-L$, we expect the solution to take the form
   of a constant plus some function of the increments $\partial X_t$.
   Proceeding recursively, we obtain
\[
  X_t =    X_{t-1} + \partial X_t = X_{t-2} + \partial X_t + \partial X_{t-1} = \ldots
]
    Let us suppose an initial time of $t=0$ for this process, so that the
      solution is expressed as 
\[
 X_t = X_0 + \sum_{j=1}^t \partial X_j
\]
    for $t \geq 1$ (and can be extended to $t=0$ by taking the sum to be empty in that case).
  Note that this involves a constant function of time,
  the term $X_0$.  Moreover, applying $X_j = \mu + Z_j$ and the spectral representation,
  we obtain
\begin{equation*}
 X_t = X_0 + t \, \mu +   \int_{-\pi}^{\pi} \sum_{j=1}^t e^{i \omega j}
   \, \mathcal{Z} (d\omega).  
\end{equation*}
  The summation inside the integral can be re-expressed when $\omega \neq 0$ as
  $(1 - e^{i \omega (t+1)})/(1-e^{i \omega})$.
\end{Example}   


\begin{Example} {\bf Representation for an $I(2)$ Process.} \rm
\label{exam:i2-rep}
  Now we set  $\delta (L)= {(1-L)}^2$  for a twice-integrated process,
  denoted as $I(2)$ for short.    So first differences of $\{ X_t \}$ 
  have a representation as an $I(1)$ process, and the expression for
   $X_t$ will involve a linear function of time $t$, because $t$ is
  annihilated by $\delta (L)$.  Applying the recursive technique of 
  Example \ref{exam:i1-rep} twice, we obtain
 \[
 X_t = (t+1) \, X_0 - t \, X_{-1}  + \sum_{j=1}^t (t+1-j) \, \partial X_j,
\]
 which holds for $t \geq 1$  (but can be extended to $t=0,-1$ by setting
  the summation to zero).  The linear function of time has slope
  $X_0 - X_{-1}$ and intercept $X_0$.
 Applying $X_j = \mu + Z_j$ and the spectral representation,
  we obtain
\begin{equation*}
 X_t =(t+1) \, X_0 - t \, X_{-1}  + \binom{t+1}{2} \, \mu
 +  \int_{-\pi}^{\pi} \sum_{j=1}^t (t+1-j) \, e^{i \omega j}
   \, \mathcal{Z} (d\omega).  
\end{equation*}
 It can be verified that $(1-L) X_t$ is an $I(1)$ process with level
 $X_0 - X_{-1}$.
\end{Example}

A general technique for obtaining the representation for non-stationary 
processes involves the     inverse of the polynomial of $\delta(L)$, which is
 denoted by $\xi (z)$:
\begin{equation}
\label{eq:xi-def}
 \xi (L) = 1/ \delta (L) = \sum_{j \geq 0 }  \xi_j \, L^j.
\end{equation}
 This $\xi (z)$ is a power series that converges on a disk inside the unit circle.
  Because $\delta (z) \, \xi (z) = 1$, one can recursively solve for the
   coefficients $\xi_j$ in terms of past coefficients, using the $\delta_k$.
  In particular, the $j$th coefficient of $\delta (z) \, \xi (z)$ is given by
  the convolution formula:
\begin{equation}
\label{eq:delta.xi-conv}
  \sum_{k \geq 0} \delta_k \, \xi_{j-k} = 1_{ \{ j=0 \} }.
\end{equation}
  In this formula, the equality is due to the fact that the $j$th coefficient of
   the constant function $1$ (viewed as a power series in $z$) is zero unless $j=0$,
   in which case it equals one.  On the left hand side of (\ref{eq:delta.xi-conv})
   the sum runs from $0$ to  $j \wedge d$.  From the assumption that $\delta (0) =1$,
 it is immediate that $\xi_0 = 1$ and 
  \[
  \xi_j = - \sum_{k \geq 1} \delta_k \, \xi_{j-k}
  \]
for $j > 0$.  Next, for any $ 0 \leq h \leq d-1$ we define
\begin{equation}
\label{eq:init.cond-fcns}
  A_{t} (h) = \sum_{k=0}^h \delta_k \, \xi_{h+t-k}.
\end{equation}
 Note that for $h \geq d$ the formula (\ref{eq:init.cond-fcns}) equals 
 (\ref{eq:delta.xi-conv}), and hence equals zero unless $h+t = 0$; but because
$h < d$ in the definition of $A_t (h)$, the function is non-zero.
  Also, when $ t \leq 0$ we have $\xi_{h+t-k} = 0$ for $ k > h$, so that
\[
 A_t (h) = \sum_{k=0}^d \delta_k \, \xi_{h+t-k} = 1_{ \{ h+t \} = 0} = 1_{ \{ t = -h \}}.
\]
 Using these definitions, we can state the following result.
 
 \begin{Theorem}
 \label{thm:nonstat-rep}
 The solution for $t \geq 1-d$ to $\delta (L) X_t = \partial X_t$ is given by
\begin{equation}
 \label{eq:nonstatCausalRep}
 X_t  = \sum_{h=0}^{d-1} A_{t} (h) \, X_{-h} + 
  \sum_{j=0}^{t-1} \xi_j \, \partial X_{t-j},
\end{equation}
 where the coefficients $\xi_j$ are defined recursively through (\ref{eq:delta.xi-conv}),
  and the time-varying functions $A_t (h)$ are defined via (\ref{eq:init.cond-fcns}).
  Moreover, the algebraic identity 
\begin{equation}
 \label{eq:Identity1}
  1 - \sum_{h=0}^{d-1} A_{h} (t) \, z^{t+h} = \sum_{k=0}^{t-1} \xi_k \, z^k
  \, \delta (B)
\end{equation}
holds, and hence the   spectral representation for
 $\{ X_t\}$ is
\begin{equation}
 \label{eq:nonstatRep-spec}
  X_t = \sum_{h=0}^{d-1} A_{t} (h) \, X_{-h}  +
   \sum_{k=0}^{t-1} \xi_k \, \mu +
  \int_{-\pi}^{\pi}
   \frac{ e^{i \omega t} - \sum_{h=0}^{d-1} A_{t} (h) \,  e^{-i \omega h } 
    }{ \delta (e^{-i \omega}) } \, \ZZ (d\omega).
\end{equation}
\end{Theorem}  
  
\paragraph{Proof of Theorem  \ref{thm:nonstat-rep}.}  
  We begin by proving (\ref{eq:Identity1}), from which the other results follow.
 First write
 \[
  \xi (z) = {[ \xi (z) ]}_0^{t-1} + {[ \xi (z)]}_t^{\infty}
 \]
  and multiply by $\delta (z)$, yielding
\[
  {[ \xi (z) ]}_0^{t-1} \, \delta (z) = 1 -  {[ \xi (z)]}_t^{\infty}  \, \delta (z)
\]
via application of (\ref{eq:xi-def}).  Next, 
\begin{align*}
  {[ \xi (z)]}_t^{\infty}  \, \delta (z) & = 
     \sum_{\ell \geq t} \xi_{\ell} \, z^{\ell}
     \, \sum_{k=0}^d \delta_k \, z^k \\
  & =   \sum_{k, \ell \geq 0} \delta_k \, \xi_{\ell+t} \, z^{k+\ell+t}  \\
  & = z^t \, \sum_{h \geq 0}  \left( \sum_{\ell \geq 0} 
  \delta_{h-\ell} \, \xi_{\ell+t} \right) \, z^h \\
    & = z^t \, \sum_{h \geq 0}  \left( \sum_{k=0}^h 
  \delta_{k} \, \xi_{h+t-k} \right) \, z^h.
\end{align*}
 We see that the coefficient of $z^h$ is either $1_{ \{ h+t =0 \} }$ for $h \geq d$
  or equals $A_h (t)$ for $0 \leq h \leq d-1$.  Hence for $t \geq 1-d$ the calculation
  simplifies to
\[
 {[ \xi (z)]}_t^{\infty}  \, \delta (z) =  z^t \sum_{h=0}^{d-1} A_h (t) \, z^h,
\]
 from which  (\ref{eq:Identity1}) follows. 
 Next, multiply both sides of (\ref{eq:Identity1}) by $\delta (z)$, replace $z$ by $L$,
  and apply the resulting power series to $\{ X_t \}$.  Using
  $\delta (L) X_t = \partial X_t$, this yields
  \[
   \sum_{j=0}^{t-1} \xi_j \, \partial X_{t-j} = 
    X_t - \sum_{h=0}^{d-1} A_{h} (t) \, X_{-h},
\]
 from which (\ref{eq:nonstatCausalRep}) follows.  The spectral representation is
  obtained from (\ref{eq:nonstatCausalRep}) as follows:
\begin{align*}
 X_t  & = \sum_{h=0}^{d-1} A_{t} (h) \, X_{-h} + 
  \sum_{j=0}^{t-1} \xi_j \,  \left( \mu + 
  \int_{-\pi}^{\pi} e^{i \omega (t-j)}  \, \ZZ (d\omega) \right) \\
  & = \sum_{h=0}^{d-1} A_{t} (h) \, X_{-h} + 
  \sum_{j=0}^{t-1} \xi_j \, \mu  +
    \int_{-\pi}^{\pi} e^{i \omega t } \, \sum_{j=0}^{t-1} \xi_j \,e^{-i \ommega j}
    \, \ZZ (d\omega),
\end{align*}
  from which (\ref{eq:nonstatRep-spec}) follows.  $\quad \Box$
 
  
  Theorem  \ref{thm:nonstat-rep} shows how a non-stationary process
  can be represented in  terms of a predictable
  portion -- determined by the functions $A_{h} (t)$ and the
  variables $X_{1-d}, \ldots, X_{-1}, X_0$ -- and a
  non-predictable portion involving a time-varying filter of the $\{
  \partial X_t \}$ series. 
  The time-varying function $\sum_{k=0}^{t-1} \xi_k $ can be computed
  by evaluating (\ref{eq:Identity1}) at $z=1$ and dividing by $\delta (1)$
   so long as this is non-zero.  Otherwise, if $\delta (1) = 0$ we can
   use L'Hopital's rule to obtain
  \[
   \sum_{k=0}^{t-1} \xi_k = \sum_{k=0}^{t-1} \xi_k \, z^k \vert_{z = 1}
   = \frac{ - \sum_{h=0}^{d-1} A_h (t) \, (t+h) }{ \dot{\delta} (1)}.
  \]
     Each of the time-varying functions $A_h (t)$ is annihilated
 by $\delta (L)$, i.e., $\delta (L) A_{h} (t) = 0$ for $0 \leq
 h \leq d-1$.   As a consequence, we can rewrite each $A_{h} (t)$ as a linear
 combination of the basis functions of the null space of $\delta
 (L)$, which are given by $\zeta^{-t}$ for non-repeated roots $\zeta$
  of $\delta (z)$ (when the roots are repeated, we instead consider
  functions $t \, \zeta{-t}$, etc.).  
    Let the basis  functions be denotes 
    $\phi_j (t) $ for $1 \leq j \leq d$; see Brockwell and Davis (1991)
     for additional details about difference equations.  Then we can
 write $A_{h} (t) = \sum_{k=1}^d \alpha_{hk} \phi_k (t)$ for each $0 \leq
 h \leq d-1$, for some coefficients $\alpha_{hk}$.  It follows that
\[
 \sum_{h=0}^{d-1} A_{h}(t) \, z^h = \sum_{k=1}^d \left(
 \sum_{h=0}^{d-1} \alpha_{hk} z^h \right)  \, \phi_k (t).
\]
 Each expression in parentheses on the right hand side is a degree
 $d-1$ polynomial in $z$, and will henceforth be denoted as $p^{(k)}
 (z)$.   Substituting the new formulation, we obtain
\begin{equation}
\label{eq:nonstat.rep-basis}
 X_t = \sum_{k=1}^d \phi_k (t) p^{(k)} (L) \, X_{0} + 
  \sum_{k=0}^{t-1} \xi_k \, \mu  +  \int_{-\pi}^{\pi}
 \frac{ e^{i \omega t} - \sum_{k=1}^d \phi_k (t) \, p^{(k)} ( e^{-i \omega
 } )}{ \delta (e^{-i \omega}) } \; d \ZZ (\omega),
\end{equation}
 where $p^{(k)} (L)$ acts on $x_0$ by shifting the time index $t=0$
 back in time for each power of $L$.  
  This representation allows us to 
  understand the action of a filter on a non-stationary time series,
  as the following result demonstrates.
  
\begin{Proposition}
  \label{prop:filter-nonstat}
  The application of a filter $\Psi (L)$ to a non-stationary process $\{ X_t \}$
  with representation  (\ref{eq:nonstat.rep-basis}) has spectral representation
 \[
 \Psi (B) X_t =   \sum_{k=1}^d \Psi (L) \phi_k (t) \, p^{(k)} (L) \, X_{0}  +
   \sum_{k \geq 1}  \Psi (L)  \xi_{t-k} \, \mu +
  \int_{-\pi}^{\pi}
   \frac{ e^{i \omega t} \, \Psi (e^{-i \omega}) 
   - \sum_{k=1}^d \Psi (L) \phi_k (t) \, p^{(k)} (e^{-i \omega}) 
    }{ \delta (e^{-i \omega}) } \, \ZZ (d\omega).
 \]
 \end{Proposition}
  
  
   
   
 

   
  HERE material on WK : signal and noise spectra non-stat case, and basic formulas
    in freq domain
    
  HERE illustrations by LLM, STM, and Example 5 of E and S paper
  
  

\begin{Example} {\bf Model-Based Random Walk Trend.} \rm
\label{exam:trend-i1}
  The Local Level Model (LLM) discussed in Harvey (1989) is capable
  of modeling a time series consisting
 of a  random walk trend $\{ \mu_t \}$ and a     white noise irregular
 $\{ \iota_t \}$, such  that $X_t = \mu_t + \iota_t$.   
 Both the multivariate trend and  irregular are driven by independent 
 white noise processes, with respective covariance matrices
   $\Sigma_{\mu}$ and $\Sigma_{\iota}$,
 and the frf for the optimal trend extraction filter (McElroy and Trimbur, 2015) is
\[ 
 \Psi (e^{-i \omega}) = \Sigma_{\mu} \, { \left[ \Sigma_{\mu} + (2 - 2 \, \cos (\omega)) \, \Sigma_{\iota} \right] }^{-1}.
\]
 \end{Example}



\begin{Example} {\bf Model-Based Integrated Random Walk Trend.} \rm
\label{exam:trend-i2}
  Example \ref{exam:trend-i1} can be generalized to the Smooth 
  Trend Model (STM) developed in Harvey (1989),
 where now the trend $\{ \mu_t \}$ is an integrated random walk, 
 i.e., ${(1-L)}^2 \mu_t$ is white noise of
 covariance matrix $\Sigma_{\mu}$.   Then the frf for the optimal 
 trend extraction filter -- which also coincides
 with the multivariate HP filter (cf. McElroy and Trimbur, 2015) 
 -- is given by
\[ 
 \Psi (e^{-i \omega}) = \Sigma_{\mu} \, { \left[ \Sigma_{\mu} + {(2 - 2 \, \cos (\omega))}^2 \, \Sigma_{\iota} \right] }^{-1}.
\]
 The chief difference with the frf of the LLM is that the sinusoidal factor is now squared.  
\end{Example}




\begin{Example} {\bf Model-Based Seasonal Adjustment.} \rm
\label{exam:sa}
  Flexible structural models were discussed in McElroy (2017), 
  with atomic components for each distinct unit root
 (with any conjugate roots) in the differencing operator.  
 For monthly data  where $\delta (L) = (1-L)(1-L^{12})$,
 we obtain an integrated random walk trend component $\{ \mu_t \}$ 
 (identical to the trend discussed in Example \ref{exam:trend-i2})
 and six atomic seasonal components that combine into a single 
 seasonal component $\{ \xi_t \}$ with differencing
 operator $U(L) = 1 + L + L^2 + \ldots + L^{11}$, along with the
 irregular $\{ \iota_t \}$.  
  Six separate covariance matrices govern the dynamics of the seasonal
  component, allowing for different degrees of
 smoothness at each of the six seasonal frequencies.  The filter
 that suppresses the seasonal component $\{ \xi_t \}$
 and extracts trend $\{ \mu_t \}$ and irregular $\{ \iota_t \}$ is a model-based  seasonal adjustment filter, and is an 
 example of a multivariate WK filter.  
\end{Example}



\section{Error Criterion and Computation}
\label{sec:mdfa-nonstat}

HERE material from E and S paper section 4.3

% We here consider difference-stationary vector time series, which means there exists a scalar differencing polynomial $\delta (L)$ such
%  that $\partial X_t = \delta (L) X_t$ is mean zero and covariance stationary.  
%  Examination of (\ref{eq:dfa-error}) indicates that the error process is not stationary unless we make certain assumptions
%  about $\Delta (L) = \Psi (L) - \widehat{\Psi} (L)$.     It is necessary that we can factor $\delta (L)$ from $\Delta (L)$, i.e., there exists
%  $\widetilde{\Delta } (L)$ such that
% \begin{equation}
%  \label{eq:delta.factor}
%   \Delta (L) = \widetilde{\Delta } (L) \, \delta (L),
% \end{equation}
%  as otherwise we cannot guarantee that $\{ E_t \}$ will be stationary.  However, (\ref{eq:delta.factor}) is sufficient to guarantee
%  that the filter error be stationary, because
% \[
%   E_t = \widetilde{\Delta} (L) \, \partial X_t
% \]
%  in such a case.   We next discuss a set of filter constraints that guarantee (\ref{eq:delta.factor}), beginning with a lemma
%  that discusses factorization of filters.  We say a filter $\Psi (L)$ is absolutely convergent if $\sum_{j \in \ZZ} \| \psi (j) \| < \infty$
%  for a given matrix norm $\| \cdot \|$.
% 
% \begin{Proposition}
% \label{prop:filter-decompose}
%  Any linear filter $\Psi (L)$ can be expressed as
% \[
%   \Psi (L) = \Psi (\zeta) + (L - \zeta) \, \Psi^{\sharp} (L)
% \]
%  for any $\zeta \in \CC$  such that $| \zeta | = 1$, 
%   and an absolutely convergent filter $\Psi^{\sharp} (L)$, so long as  $\partial \Psi (L) $ is absolutely convergent.
%  If in addition $ \partial \partial \Psi (L) = \sum_{ j \in \ZZ} j (j-1) \, \psi (j) \, L^j$
%    is absolutely convergent, then there also exists an absolutely convergent filter $\Psi^{\flat} (L)$ 
%  such that
% \[
%  \Psi (L) = \Psi (\zeta) + \partial \Psi (\zeta) \, (L- \zeta) \, \overline{\zeta} + {(L - \zeta)}^2 \, \Psi^{\flat} (L).
% \]
% \end{Proposition}
% 
%  Note that if $\Psi (\zeta) = 0$, it follows from Proposition \ref{prop:filter-decompose} that $L-\zeta$ can be factored from
%   $\Psi (L)$.  Similarly, ${(L- \zeta)}^2$ can be factored from $\Psi (L)$ is $\Psi(\zeta) = \partial \Psi (\zeta) =0$.
% 
% \begin{Definition} \rm
% \label{def:filter-noise}
%  For $\omega \in [-\pi, \pi]$, a filter $\Psi (L)$ annihilates $\omega$-noise of order $1$ if $\Psi (e^{-i \omega}) = 0$,
%  and annihilates $\omega$-noise of order $2$ if in addition $\partial \Psi (e^{-i \omega}) = 0$.
% \end{Definition}
% 
% 
% Hence, we have the following immediate corollary of Proposition \ref{prop:filter-decompose}.
% 
% \begin{Corollary}
%  \label{cor:filter-noise}
%   If a filter $\Psi (L)$ annihilates $\omega$-noise of order $1$ and $\partial \Psi (L)$ is absolutely convergent, then
% \[
%   \Psi (L) = (L- e^{-i \omega}) \, \Psi^{\sharp} (L).
% \]
%  If a filter $\Psi (L)$ annihilate $\omega$-noise of order $2$,  and $\partial \partial \Psi (L)$ is absolutely convergent, then
% \[
%   \Psi (L) = {(L- e^{-i \omega}) }^2 \, \Psi^{\flat} (L).
% \]
% \end{Corollary}
% 
%  We can apply Corollary \ref{cor:filter-noise} to factor a noise-differencing polynomial $\delta^N (L)$ from $\Delta (L)$:
%  for each $\omega$ such that the target filter $\Psi (L)$ annihilate $\omega$-noise of order $d$, we impose the constraint
%  that $\widehat{\Psi} (L)$ shall have the same property, and hence ${(L- e^{-i \omega})}^d$ can be factored from both
%  filters.   For instance, if noise frequencies are $\omega_{\ell}$ with multiplicities $d_{\ell}$, then repeated application of 
%  Corollary \ref{cor:filter-noise} yields
% \[
%  \Psi (L) = \prod_{\ell} {(L -  e^{-i \omega_{\ell}})}^{d_{\ell}} \, \Psi^{\natural} (L)
%    = \delta^N (L) \, \Psi^{\star} (L)
% \]
%  for some residual filter $\Psi^{\natural} (L)$, where $\Psi^{\star} (L) = \prod_{\ell} -e^{-i \omega_{\ell} d_{\ell}} \, \Psi^{\natural} (L)$
%  and $\delta^N (L) = \prod_{\ell} (1 - e^{i \omega_{\ell}} \, L)$.
%  By imposing the same linear constraints on $\widehat{\Psi} (L)$, we likewise obtain $\widehat{\Psi} (L) = \delta^N (L) \, \widehat{\Psi}^{\star} (L)$,
%  and hence 
% \begin{equation}
%  \label{eq:delta-noise}
% \Delta (L) = \left(  {\Psi}^{\star} (L) - \widehat{\Psi}^{\star} (L) \right) \, \delta^N (L).
% \end{equation}
%   So if $\delta (L) = \delta^N (L)$, then (\ref{eq:delta.factor}) holds at once.  More generally, a given process' differencing polynomial
%  may be factored into relatively prime polynomials $\delta^N (z)$ and $\delta^S (z)$, which correspond to noise and signal dynamics
%  respectively -- see Bell (1984) and McElroy (2008a).  Many  signal extraction filters $\Psi (L)$   have the property that they
%  annihilate $\omega$-noise of the appropriate order, such that $\delta^N (L)$ can be factored; in addition, the noise filter $1_N - \Psi (L)$
%  has the same property with respect to the signal frequencies, i.e., $\delta^S (L)$ can be factored from $1_N - \Psi (L)$ in the same manner.
%  Hence  $1_N -  \Psi (L) =   \delta^S (L) \, \Psi^{\diamond} (L)$ for some factor $\Psi^{\diamond} (L)$,
%  and imposing the same constraints on the concurrent filter yields
% \[
%   \Delta (L) = (1_N - \widehat{\Psi} (L)) - (1_N - \Psi (L)) = \left(  \widehat{\Psi}^{\diamond} (L) - \Psi^{\diamond} (L)  \right) \, \delta^S (L).
% \]
%   However, (\ref{eq:delta-noise}) also holds, and the roots of $\delta^S (z)$ and $\delta^N (z)$ are distinct (because the polynomials
%  are relatively prime by assumption), and hence $\delta (L) = \delta^N (L) \, \delta^S (L)$ must be a factor.  Therefore,
%  $\widetilde{\Delta} (L) =  (  \widehat{\Psi}^{\diamond} (L) - \Psi^{\diamond} (L)   )/ \delta^N (L)$, and (\ref{eq:delta.factor}) holds.
% 
% In summary, given a factorization of $\delta (z)$ into signal and noise differencing polynomials, the noise constraints and signal constraints
%  on $\Psi (L)$ must also be imposed upon $\widehat{\Psi} (L)$, and this ensures that $\{ E_t \}$ will be stationary with mean zero.  
%  If $\omega$ satisfies $\delta^N (e^{-i \omega}) = 0$, then we impose that $\widehat{\Psi} (L)$ annihilates $\omega$-noise of order
%  given by the multiplicity of the root in $\delta^N (z)$.  Otherwise, if $\omega$ satisfies $\delta^S (e^{-i \omega})$ then we impose
%  that $\widehat{\Psi} (e^{-i \omega}) = \Psi (e^{-i \omega})$ (if the root is simple -- if a double root, then also impose that
%  $\partial \widehat{\Psi} (e^{-i \omega}) = \partial \Psi (e^{-i \omega})$).  In practice, we must determine the real and imaginary  parts of each such constraint, and write the corresponding constraints on $\widehat{\Psi} (L)$ in the form $A = [J \otimes 1_N] \, \vartheta$ for
%   filters of form (\ref{eq:conc.filter}), applying the methodology of the previous subsection.  
%   With these constraints in play, the formula (\ref{eq:dfa-mvar}) holds with $\Psi (z) - \widehat{\Psi} (z)$ replaced by $\widetilde{\Delta} (z)$
%  and $F$ being the spectral density of $\{ \partial X_t \}$, i.e., we define the nonstationary MDFA criterion 
%  function as $\det D_{\Psi } (\vartheta, G)$ for
% \begin{equation}
% \label{eq:mdfa-criterion-nonstat}
%  D_{\Psi} (\vartheta, G) =     { \langle  \widetilde{\Delta} (z)   \,   G \,  {\widetilde{\Delta} (z) }^*   \rangle }_0
%  = { \langle  \left[ \Psi (z) -   \widehat{\Psi}_{\vartheta} (z) \right] \,   G \, {|\delta (z) |}^{-2} \,
%   {  \left[ \Psi (z) -  \widehat{\Psi}_{\vartheta} (z) \right] }^{*} \rangle }_0.
% \end{equation}
%   The second expression in (\ref{eq:mdfa-criterion-nonstat}) utilizes (\ref{eq:delta.factor}), and employs the understanding
%  that poles in ${\delta (z) }^{-1}$ are exactly canceled out by the corresponding zeros in $\Psi (z) - \widehat{\Psi} (z)$.
%   Moreover, the ratio $(\Psi (z) - \widehat{\Psi} (z))/\delta (z) = \widetilde{\Delta} (z)$ is bounded in $\omega$ for $z = e^{-i \omega}$,
%  as the previous discussion guarantees.  As a matter of convenience, given that the frequencies of singularity in
%  ${|\delta (z) |}^{-2}$ are a set of Lebesgue measure zero, calculation of $D_{\Psi} (\vartheta, G)$ can proceed by using
%  the second expression, computing the numerical integration over only those frequencies where $\delta (z)$ is nonzero.
%   Whereas the theoretical filter error MSE is given by $D_{\Psi, F}$, with $F$ being the spectral density of $\{ \partial X_t \}$,
%  for estimation we approximate the integral over Fourier frequencies, and utilize the periodogram of the differenced data for $G$.
%  Again, we omit any contributions to the sum arising from Fourier frequencies that correspond to zeros of $\delta (z)$, as such an omission
%  only results in a loss of order $T^{-1}$.  (The alternative is to compute the quantities $\widetilde{\Delta} (z)$ at Fourier frequencies,
%  using the factorization results of Corollary  \ref{cor:filter-noise}; this is not worth the effort in practical applications.)

HERE revisit section 1 exercises now with RW and diff - VAR(1), using LTSC.  Petrol ex

   
% 
%   We seek to solve the corresponding trend extraction LPP.    First, we can use the optimal solution  (\ref{eq:var1.lpp-opt})
% given in   Illustration \ref{ill:var1}, supposing that we know  that the VAR($1$) is correctly specified.  
%  Second, we can use MDFA, proceeding as if we do not know the true process is a VAR($1$), as we would in practice, 
% and hence use the periodogram;  MDFA should be able to replicate the optimal solution, so long as the filter class
%  $\mathcal{G}$ is sufficiently rich.    The VAR($1$)  is defined by
% \[
%   X_t =  \left[ \begin{array}{ll}  1.0  & 0.5 \\    -0.2  &  0.3  \end{array} \right] \, X_{t-1} + \epsilon_t,
% \]
%  with stationary initialization, and $\{ \epsilon_t \}$ a Gaussian white noise of identity innovation variance.
%   Operationally, we simulate this process with sample size $4500$.  Then the 
%   ideal trends $\Psi (B) X_t$ are produced by truncating the  MB  filter to length $4001$ (it is symmetric, so the indices
%  range between $-2000$ and $2000$) and applying to the  simulation, only retaining the central $500$ data points,
%  as displayed in Figure \ref{fig:petrol.var1trends}.   (In this way we can dispense with edge effects, and the extra
%  $4000$ observations are not used in the MDFA.)     The grey lines of Figure \ref{fig:petrol.var1trends}
%  are the central $500$ observations of the VAR($1$)
%  simulation, and the  black  line is the target.     We wish to use MDFA (setting $q=30$) 
%  with various constraints (LC, TSC, LTSC) 
% to obtain a   real-time estimate,  comparing  the result  to the optimal solution given by implementing  Illustration \ref{ill:var1}.  
%   In that case  we find that
% \[
%   A_{\Psi} (\Phi) = \left[ \begin{array}{ll}  0.317  &  0.218 \\    -0.054  &  0.027  \end{array} \right]
% \]
%  by direct calculation, 
% and hence the optimal filter is easily computed.  The in-sample MSEs of the various methods are displayed in Table \ref{tab:petrol.var1.mdfa}.
%   Note that the basic MDFA (no constraints) replicates the optimal filter, as their MSE is the same up to negligible   error.
%   When imposing a level constraint (LC and LTSC) there is a loss to the MDFA performance, which makes sense given that the optimal
%  filter does {\it not} impose a level constraint -- in fact, the value of the optimal concurrent filter at frequency zero is
% \[
%   \widehat{\Psi} (1) =  \left[ \begin{array}{ll}  0.914 &  0.251 \\    -0.030  &  0.842  \end{array} \right],
% \]
%  which is quite different from $1_2$.  On the other hand, the time shift constraint alone (TSC) has little impact on the performance of MDFA,
%  because $\partial \widehat{\Psi} (1) \approx 0 \cdot 1_2$, i.e., the optimal filter already has this property of zero time shift.
% 
%  
% 
% \begin{table}[!htb]
% \centering
% \begin{tabular}{clllll}
% \hline
%   Series  &  LPP Opt  &  MDFA Basic &  MDFA LC &  MDFA TSC   &  MDFA LTSC \\
%   1 & .2404  &  .2388  &  .2877  &   .2363  & .4367  \\
%   2 & .0224 &   .0217 &   .0229  &   .0216  & .0264 \\
% \hline
% \end{tabular}
% \caption{\baselineskip=10pt  LPP MSE for bivariate VAR($1$) process --  with target trend
%  given by the LLM MB trend -- for various concurrent filters: LPP Opt is the optimal filter,
%  whereas the MDFA filters are labeled according to the constraints imposed. }
% \label{tab:petrol.var1.mdfa}
% \end{table}
%  

% Exercise 1
% 
% # Simulate a Gaussian VAR(1)  
% T.sim <- 500 + 2*len
% N <- 2
% phi.matrix <- rbind(c(1,.5),c(-.2,.3))
% innovar.matrix <- diag(N)
% true.psidelta <- var1.par2psi(phi.matrix,100)
% gamma.0 <- matrix(solve(diag(N^2) - phi.matrix %x% phi.matrix) %*% 
% 	matrix(innovar.matrix,ncol=1),nrow=N)
% x.init <- t(chol(gamma.0)) %*% rnorm(N)
% x.next <- x.init
% x.sim <- NULL
% for(t in 1:T.sim)
% {
% 	x.next <- phi.matrix %*% x.next + rnorm(N)
% 	x.sim <- cbind(x.sim,x.next)
% }
% x.sim <- ts(t(x.sim))
% x.acf <- acf(x.sim,type="covariance",plot=FALSE,lag.max=T.sim)[[1]]
% x.acf <- aperm(aperm(x.acf,c(3,2,1)),c(2,1,3))
% 
% # construct and apply BW filter
% bw.filter <- wk.trend[[1]]
% x.trend.bw11 <- filter(x.sim[,1],bw.filter[1,1,],method="convolution",sides=2)
% x.trend.bw12 <- filter(x.sim[,2],bw.filter[1,2,],method="convolution",sides=2)
% x.trend.bw21 <- filter(x.sim[,1],bw.filter[2,1,],method="convolution",sides=2)
% x.trend.bw22 <- filter(x.sim[,2],bw.filter[2,2,],method="convolution",sides=2)
% x.trend.bw <- cbind(x.trend.bw11 + x.trend.bw12,x.trend.bw21 + x.trend.bw22)
% x.trend.bw <- x.trend.bw[(len+1):(T.sim-len),] 
%  
% 
% # visualize
% pdf(file="petrolVAR1trends.pdf",width=4, height=4.6)
% par(oma=c(2,0,0,0),mar=c(2,4,2,2)+0.1,mfrow=c(2,1),cex.lab=.8)
% plot(ts(x.sim[(len+1):(T.sim-len),1]),ylab="Series 1",xlab="",yaxt="n",xaxt="n",col=grey(.7))
% axis(1,cex.axis=.5)
% axis(2,cex.axis=.5)
% lines(x.trend.bw[,1],col=1,lwd=2)
% #lines(ts(x.sim[(len+1):(T.sim-len),1]))
% plot(ts(x.sim[(len+1):(T.sim-len),2]),ylab="Series 2",xlab="",yaxt="n",xaxt="n",col=grey(.7))
% axis(1,cex.axis=.5)
% axis(2,cex.axis=.5)
% lines(x.trend.bw[,2],col=1,lwd=2)
% #lines(ts(x.sim[(len+1):(T.sim-len),2]))
% mtext("Time", side = 1, line = 1,outer=TRUE)
% dev.off()
% 
% # get LPP soln using true VAR(1)
% A.next <- diag(2)
% A.phi <- matrix(0,2,2)
% for(j in 1:len)
% {
% 	A.next <- A.next %*% phi.matrix
% 	A.phi <- A.phi + bw.filter[,,(len+1-j)] %*% A.next
% }
% bw.lpp <- bw.filter[,,(len+1):(2*len+1)]
% bw.lpp[,,1] <- bw.lpp[,,1] + A.phi
% 
% 
% # get MDFA concurrent filter
% q <- 30
% Grid <- T.sim
% frf.trend <- mdfa.frf(wk.trend[[1]],len,Grid)
% spec.hat <- mdfa.pergram(x.sim,1)
% # choose one of the four constraint scenarios for MDFA	
% bw.mdfa <- mdfa.unconstrained(frf.trend,spec.hat,q)
% bw.mdfa <- mdfa.levelconstraint(frf.trend,spec.hat,q)
% bw.mdfa <- mdfa.tsconstraint(frf.trend,spec.hat,q)
% bw.mdfa <- mdfa.ltsconstraint(frf.trend,spec.hat,q)
%  
% # apply the MDFA concurrent filter
% x.trend.mdfa11 <- filter(x.sim[,1],bw.mdfa[[1]][1,1,],method="convolution",sides=1)
% x.trend.mdfa12 <- filter(x.sim[,2],bw.mdfa[[1]][1,2,],method="convolution",sides=1)
% x.trend.mdfa21 <- filter(x.sim[,1],bw.mdfa[[1]][2,1,],method="convolution",sides=1)
% x.trend.mdfa22 <- filter(x.sim[,2],bw.mdfa[[1]][2,2,],method="convolution",sides=1)
% x.trend.mdfa <- cbind(x.trend.mdfa11 + x.trend.mdfa12,x.trend.mdfa21 + x.trend.mdfa22)
% x.trend.mdfa <- x.trend.mdfa[(len+1):(T.sim-len),] 
% 
% # construct and apply concurrent BW filter
% conc.filter <- conc.trend
% x.trend.conc11 <- filter(x.sim[,1],rev(conc.filter[1,1,]),method="convolution",sides=1)
% x.trend.conc12 <- filter(x.sim[,2],rev(conc.filter[1,2,]),method="convolution",sides=1)
% x.trend.conc21 <- filter(x.sim[,1],rev(conc.filter[2,1,]),method="convolution",sides=1)
% x.trend.conc22 <- filter(x.sim[,2],rev(conc.filter[2,2,]),method="convolution",sides=1)
% x.trend.conc <- cbind(x.trend.conc11 + x.trend.conc12,x.trend.conc21 + x.trend.conc22)
% x.trend.conc <- x.trend.conc[(len+1):(T.sim-len),] 
% 
% # construct and apply optimal LPP filter
% x.trend.lpp11 <- filter(x.sim[,1],bw.lpp[1,1,],method="convolution",sides=1)
% x.trend.lpp12 <- filter(x.sim[,2],bw.lpp[1,2,],method="convolution",sides=1)
% x.trend.lpp21 <- filter(x.sim[,1],bw.lpp[2,1,],method="convolution",sides=1)
% x.trend.lpp22 <- filter(x.sim[,2],bw.lpp[2,2,],method="convolution",sides=1)
% x.trend.lpp <- cbind(x.trend.lpp11 + x.trend.lpp12,x.trend.lpp21 + x.trend.lpp22)
% x.trend.lpp <- x.trend.lpp[(len+1):(T.sim-len),] 
% 
% 
% 
% # visualize
% par(oma=c(2,0,0,0),mar=c(2,4,2,2)+0.1,mfrow=c(2,1),cex.lab=.8)
% plot(ts(x.trend.bw[,1]),ylab="",xlab="",yaxt="n",xaxt="n",col=1,lwd=1,
% 	ylim=c(min(x.trend.bw[,1])-4,max(x.trend.bw[,1])))
% axis(1,cex.axis=.5)
% axis(2,cex.axis=.5)
% lines(x.trend.mdfa[,1]-2,col=grey(.4),lwd=1,lty=1)
% lines(x.trend.lpp[,1]-4,col=grey(.6),lwd=1,lty=1)
% plot(ts(x.trend.bw[,2]),ylab="",xlab="",yaxt="n",xaxt="n",col=1,lwd=1,
% 	ylim=c(min(x.trend.bw[,2])-4,max(x.trend.bw[,2])))
% axis(1,cex.axis=.5)
% axis(2,cex.axis=.5)
% lines(x.trend.mdfa[,2]-2,col=grey(.4),lwd=1,lty=1)
% lines(x.trend.lpp[,2]-4,col=grey(.6),lwd=1,lty=1)
% mtext("Time", side = 1, line = 1,outer=TRUE)
%  
% # compare in-sample performance: MDFA for series, and then Conc for series
% print(c(mean((x.trend.bw[,1] - x.trend.mdfa[,1])^2),
% 	mean((x.trend.bw[,2] - x.trend.mdfa[,2])^2)))
% print(c(mean((x.trend.bw[,1] - x.trend.conc[,1])^2),
% 	mean((x.trend.bw[,2] - x.trend.conc[,2])^2)))
% print(c(mean((x.trend.bw[,1] - x.trend.lpp[,1])^2),
% 	mean((x.trend.bw[,2] - x.trend.lpp[,2])^2)))
% 
% # compare to criterion value
% print(diag(bw.mdfa[[2]]))
% 
% 

% 
% 
% ##########################################################
% ### Exercise 2: replication - simulate bivariate LLM and apply MDFA
% 
% # Simulate a Gaussian LLM  
% psi.sim <- c(2.17150287559847+1i, -8.36795922528+1i, -6.04133725367594+1i, 
% 0.0648981656699+1i, -6.80849700177184+1i, -6.66004335288479+1i, 
% -0.00016098322952+1i, 0.00051984185863+1i)
% psi.sim[7:8] <- c(0+1i,0+1i)
% psi.sim <- sigex.par2psi(param.sim,flag.default,mdl)
% T.sim <- 500 + 2*len
% N <- 2
% burnin <- 0
% init <- matrix(rnorm(2*N),ncol=N)
% 
%  
% x.sim <- sigex.sim(psi.sim,mdl,T.sim,burnin,Inf,init)
% plot(ts(x.sim))
% 
% # construct and apply BW filter
% bw.filter <- wk.trend[[1]]
% x.trend.bw11 <- filter(x.sim[,1],bw.filter[1,1,],method="convolution",sides=2)
% x.trend.bw12 <- filter(x.sim[,2],bw.filter[1,2,],method="convolution",sides=2)
% x.trend.bw21 <- filter(x.sim[,1],bw.filter[2,1,],method="convolution",sides=2)
% x.trend.bw22 <- filter(x.sim[,2],bw.filter[2,2,],method="convolution",sides=2)
% x.trend.bw <- cbind(x.trend.bw11 + x.trend.bw12,x.trend.bw21 + x.trend.bw22)
% x.trend.bw <- x.trend.bw[(len+1):(T.sim-len),] 
% 
% # visualize
% pdf(file="petrolLLMtrendsNull.pdf",width=4, height=4.6)
% par(oma=c(2,0,0,0),mar=c(2,4,2,2)+0.1,mfrow=c(2,1),cex.lab=.8)
% plot(ts(x.sim[(len+1):(T.sim-len),1]),ylab="Consumption",xlab="",yaxt="n",xaxt="n",col=grey(.7))
% axis(1,cex.axis=.5)
% axis(2,cex.axis=.5)
% lines(x.trend.bw[,1],col=1,lwd=1)
% plot(ts(x.sim[(len+1):(T.sim-len),2]),ylab="Imports",xlab="",yaxt="n",xaxt="n",col=grey(.7))
% axis(1,cex.axis=.5)
% axis(2,cex.axis=.5)
% lines(x.trend.bw[,2],col=1,lwd=1)
% mtext("Time", side = 1, line = 1,outer=TRUE)
% dev.off()
% 
% # get MDFA concurrent filter for I(1) case
% q <- 30
% x.diff <- diff(x.sim)
% spec.hat <- mdfa.pergram(x.diff,c(1,-1))
% Grid <- T.sim - 1
% frf.trend <- mdfa.frf(wk.trend[[1]],len,Grid)
% #bw.mdfa <- mdfa.levelconstraint(frf.trend,spec.hat,q)[[1]]
% constraints.mdfa <- mdfa.getconstraints(frf.trend,0,NULL,q)
% bw.mdfa <- mdfa.filter(frf.trend,spec.hat,constraints.mdfa[[1]],constraints.mdfa[[2]])[[1]]
% 
% # apply the MDFA concurrent filter
% x.trend.mdfa11 <- filter(x.sim[,1],bw.mdfa[1,1,],method="convolution",sides=1)
% x.trend.mdfa12 <- filter(x.sim[,2],bw.mdfa[1,2,],method="convolution",sides=1)
% x.trend.mdfa21 <- filter(x.sim[,1],bw.mdfa[2,1,],method="convolution",sides=1)
% x.trend.mdfa22 <- filter(x.sim[,2],bw.mdfa[2,2,],method="convolution",sides=1)
% x.trend.mdfa <- cbind(x.trend.mdfa11 + x.trend.mdfa12,x.trend.mdfa21 + x.trend.mdfa22)
% x.trend.mdfa <- x.trend.mdfa[(len+1):(T.sim-len),] 
% 
% # construct and apply concurrent BW filter
% conc.filter <- conc.trend
% x.trend.conc11 <- filter(x.sim[,1],rev(conc.filter[1,1,]),method="convolution",sides=1)
% x.trend.conc12 <- filter(x.sim[,2],rev(conc.filter[1,2,]),method="convolution",sides=1)
% x.trend.conc21 <- filter(x.sim[,1],rev(conc.filter[2,1,]),method="convolution",sides=1)
% x.trend.conc22 <- filter(x.sim[,2],rev(conc.filter[2,2,]),method="convolution",sides=1)
% x.trend.conc <- cbind(x.trend.conc11 + x.trend.conc12,x.trend.conc21 + x.trend.conc22)
% x.trend.conc <- x.trend.conc[(len+1):(T.sim-len),] 
% 
% # visualize
% pdf(file="petrolLLMrealtimeNull.pdf",width=4, height=4.6)
% par(oma=c(2,0,0,0),mar=c(2,4,2,2)+0.1,mfrow=c(2,1),cex.lab=.8)
% plot(ts(x.trend.bw[,1]),ylab="Consumption",xlab="",yaxt="n",xaxt="n",col=1,lwd=1,
% 	ylim=c(min(x.trend.bw[,1])-2/2,max(x.trend.bw[,1])))
% axis(1,cex.axis=.5)
% axis(2,cex.axis=.5)
% lines(x.trend.mdfa[,1]-1/2,col=grey(.4),lwd=1,lty=1)
% lines(x.trend.conc[,1]-2/2,col=grey(.6),lwd=1,lty=1)
% plot(ts(x.trend.bw[,2]),ylab="Imports",xlab="",yaxt="n",xaxt="n",col=1,lwd=1,
% 	ylim=c(min(x.trend.bw[,2])-2,max(x.trend.bw[,2])))
% axis(1,cex.axis=.5)
% axis(2,cex.axis=.5)
% lines(x.trend.mdfa[,2]-1,col=grey(.4),lwd=1,lty=1)
% lines(x.trend.conc[,2]-2,col=grey(.6),lwd=1,lty=1)
% mtext("Time", side = 1, line = 1,outer=TRUE)
% dev.off()
%  
% # compare in-sample performance: MDFA for series, and then Conc for series
% print(c(mean((x.trend.bw[,1] - x.trend.mdfa[,1])^2),
% 	mean((x.trend.bw[,2] - x.trend.mdfa[,2])^2)))
% print(c(mean((x.trend.bw[,1] - x.trend.conc[,1])^2),
% 	mean((x.trend.bw[,2] - x.trend.conc[,2])^2)))
% 
% 



HERE  HP exercise with connection to STM, and MB replication.  Ndc ex

HERE CF apply to RW

HERE  Starts exercise
