 \chapter{Integrated Processes}\label{int_sec}


The univariate DFA was introduced in Chapter 2 for the case of a stationary time series.  
However, most economic series are non-stationary, requiring suitable differencing to be
rendered stationary.  Such time series are called {\it integrated processes}.  This chapter discusses the  MDFA methodology for integrated processes, discussing how
 filter constraints can be handled.
 
\section{Constrained Multivariate DFA}


Various constraints upon the concurrent filter can be envisioned,
 and imposing such strictures results in  a constrained MDFA.  
  Writing $\Delta (L) = \Psi (L) - \widehat{\Psi} (L)$ as the discrepancy filter, we see
 from (\ref{eq:dfa-error})   that $\EE [ E_t ] = \Delta (L) \, \EE [ X_t ]$; 
 by Definition \ref{def:lpp}, we require
 that $\EE [ E_t ] = 0$ for any LPP.  If $\EE [ X_t] = 0$
 then this condition is always satisfied, but with nonzero means 
 additional constraints on $\Delta (L)$ must be imposed, which implicitly 
   amount to constraints on $\widehat{\Psi} (L)$.
   To formulate these constraints, we define  the derivative of a filter 
    via $\partial \Psi (L) = \sum_{j \in \ZZ} j \, \psi(j) \, L^{j-1}$.
   
 The following results are well-known (Brockwell and Davis, 1991): 
 if $\EE [ X_t ]$ is constant but nonzero, then we require $\Delta (1) = 0$.
  If $\EE [ X_t ]$ is linear in $t$, then we require $\Delta (1) = 0$ and
  $\partial {\Delta} (1) = 0$.  Hence, we obtain
 three fundamental types of constraints: Level Constraint (LC), 
 Time-Shift Constraint (TSC), and Level and Time-Shift Constraint (LTSC).
  These are defined as follows:
\begin{align*}
 \mbox{LC} : &  \;  \Delta (1) = 0 \quad \mbox{or} \quad \Psi (1) = \widehat{\Psi} (1) \\
 \mbox{TSC} : &  \;   \partial {\Delta} (1) = 0 \quad \mbox{or} 
 \quad  \partial {\Psi} (1) = \partial {\widehat{\Psi}} (1)  \\
 \mbox{LTSC} : &  \;  \Delta (1) = 0,  \,  \partial {\Delta} (1) = 0 
    \quad \mbox{or} \quad
 \Psi (1) = \widehat{\Psi} (1), \; \partial {\Psi} (1) = \partial {\widehat{\Psi}} (1).
\end{align*}
 In the case of  concurrent filters of form  (\ref{eq:conc.filter}), 
 LC is accomplished by demanding that 
  $\sum_{j=0}^{q-1} \widehat{\psi} (j) = \Psi(1)$.  
  More generally, we consider  linear constraints  formulated via
  \begin{equation}
\label{eq:concurrent-constrain}
  \Xi = R \, \Phi + Q,
\end{equation}
 where $R$ is $n q \times n r$ and $\Phi$ is $n r \times n$ dimensional, consisting of 
 free parameters; $Q$ is a matrix of constants, and is $n q \times n$ dimensional.
 This is not the most general formulation (we could instead work 
  with $\mbox{vec} [ \vartheta^{\prime}]$,
 but is sufficient to describe LC, TSC, and LTSC.

\vspace{.5cm}

\noindent {\bf Level Constraint (LC).}  
  $\sum_{j=0}^{q-1} \widehat{\psi} (j) = \Psi(1)$ implies that
\begin{equation}
\label{eq:lc-gamma0}
 \widehat{\psi} (0) = \Psi(1) - \sum_{j=1}^{q-1} \widehat{\psi} (j).
\end{equation}
 Hence  $ \varphi^{\prime}  = [ \widehat{\psi} (1), \widehat{\psi} (2),
   \ldots, \widehat{\psi} (q-1) ] $ and
\[
	R  = \left[ \begin{array}{ccc} -1 & \ldots & -1 \\ 1 & 0 & 0 \\
		\vdots & \ddots & \vdots \\ 0 & 0 & 1  \end{array} \right]  \otimes 1_n \qquad
	Q = \left[ \begin{array}{c} \Psi (1) \\ 0 \\ \vdots \\ 0 \end{array} \right].
\]


\vspace{.5cm}

\noindent {\bf Time Shift Constraint (TSC).}    The constraint is
$\partial {\Psi} (1) = \partial \widehat{\Psi} (1)
   = \sum_{j=0}^{q-1} j \, \widehat{\psi} (j)$,
 or $\widehat{\psi} (1)  = \partial {\Psi} (1)  -  \sum_{j=2}^{q-1} j \, \widehat{\psi} (j) $.
 Hence  $ \varphi^{\prime}  = [ \widehat{\psi} (0), 
 \widehat{\psi} (2), \ldots, \widehat{\psi} (q-1) ] $ and
\[
	R  = \left[ \begin{array}{cccc} 1 & 0 &  \ldots &  0  \\  0 & -2  &  -3  & \ldots  \\
		0 & 1 & 0 & \ldots \\ 
		\vdots & \ddots & \vdots & \vdots \\ 0 & \ldots & 0 & 1 \end{array} \right] 
		\otimes 1_n \qquad
	Q = \left[ \begin{array}{c} 0 \\ \partial {\Psi} (1) \\ 0 \\ \vdots \\ 0 \end{array} \right].
\]


\vspace{.5cm}

\noindent {\bf Level and Time Shift Constraint (LTSC).}    
Take the Time Shift constraint formula for $\widehat{\psi} (1)$,
 and plug this into (\ref{eq:lc-gamma0}), to obtain
\begin{align*}
 \widehat{\psi} (0)  & = \Psi (1) - \left( \partial {\Psi} (1)  -  \sum_{j=2}^{q-1} j 
 \, \widehat{\psi} (j) \right) -  \sum_{j=2}^{q-1} 
 \widehat{\psi} (j)  \\
	& = \Psi (1) -  \partial {\Psi} (1)  +  \sum_{j=2}^{q-1} (j-1)  \, \widehat{\psi} (j).
\end{align*}
 Hence  $ \varphi^{\prime}  = [  \widehat{\psi} (2), \ldots, \widehat{\psi} (q-1)  ] $ and
\[
	R  = \left[ \begin{array}{cccc} 1 & 2  &  3  &   \ldots    \\  -2  & -3  &  -4  & \ldots  \\
		 1  & 0 & \ldots & 0 \\ 
		\vdots & \ddots & \vdots & \vdots \\ 0 & \ldots & 0 & 1 \end{array} \right] 
		\otimes 1_n \qquad
	Q = \left[ \begin{array}{c} \Psi (1) - \partial {\Psi} (1)  \\  \partial {\Psi} (1) \\ 0 \\ \vdots \\ 0 \end{array} \right].
\]
 
 
   
   %HERE
   
   
 More generally, we can envision an LPP involving $M$ linear constraints on each scalar filter in $\Xi$, taking the form
 $   K = [ J \otimes I_N ] \, \Xi$, where $J$ is $M \times q$ dimensional ($M < q$) and $K$ is $N M \times N$ dimensional.
 (The LC, TSC, and LTSC examples all have this form.)  In order to express this constraint in the form 
 (\ref{eq:concurrent-constrain}), we use the Q-R decomposition (Golub and Van Loan, 1996) of $J$, writing
 $J = C \, G \, \Pi$ for an orthogonal matrix $C$ (which is $M \times M$ dimensional), a rectangular upper triangular matrix $G$
 (which is $M \times q$ dimensional), and a permuation matrix $\Pi$ (which is $q \times q$ dimensional).  
 Standard matrix software will provide the Q-R decomposition $J$, and should produce the rank of $J$ as  a by-product --
 if this rank  is less than $M$, then there are redundancies in the constraints that should first be eliminated.  Hence
 proceeding with a full rank $J$, we partition $G$ as $G = [ G_1 \, G_2]$ such that $G_1$ has $M$ columns and $G_2$
 has $q-M$ columns.  This quantity $q-M$ corresponds to the number of free coefficient matrices, and is therefore the same as $r$.
 The Q-R decomposition guarantees that $G_1$ is an upper triangular matrix, and moreover it is invertible.  Therefore
\[
  \left[ G_1^{-1} \, C^{-1} \otimes I_N \right] \, K  = \left( \left[ I_M , \, G_1^{-1} \, G_2 \right] \, \Pi \otimes I_N  \right) \, \Xi,
\]
 and the action of $\Pi$ (together with the tensor product) amounts to a block-wise permutation of the elements of $\Xi$.
  Let the output of this permutation be denoted
\[
   { \left[ { \Xi^{\sharp} }^{\prime},  {  \Xi^{\flat} }^{\prime}  \right] }^{\prime} =
%   \left[ \begin{array}{l} \overline{\Xi} \\ \underline{\Xi} \end{array} \right] = 
 \left( \Pi \otimes I_N \right) \, \Xi,
\]
 where $ {\Xi}^{\sharp}$ is $N M \times N$ dimensional and $ {\Xi}^{\flat}$ is $N r \times N$ dimensional.  
 Then  by substitution we can solve for ${\Xi}^{\sharp}$ in terms of ${\Xi}^{\flat}$:
\[
  {\Xi}^{\sharp} =  \left[ G_1^{-1} \, C^{-1} \otimes I_N \right] \, K - \left[  G_1^{-1} \, G_2  \otimes I_N   \right] \, {\Xi}^{\flat}.
\]
 Therefore we recognize the free variables $\Phi = {\Xi}^{\flat}$, and obtain $R$ and $Q$ in 
(\ref{eq:concurrent-constrain}) via
\begin{align*}
   R & = \Pi^{-1} \, \left[ \begin{array}{c} - G_1^{-1} \, G_2 \\ I_{r} \end{array} \right] \otimes I_N  \\
  Q & = \left( \Pi^{-1}  \, \left[ \begin{array}{c}  G_1^{-1} \, C^{-1} \\ 0 \end{array} \right] \otimes I_N  \right) \, K.
\end{align*}
  These formulas allow one to compute the   form (\ref{eq:concurrent-constrain})  from given constraints, and
 an analytical solution to the resulting MDFA criterion  be obtained from the following result.

\begin{Proposition}
\label{prop:mdfa.quadsoln-constrain}
 The minimizer of the  MDFA criterion given by the determinant of  (\ref{eq:mdfa-criterion}),
 with respect to  $\mathcal{G}$ consists of all length $q$ concurrent filters subject to 
 linear constraints of the form (\ref{eq:concurrent-constrain}),  is
\begin{equation}
\label{eq:phi.soln-constained}
 \Phi =  { \left[ R^{\prime} \, B \, R \right] }^{-1} \, R^{\prime} \, \left( A - B \, Q \right).
\end{equation}
  Letting $H = I_{Nq} - R \,   { \left[ R^{\prime} \, B \, R \right] }^{-1} \, R^{\prime} \, B$, the minimal value is the determinant of
\begin{equation}
\label{eq:opt.val.mdfa-constrained}
{ \langle \Psi (e^{-i \omega}) \, G (\omega) \, { \Psi (e^{i \omega}) }^{\prime} \rangle }_0 -
 A^{\prime} \, R \, { \left[ R^{\prime} \, B \, R \right] }^{-1} \, R^{\prime} \,  A
	+ Q^{\prime} \, B \, H \, Q - 2 \, A^{\prime} \, H \, Q.
\end{equation}
\end{Proposition}

For computation, we utilize the same approximations to $B$ and $A$ as discussed in the previous subsection,
 obtaining the constrained MDFA filter $\Xi$ via (\ref{eq:phi.soln-constained}) followed by (\ref{eq:concurrent-constrain}).




\section{Integrated Multivariate Processes}


HERE:   basic discussion, and then some examples of LPPs arising from
 integrated processes.



\begin{Example} {\bf Model-Based Random Walk Trend.} \rm
\label{exam:trend-i1}
  The Local Level Model (LLM) discussed in Harvey (1989) is capable of modeling a time series consisting
 of a  random walk trend $\{ \mu_t \}$ and a     white noise irregular $\{ \iota_t \}$, such
 that $X_t = \mu_t + \iota_t$.    Both the multivariate trend and  irregular are driven by independent 
 white noise processes, with respective covariance matrices $\Sigma_{\mu}$ and $\Sigma_{\iota}$,
 and the frf for the optimal trend extraction filter (McElroy and Trimbur, 2015) is
\[ 
 \Psi (e^{-i \omega}) = \Sigma_{\mu} \, { \left[ \Sigma_{\mu} + (2 - 2 \, \cos (\omega)) \, \Sigma_{\iota} \right] }^{-1}.
\]
 \end{Example}



\begin{Example} {\bf Model-Based Integrated Random Walk Trend.} \rm
\label{exam:trend-i2}
  Example \ref{exam:trend-i1} can be generalized to the Smooth Trend Model (STM) developed in Harvey (1989),
 where now the trend $\{ \mu_t \}$ is an integrated random walk, i.e., ${(1-L)}^2 \mu_t$ is white noise of
 covariance matrix $\Sigma_{\mu}$.   Then the frf for the optimal trend extraction filter -- which also coincides
 with the multivariate HP filter (cf. McElroy and Trimbur, 2015) -- is given by
\[ 
 \Psi (e^{-i \omega}) = \Sigma_{\mu} \, { \left[ \Sigma_{\mu} + {(2 - 2 \, \cos (\omega))}^2 \, \Sigma_{\iota} \right] }^{-1}.
\]
 The chief difference with the frf of the LLM is that the sinusoidal factor is now squared.  
\end{Example}




\begin{Example} {\bf Model-Based Seasonal Adjustment.} \rm
\label{exam:sa}
  Flexible structural models were discussed in McElroy (2017), with atomic components for each distinct unit root
 (with any conjugate roots) in the differencing operator.  For monthly data  where $\delta (L) = (1-L)(1-L^{12})$,
 we obtain an integrated random walk trend component $\{ \mu_t \}$ (identical to the trend discussed in Example \ref{exam:trend-i2})
 and six atomic seasonal components that combine into a single seasonal component $\{ \xi_t \}$ with differencing
 operator $U(L) = 1 + L + L^2 + \ldots + L^{11}$, along with the irregular $\{ \iota_t \}$.  
  Six separate covariance matrices govern the dynamics of the seasonal component, allowing for different degrees of
 smoothness at each of the six seasonal frequencies.  The filter that suppresses the seasonal component $\{ \xi_t \}$
 and extracts trend $\{ \mu_t \}$ and irregular $\{ \iota_t \}$ is a model-based  seasonal adjustment filter, and is an 
 example of a multivariate WK filter.  
\end{Example}




\section{Non-stationary Multivariate DFA}


We here consider difference-stationary vector time series, which means there exists a scalar differencing polynomial $\delta (L)$ such
 that $\partial X_t = \delta (L) X_t$ is mean zero and covariance stationary.  
 Examination of (\ref{eq:dfa-error}) indicates that the error process is not stationary unless we make certain assumptions
 about $\Delta (L) = \Psi (L) - \widehat{\Psi} (L)$.     It is necessary that we can factor $\delta (L)$ from $\Delta (L)$, i.e., there exists
 $\widetilde{\Delta } (L)$ such that
\begin{equation}
 \label{eq:delta.factor}
  \Delta (L) = \widetilde{\Delta } (L) \, \delta (L),
\end{equation}
 as otherwise we cannot guarantee that $\{ E_t \}$ will be stationary.  However, (\ref{eq:delta.factor}) is sufficient to guarantee
 that the filter error be stationary, because
\[
  E_t = \widetilde{\Delta} (L) \, \partial X_t
\]
 in such a case.   We next discuss a set of filter constraints that guarantee (\ref{eq:delta.factor}), beginning with a lemma
 that discusses factorization of filters.  We say a filter $\Psi (L)$ is absolutely convergent if $\sum_{j \in \ZZ} \| \psi (j) \| < \infty$
 for a given matrix norm $\| \cdot \|$.

\begin{Proposition}
\label{prop:filter-decompose}
 Any linear filter $\Psi (L)$ can be expressed as
\[
  \Psi (L) = \Psi (\zeta) + (L - \zeta) \, \Psi^{\sharp} (L)
\]
 for any $\zeta \in \CC$  such that $| \zeta | = 1$, 
  and an absolutely convergent filter $\Psi^{\sharp} (L)$, so long as  $\partial \Psi (L) $ is absolutely convergent.
 If in addition $ \partial \partial \Psi (L) = \sum_{ j \in \ZZ} j (j-1) \, \psi (j) \, L^j$
   is absolutely convergent, then there also exists an absolutely convergent filter $\Psi^{\flat} (L)$ 
 such that
\[
 \Psi (L) = \Psi (\zeta) + \partial \Psi (\zeta) \, (L- \zeta) \, \overline{\zeta} + {(L - \zeta)}^2 \, \Psi^{\flat} (L).
\]
\end{Proposition}

 Note that if $\Psi (\zeta) = 0$, it follows from Proposition \ref{prop:filter-decompose} that $L-\zeta$ can be factored from
  $\Psi (L)$.  Similarly, ${(L- \zeta)}^2$ can be factored from $\Psi (L)$ is $\Psi(\zeta) = \partial \Psi (\zeta) =0$.

\begin{Definition} \rm
\label{def:filter-noise}
 For $\omega \in [-\pi, \pi]$, a filter $\Psi (L)$ annihilates $\omega$-noise of order $1$ if $\Psi (e^{-i \omega}) = 0$,
 and annihilates $\omega$-noise of order $2$ if in addition $\partial \Psi (e^{-i \omega}) = 0$.
\end{Definition}


Hence, we have the following immediate corollary of Proposition \ref{prop:filter-decompose}.

\begin{Corollary}
 \label{cor:filter-noise}
  If a filter $\Psi (L)$ annihilates $\omega$-noise of order $1$ and $\partial \Psi (L)$ is absolutely convergent, then
\[
  \Psi (L) = (L- e^{-i \omega}) \, \Psi^{\sharp} (L).
\]
 If a filter $\Psi (L)$ annihilate $\omega$-noise of order $2$,  and $\partial \partial \Psi (L)$ is absolutely convergent, then
\[
  \Psi (L) = {(L- e^{-i \omega}) }^2 \, \Psi^{\flat} (L).
\]
\end{Corollary}

 We can apply Corollary \ref{cor:filter-noise} to factor a noise-differencing polynomial $\delta^N (L)$ from $\Delta (L)$:
 for each $\omega$ such that the target filter $\Psi (L)$ annihilate $\omega$-noise of order $d$, we impose the constraint
 that $\widehat{\Psi} (L)$ shall have the same property, and hence ${(L- e^{-i \omega})}^d$ can be factored from both
 filters.   For instance, if noise frequencies are $\omega_{\ell}$ with multiplicities $d_{\ell}$, then repeated application of 
 Corollary \ref{cor:filter-noise} yields
\[
 \Psi (L) = \prod_{\ell} {(L -  e^{-i \omega_{\ell}})}^{d_{\ell}} \, \Psi^{\natural} (L)
   = \delta^N (L) \, \Psi^{\star} (L)
\]
 for some residual filter $\Psi^{\natural} (L)$, where $\Psi^{\star} (L) = \prod_{\ell} -e^{-i \omega_{\ell} d_{\ell}} \, \Psi^{\natural} (L)$
 and $\delta^N (L) = \prod_{\ell} (1 - e^{i \omega_{\ell}} \, L)$.
 By imposing the same linear constraints on $\widehat{\Psi} (L)$, we likewise obtain $\widehat{\Psi} (L) = \delta^N (L) \, \widehat{\Psi}^{\star} (L)$,
 and hence 
\begin{equation}
 \label{eq:delta-noise}
\Delta (L) = \left(  {\Psi}^{\star} (L) - \widehat{\Psi}^{\star} (L) \right) \, \delta^N (L).
\end{equation}
  So if $\delta (L) = \delta^N (L)$, then (\ref{eq:delta.factor}) holds at once.  More generally, a given process' differencing polynomial
 may be factored into relatively prime polynomials $\delta^N (z)$ and $\delta^S (z)$, which correspond to noise and signal dynamics
 respectively -- see Bell (1984) and McElroy (2008a).  Many  signal extraction filters $\Psi (L)$   have the property that they
 annihilate $\omega$-noise of the appropriate order, such that $\delta^N (L)$ can be factored; in addition, the noise filter $1_N - \Psi (L)$
 has the same property with respect to the signal frequencies, i.e., $\delta^S (L)$ can be factored from $1_N - \Psi (L)$ in the same manner.
 Hence  $1_N -  \Psi (L) =   \delta^S (L) \, \Psi^{\diamond} (L)$ for some factor $\Psi^{\diamond} (L)$,
 and imposing the same constraints on the concurrent filter yields
\[
  \Delta (L) = (1_N - \widehat{\Psi} (L)) - (1_N - \Psi (L)) = \left(  \widehat{\Psi}^{\diamond} (L) - \Psi^{\diamond} (L)  \right) \, \delta^S (L).
\]
  However, (\ref{eq:delta-noise}) also holds, and the roots of $\delta^S (z)$ and $\delta^N (z)$ are distinct (because the polynomials
 are relatively prime by assumption), and hence $\delta (L) = \delta^N (L) \, \delta^S (L)$ must be a factor.  Therefore,
 $\widetilde{\Delta} (L) =  (  \widehat{\Psi}^{\diamond} (L) - \Psi^{\diamond} (L)   )/ \delta^N (L)$, and (\ref{eq:delta.factor}) holds.

In summary, given a factorization of $\delta (z)$ into signal and noise differencing polynomials, the noise constraints and signal constraints
 on $\Psi (L)$ must also be imposed upon $\widehat{\Psi} (L)$, and this ensures that $\{ E_t \}$ will be stationary with mean zero.  
 If $\omega$ satisfies $\delta^N (e^{-i \omega}) = 0$, then we impose that $\widehat{\Psi} (L)$ annihilates $\omega$-noise of order
 given by the multiplicity of the root in $\delta^N (z)$.  Otherwise, if $\omega$ satisfies $\delta^S (e^{-i \omega})$ then we impose
 that $\widehat{\Psi} (e^{-i \omega}) = \Psi (e^{-i \omega})$ (if the root is simple -- if a double root, then also impose that
 $\partial \widehat{\Psi} (e^{-i \omega}) = \partial \Psi (e^{-i \omega})$).  In practice, we must determine the real and imaginary  parts of each such constraint, and write the corresponding constraints on $\widehat{\Psi} (L)$ in the form $A = [J \otimes 1_N] \, \vartheta$ for
  filters of form (\ref{eq:conc.filter}), applying the methodology of the previous subsection.  
  With these constraints in play, the formula (\ref{eq:dfa-mvar}) holds with $\Psi (z) - \widehat{\Psi} (z)$ replaced by $\widetilde{\Delta} (z)$
 and $F$ being the spectral density of $\{ \partial X_t \}$, i.e., we define the nonstationary MDFA criterion 
 function as $\det D_{\Psi } (\vartheta, G)$ for
\begin{equation}
\label{eq:mdfa-criterion-nonstat}
 D_{\Psi} (\vartheta, G) =     { \langle  \widetilde{\Delta} (z)   \,   G \,  {\widetilde{\Delta} (z) }^*   \rangle }_0
 = { \langle  \left[ \Psi (z) -   \widehat{\Psi}_{\vartheta} (z) \right] \,   G \, {|\delta (z) |}^{-2} \,
  {  \left[ \Psi (z) -  \widehat{\Psi}_{\vartheta} (z) \right] }^{*} \rangle }_0.
\end{equation}
  The second expression in (\ref{eq:mdfa-criterion-nonstat}) utilizes (\ref{eq:delta.factor}), and employs the understanding
 that poles in ${\delta (z) }^{-1}$ are exactly canceled out by the corresponding zeros in $\Psi (z) - \widehat{\Psi} (z)$.
  Moreover, the ratio $(\Psi (z) - \widehat{\Psi} (z))/\delta (z) = \widetilde{\Delta} (z)$ is bounded in $\omega$ for $z = e^{-i \omega}$,
 as the previous discussion guarantees.  As a matter of convenience, given that the frequencies of singularity in
 ${|\delta (z) |}^{-2}$ are a set of Lebesgue measure zero, calculation of $D_{\Psi} (\vartheta, G)$ can proceed by using
 the second expression, computing the numerical integration over only those frequencies where $\delta (z)$ is nonzero.
  Whereas the theoretical filter error MSE is given by $D_{\Psi, F}$, with $F$ being the spectral density of $\{ \partial X_t \}$,
 for estimation we approximate the integral over Fourier frequencies, and utilize the periodogram of the differenced data for $G$.
 Again, we omit any contributions to the sum arising from Fourier frequencies that correspond to zeros of $\delta (z)$, as such an omission
 only results in a loss of order $T^{-1}$.  (The alternative is to compute the quantities $\widetilde{\Delta} (z)$ at Fourier frequencies,
 using the factorization results of Corollary  \ref{cor:filter-noise}; this is not worth the effort in practical applications.)


\section{Replicating and Generalizing Model-Based Solutions}
  
  
     


