\def\pf{{\bf Proof. }}
\def\logimplies{\Rightarrow}
\def\convinlaw{\stackrel{{\cal L}}{\Longrightarrow }}
\def\convinp{\stackrel{P}{\longrightarrow }}
\def\convas{\stackrel{a.s.}{\longrightarrow }}
\def\convv{\stackrel{v}{\longrightarrow}}
\def\asymp{\stackrel{{\mathbb P}}{\sim}}
\def\RR{\mathbb R}
\def\ZZ{\mathbb Z}
\def\QQ{\mathbb Q}
\def\NN{\mathbb N}
\def\MM{\mathbb M}
\def\LL{\mathbb L}
\def\EE{\mathbb E}
\def\PP{\mathbb P}
\def\DD{\mathbb D}
\def\WW{\mathbb W}
\def\FF{\mathbb F}
\def\II{\mathbb I}
\def\FF{\mathbb F}
\def\ttheta{\widetilde{\theta}}
\def\tTheta{\widetilde{\Theta}}
\def\tsig{\widetilde{\sigma}^2}
\def\tc{\widetilde{c}}
\def\etheta{\widehat{\theta}}
\def\eTheta{\widehat{\Theta}}
\def\esig{\widehat{\sigma}^2}
\def\ptheta{\underline{\theta}}
\def\pTheta{\underline{\Theta}}
\def\psig{\underline{\sigma}^2}

\def\eqinlaw{\stackrel{{\cal L}}{=}}
\def\tends{\rightarrow}
\def\tendsinf{\rightarrow\infty}
\def\isodynamo{\Leftrightarrow}

\chapter{Integrated Processes}\label{int_sec}


The univariate DFA was introduced in Chapter 2 for the case of a stationary time series.  
However, most economic series are non-stationary, requiring suitable differencing to be
rendered stationary.  Such time series are called {\it integrated processes}.  This chapter discusses the
DFA methodology for multivariate integrated processes.  We also give a thorough and novel treatment
of co-integration, and how this is related to DFA filter constraints.

\section{Stationary Multivariate DFA}

Here we set forth our basic treatment of the stationary multivariate DFA, which builds on the univariate material 
of previous chapters.  Moreover, we distinguish carefully between theoretical and empirical versions of the DFA
criteria.  Consider an $m$-variate time series $\{ x_t \}$  with component
series $\{ x_t^{(j)} \}$ for $1 \leq j \leq m$.    Let our target
signal be defined as $y_t = \Gamma (B) x_t$, where $\Gamma (z)$ is a $1
\times m$ dimensional multivariate filter.  Let the component 
filters be denoted $\Gamma^{(j)} (B)$, so that
\[
  y_t = \sum_{j=1}^m \Gamma^{(j)} (B) x_t^{(j)},
\]
 where each $\Gamma^{(j)} (B)$ is a univariate filter that operates on the $j$th component input series. 
 The real-time estimation problem is to find $m$ univariate concurrent
filters $\widehat{\Gamma}^{(j)} (B)$ to causally estimate the signal via
\[
 \widehat{y}_t = \sum_{j=1}^m \widehat{\Gamma}^{(j)} (B) x_t^{(j)}.
\]
 Together the concurrent filters form a single multivariate filter
 $\widehat{\Gamma} (B)$ that is $1 \times m$ dimensional.  We seek concurrent
 filters such that the univariate filter error process $y_t - \widehat{y}_t$ is
 stationary with minimal variance. 

The coefficients of the filters are denoted $\{ \gamma_k \}$ and $\{ \widehat{\gamma}_k \}$ respectively for 
the target and real-time cases.  These are each $1 \times m$ dimensional matrices, with a superscript denoting the particular component.
Thus, $\gamma_k^{(j)}$ weights the $j$th series, at $k$ time units in the past of the present time of consideration, and
$\widehat{\gamma}_k^{(j)}$ is similarly defined for the real-time estimator.

 Recall that in the univariate DFA (Chapter 4), the concurrent filters may satisfy certain filter constraints
 that are imposed by the user.  As we discuss below, some of these constraints may be necessary
when the input process is integrated, in order that the filter error be stationary and mean zero.
 Because $\{ x_t \}$ is stationary, there exists a spectral representation in terms of a vector orthogonal increments
process $\ZZ (\omega)$, as described in Brockwell and Davis (1991):
\[
  x_t = \int_{-\pi}^{\pi} e^{i \omega t} \, d\ZZ (\omega).
\]
  The orthogonal increments process is a random measure with the uncorrelated increments property, 
 which says that for any two disjoint subsets $A_1, A_2 \subset [-\pi, \pi]$, the random vectors
$\ZZ (A_1)$ and $\ZZ (A_2)$ are mean zero and uncorrelated with one another.  Moreover, for any such set $A$
we have
\begin{equation}
 \label{eq:chapnstat_varoi}
  \mbox{Var} \ZZ (A) = \EE \ZZ(A) \overline{\ZZ (A)}^{\prime} = \frac{1}{2 \pi} \int_{A} dH(\omega),
\end{equation}
 where $H$ is the spectral distribution function of the process $\{ x_t \}$.  When the process is purely nondeterministic (which 
we henceforth assume), the spectral distribution is absolutely continuous in each of its $m^2$ component functions, and its derivative 
$h$ is called the spectral density.  It is a matrix-valued function of frequency $\omega \in [-\pi, \pi]$.

It is convenient to introduce notation for the integration in (\ref{eq:chapnstat_varoi}):
\[
 \langle g \rangle =  \frac{1}{2 \pi} \int_{-\pi}^{\pi} g(\omega) \, d\omega
\]
 for any function $g$; hence $\mbox{Var} \ZZ (A)$ can be rewritten $\langle 1_A \, h \rangle$.  The autocovariances $R (j)$ of 
the process have the relationship
\[
  R(j) = \EE [ x_t x_{t-j}^{\prime} ] = \langle e^{i j\cdot } H \rangle.
\]
  Applying the filter $\Gamma (B)$ and using the linearity of the spectral representation, we obtain
\[
  y_t = \int_{-\pi}^{\pi} e^{i \omega t} \, \Gamma (e^{-i \omega}) \, d\ZZ (\omega).
\]
  Recall that $\Gamma (e^{-i \omega})$ is the frequency response function (frf) of the filter.  For the real-time filter, we also have
\[
  \widehat{y}_t = \int_{-\pi}^{\pi} e^{i \omega t} \, \widehat{\Gamma} (e^{-i \omega}) \, d\ZZ (\omega).
\]
 In this case, the real-time filter $\widehat{\Gamma} (B)$ is causal, so its frf will typically be complex-valued.  This differs from the target frf,
 which in many cases is a symmetric filter, and hence real-valued.

Next, the real-time estimation error is 
\[
  y_t  - \widehat{y}_t = \int_{-\pi}^{\pi} e^{i \omega t} \, \left( \Gamma (e^{-i \omega}) -  \widehat{\Gamma} (e^{-i \omega}) \right) \, d\ZZ (\omega).
\]
  Note that the integrand only depends on $\omega$ through $e^{-i \omega}$.  It is therefore convenient to use the abbreviation $z = e^{-i \omega}$.
  Although the phase properties of this error row vector may be of interest, in DFA we focus on the mean squared error, which works out to be
\begin{equation}
 \label{eq:chapnstat_dfamvar}
 \EE (y_t  - \widehat{y}_t) {(y_t  - \widehat{y}_t)}^{\prime} = \langle  \left( \Gamma (z) -  \widehat{\Gamma} (z) \right) \, h \,
  {  \left( \Gamma (\overline{z}) -  \widehat{\Gamma} (\overline{z}) \right) }^{\prime} \rangle.
\end{equation}
  Pretending that the spectral density $h$ is known, we could use (\ref{eq:chapnstat_dfamvar}) as a criterion to obtain an optimal real-time
approximation to the given $\Gamma$.  The optimization would be computed over a particular class of concurrent filters.  

For an illustration (that generalizes the earlier univariate treatments), suppose we consider $\widehat{\Gamma}$ from the space of causal length $L$ filters, with no constraint on the numerical values of
the filter coefficients other than they are real numbers.  Write these coefficients as a single vector $\phi$ as follows:
\[
  \phi^{\prime} = [ \widehat{\Gamma}_0, \widehat{\Gamma}_1, \cdots, \widehat{\Gamma}_{L-1} ].
\]
 So $\phi$ is a column vector of length $Lm$.  Then the criterion (\ref{eq:chapnstat_dfamvar}) can be rewritten as
\[
 \mathcal{L} (\phi) = \phi^{\prime} \, A \, \phi - \phi^{\prime} \, b - \overline{b^{\prime}} \, \phi + \langle \Gamma (z) \, h \, \Gamma^{\prime} (\overline{z}) \rangle,
\]
 where 
\[
  \overline{b^{\prime}}  = [ \langle \Gamma (z) \, h \rangle, \langle \Gamma (z) \, h \overline{z} \rangle, \cdots, \langle \Gamma (z) \, h \overline{z}^{L-1} \rangle ] 
\]
 and $A$ is a block matrix consisting of the autocovariances $R (j)$ of the data process.  Because this objective function is a quadratic in $\phi$, we obtain the minimizer
by an analytic formula, namely
\[
 \phi = A^{-1} \, \Re (b).
\]
  This follows from $\nabla \mathcal{L} (\phi) = -b - \overline{b} + 2 A \phi$.  

However, the discussion so far has involved the true spectral density, which in practice is unknown.  In that case, one just substitutes some estimate of $h$, typically
 either the periodogram or a tapered version of such, or a model-based estimator of $h$.  For example, one could fit a high order Vector Autoregression (VAR) to the data to 
obtain a plug-in estimator of $h$.  In this chapter we focus upon the periodogram.

In keeping with previous notation, the DFT is $T^{-1/2} \sum_{t=1}^T x_t \exp \{ - i t \omega \}$, denoted $\Xi_{T X} (\omega)$.  Taking the conjugate outer product of this quantity yields the periodogram, a rank one matrix:
\[
  I_{TX} (\omega) = \Xi_{TX} (\omega) {\Xi_{TX} (-\omega)}^{\prime}.
\]
  It is a simple exercise to show that
\[
  \widehat{R}_j = T^{-1} \sum_{t=1}^{T-j} x_{t+j} x_t^{\prime} = \langle I_{TX} \overline{z}^j \rangle
\]
 for $j \geq 0$.  With these estimators, it is possible to plug directly into the same multivariate DFA equations, obtaining the empirical version of (\ref{eq:chapnstat_dfamvar}):
\[
   \langle  \left( \Gamma (z) -  \widehat{\Gamma} (z) \right) \, I_{TX} \,
  {  \left( \Gamma (\overline{z}) -  \widehat{\Gamma} (\overline{z}) \right) }^{\prime} \rangle.
 \]
 In the previous example, this has the effect of replacing $A$ with a matrix of sample autocovariances, and updating the entries of the $b$ vector with quantities based on the 
periodogram.  But the final solution has the same form.

The basic treatment of length $L$ concurrent filters can be extended to handle filter constraints.  The general problem still involves optimization of $\mathcal{L} (\phi)$, but now
$\phi$ is subject to linear constraints of the form
\[
   \phi = R \, \varphi + c,
\]
 where $R$ and $c$ are known quantities.  Essentially, we end up optimizing over $\varphi$ instead.  Plugging in, we obtain
\begin{align*}
  \mathcal{L} (\varphi) & = \varphi^{\prime} \, R^{\prime} \, A \, R \, \varphi - \phi^{\prime} \,  R^{\prime} \, [ b - A \, c ] 
 - [ \overline{b^{\prime}} - c^{\prime} \, A ] \, R \, \varphi + \langle \Gamma (z) \, h \, \Gamma^{\prime} (\overline{z}) \rangle \\
  & + c^{\prime} \, A \, c - c^{\prime} \, b - \overline{b}^{\prime} \, c.
\end{align*}
 The minimum of this function has the formula
\[
 \varphi = {[ R^{\prime} \, A \, R ]}^{-1} \, R^{\prime} \, \left( \Re (b) - A \, c \right).
\]
In Chapter 4, the level constraint $i1$ was studied, which in the multivariate context states that
\[
  \Gamma (1) = \widehat{\Gamma} (1).
\]
 By writing $\widehat{\Gamma}_0$ in terms of the other free coefficients, we can express the level constraint in terms of the general linear constraint framework.  We have
 $\widehat{\gamma}_0 = \Gamma (1) - \sum_{\ell=1}^{L-1} \widehat{\gamma}_{\ell}$, so that
\begin{align*}
 \varphi^{\prime} & = [ \widehat{\gamma}_1, \widehat{\gamma}_2, \cdots, \widehat{\gamma}_{L-1} ] \\
 c^{\prime} & = [ { \Gamma (1) }^{\prime}, 0, \cdots ] 
\end{align*}
 and the constraint matrix is
\[
 R  = \left[ \begin{array}{lll} -1_m & \cdots & -1_m \\ 1_m & 0 & 0 \\ \vdots & \vdots & \vdots \\ 0 & 0 & 1_m \end{array} \right].
\]
 Alternatively, the time-shift constraint $i2$ is expressed as $\dot{\Gamma} (1) = \dot{\widehat{\Gamma}} (1)$, which holds if and only if
\[
   \sum_{\ell} \ell \gamma_{\ell} = \sum_{\ell =0}^{L-1} \ell \widehat{\gamma}_{\ell}.
\]
 Solving for $\widehat{\gamma}_1$ in terms of the other coefficients gives $\widehat{\gamma}_1 = \dot{\Gamma} (1) - \sum_{\ell=2}^{L-1} \ell \widehat{\gamma}_{\ell}$, 
so that
\begin{align*}
 \varphi^{\prime} & = [ \widehat{\gamma}_0, 0, \widehat{\gamma}_2, \cdots, \widehat{\gamma}_{L-1} ] \\
 c^{\prime} & = [ 0, { \dot{\Gamma} (1) }^{\prime}, 0, \cdots ] 
\end{align*}
 with constraint matrix 
\[
 R  = \left[ \begin{array}{llll} 1_m & 0 & 0 & \cdots \\ 0 & -2 1_m & -3 1_m & \cdots \\ 0 & 1_m & 0 & \cdots \\
  0 & 0 & 1_m & 0 \\
  \vdots & \vdots & \vdots & \vdots  \end{array} \right].
\]
 Finally, we might consider imposing both $i1$ and $i2$ constraints, which is equivalent to
\begin{align*}
  \widehat{\gamma}_1 & = \sum_{\ell} \ell \gamma_{\ell} - \sum_{\ell=2}^{L-1} \ell \widehat{\gamma}_{\ell} \\
 \widehat{\gamma}_0 & = \sum_{\ell}  (\ell - 1) \gamma_{\ell} + \sum_{\ell=2}^{L-1} (\ell -1)  \widehat{\gamma}_{\ell}.
\end{align*} 
  Then the linear constraint quantities are expressed
\begin{align*}
 \varphi^{\prime} & = [  \widehat{\gamma}_2, \cdots, \widehat{\gamma}_{L-1} ] \\
 c^{\prime} & = [ \sum_{\ell} (\ell-1)  \gamma_{\ell}^{\prime},  \sum_{\ell} \ell \gamma_{\ell}^{\prime}, 0, \cdots ] 
\end{align*}
 with constraint matrix 
\[
 R  = \left[ \begin{array}{llll} 1_m & 2 1_m  & 3 1_m & \cdots  \\ -2 1_m & -3 1_m & -4 1_m & \cdots \\
  1_m & 0 & \cdots & \cdots \\  0 & 1_m & 0& \cdots \\
\vdots & \vdots & \vdots \\ 0 & 0 & 1_m \end{array} \right].
\]
  These are the main cases of interest for stationary multivariate series.  The resulting concurrent filters are denoted as moving average MDFA filters.


\section{Non-stationary Multivariate Processes}


We next consider processes that when differenced are 
stationary, which are the most common type occuring in econometrics and finance.   The general treatment of co-integration is complicated when
 multiple unit roots are present.  For example, if there are trend
 and seasonal roots present, application of a co-integrating vector
 to the data process may only reduce the order of non-stationarity
 somewhat, rather than making the series stationary. 

 We first illustrate this point through dynamic factor component models.  Let
 the differencing polynomial be $\delta (B) = \prod_{\ell=1}^p {(1 -
 e^{i \omega_{\ell}} B )}^{q_{\ell}}$, where $q_{\ell}$ is the
 multiplicity of each unit root at frequency $\omega_{\ell}$.  When $\omega_{\ell}$ is not $0$ or $\pi$,
 we know a conjugate factor must appear in $\delta (B)$.  By a convenient abuse of notation, we denote
the pairing of such conjugate factors by ${(1 - e^{i \omega_{\ell} } B)}^{q_{\ell}}$, with the understanding that
 $q_{\ell} $ is even and denotes the produce of conjugate factors.

Suppose that the data process can be written as the sum of
 non-stationary latent processes, each of which has differencing
 polynomial ${(1 - e^{i \omega_{\ell}} B )}^{q_{\ell}}$, plus a
 residual stationary process.  We write this as
\begin{equation}
 \label{eq:chapnstat_structural}
 x_t = \sum_{\ell=1}^p s^{(\ell)}_t + s^{(0)}_t,
\end{equation}
 where ${(1 - e^{i \omega_{\ell}} B )}^{q_{\ell}} s^{(\ell)}_t$ is
 stationary for each $1 \leq \ell \leq p$, and $s^{(0)}_t$ is
 stationary as well.  Let the
 reduced polynomials $\delta^{(\ell)} (B) = \delta (B) \, {(1 -
 e^{i \omega_{\ell}} B )}^{-q_{\ell}}$ be defined.  Then applying
 $\delta (B)$ to the structural equation (\ref{eq:chapnstat_structural}) yields
\[
 \partial x_t  : = \delta^{(\ell)} (B) x_t = \sum_{\ell=1}^p \, \delta^{(\ell)} (B) \partial
 s^{(\ell)}_t + \delta (B) s^{(0)}_t.
\]
 Here the $\partial$ notation before a process refers to the
 suitably differenced version of that process, which is stationary.
  Each stationary latent process $\partial s^{(\ell)}_t$ may have
  singularities in its spectral density matrix, such that it can be
  represented as $\Lambda^{(\ell)}$ times some $c^{(\ell)}_t$, a
 stationary process of reduced dimension with spectral density
 matrix invertible at all frequencies.  Such a latent process is
 governed by a dynamic factor model (DFM), with $\Lambda^{(\ell)} =
 I_m$ recovering the general case.  We actually require
 $\Lambda^{(0)} = I_m$ in order to guarantee that the spectrum of
 $\partial x_t$ is non-singular except at a finite number of
 frequencies.

 Suppose that $\beta$ is a vector such that $\beta^{\prime}
 \Lambda^{(k)} = 0$ for some $1 \leq k \leq p$.  Then
\[
 \beta^{\prime} \, \partial x_t = \sum_{\ell \neq k} \,
 \beta^{\prime} \Lambda^{(\ell)} \, \delta^{(\ell)} (B)
 c^{(\ell)}_t + \beta^{\prime} \delta (B) s^{(0)}_t,
\]
 and note that ${(1 - e^{i \omega_{k}} B )}^{q_{k}}$ can be factored
 out of all terms on the right hand side.  Hence $\beta^{\prime}
 x_t$ only requires $\delta^{(k)} (B)$ differencing to become
 stationary; the frequency $\omega_k$ co-integrating vector $\beta$
 reduces the order of non-stationarity by the factor ${(1 - e^{i \omega_{k}} B
 )}^{q_{k}}$.  Moreover, if $\beta$ is in the left null space of
 several factor loadings $\Lambda^{(\ell)}$, the order of
 non-stationarity can be reduced further.  In an extreme case,
 $\beta^{\prime} \Lambda^{(\ell)} = 0$ for $1 \leq \ell \leq p$, so
 that $\beta^{\prime} x_t$ is stationary; however, whether or not
 the factor loadings have a non-trivial intersection of left null
 space depends on each process.

 We now proceed with a general treatment of vector non-stationary
 processes to explore what types of filter constraints are necessary
 when co-integrating vectors are present.  Crucially supposing that the
 differencing operator $\delta (B)$ is the same for each component
 series, we can write
\[
 x_t = \sum_{j=1}^d A_{j, t+d} \, x_{j-d} + \int_{-\pi}^{\pi}
 \frac{ e^{i \omega t} - \sum_{j=1}^d A_{j,t+d} \, e^{-i \omega
 (d-j)} }{ \delta (e^{-i \omega}) } \; d \ZZ (\omega),
\]
 where $d = \sum_{\ell=1}^m q_{\ell}$ and each $A_{j, t+d}$ is a
 time varying function for each $j$, and the $x_{j-d}$ are initial
 values.  This representation is chiefly useful when $t > 1$, though
 it is still valid when $t \leq 0$.  The $\ZZ (\omega)$ is the
 orthogonal increments process in the spectral representation of
 $\partial x_t$. 

 Each of the time-varying functions is in the null
 space of $\delta (B)$, i.e., $\delta (B) A_{j, t+d} = 0$ for $1 \leq
 j \leq d$, where the backshift operator works on the $t+d$ index.
 As a consequence, we can rewrite each $A_{j,t+d}$ as a linear
 combination of the basis functions of the null space of $\delta
 (B)$, which yields a more convenient representation.  Let the basis
 functions be $\phi_j (t) $ for $1 \leq j \leq d$; the existence and
 form of such functions are a basic staple of difference equation
 theory, treated briefly in Brockwell and Davis (1991).  Then we can
 write $A_{j,t+d} = \sum_{k=1}^d G_{jk} \phi_k (t)$ for each $1 \leq
 j \leq d$, for some coefficients $G_{jk}$.  It follows that
\[
 \sum_{j=1}^d A_{j,t+d} B^{d-j} = \sum_{k=1}^d \phi_k (t)  \; \left(
 \sum_{j=1}^d G_{jk} B^{d-j} \right).
\]
 Each expression in parentheses on the right hand side is a degree
 $d-1$ polynomial in $B$, and will henceforth be denoted as $p^{(k)}
 (B)$.   Substituting the new formulation, we obtain
\[
 x_t = \sum_{j=1}^d \phi_j (t) p^{(j)} (B) \, x_{0} + \int_{-\pi}^{\pi}
 \frac{ e^{i \omega t} - \sum_{j=1}^d \phi_j (t) \, p^{(j)} ( e^{-i \omega
 } )}{ \delta (e^{-i \omega}) } \; d \ZZ (\omega),
\]
 where $p^{(j)} (B)$ acts on $x_0$ by shifting the time index $t=0$
 back in time for each power of $B$.  This representation is now
 extremely convenient, because application of any factor of
 $\delta(B)$ will annihilate a corresponding basis function (when
 roots are repeated, some basis functions will also be transformed
 into others that are instead annihilated). 

 Suppose that we left multiply by $\beta^{\prime}$,
 which is a co-integrating vector at frequency $\omega_k$:
\begin{equation}
 \label{eq:co-intRep}
  \beta^{\prime} x_t = \sum_{j=1}^d \phi_j (t) p^{(j)} (B) \, \beta^{\prime} \, x_{0} + \int_{-\pi}^{\pi}
 \frac{ e^{i \omega t} - \sum_{j=1}^d \phi_j (t) \, p^{(j)} ( e^{-i \omega
 } )}{ \delta (e^{-i \omega}) } \; \beta^{\prime} \, d \ZZ
 (\omega).
\end{equation}
  From our previous discussion, we know that the result is a non-stationary
 process with differencing operator $\delta^{(k)} (B)$; this implies
 that there should be a cancelation of $\beta^{\prime} \, d\ZZ
 (\omega)$ with the ${(1 - e^{i \omega_{k}} e^{-i\omega}
 )}^{q_{k}}$ term in $\delta (e^{-i \omega})$.  As a result, we
 have the following spectral formalization of the co-integrating
 relation:
\begin{equation}
\label{eq:co-intRel}
 \beta^{\prime} \, d\ZZ (\omega) = {(1 - e^{i \omega_{k}} e^{-i\omega}
 )}^{q_{k}} \, d\ZZ^{(k)} (\omega),
\end{equation}
 where $d \ZZ^{(k)} (\omega)$ is the orthogonal increments measure
 of another stationary invertible process.  This condition
 (\ref{eq:co-intRel}) is readily satisfied by the latent dynamic
 factor process discussed earlier, which is exemplary of the general
 situation of interest.  The extreme case, where the co-integrating
 vector lies in all the left null spaces of the component processes,
 allows us to factor $\delta (e^{-i \omega})$ completely from
 $\beta^{\prime} d \ZZ (\omega)$, though such a property need not
 hold in practice.  

In order to see the full effect of condition
 (\ref{eq:co-intRel}) on $\beta^{\prime} x_t$, we re-organize terms
 in equation (\ref{eq:co-intRep}).  Let us suppose, without loss of
 generality, that frequency $\omega_k$ has corresponding basis
 functions $\phi_1, \cdots, \phi_{q_k}$, so that the first $q_k$
 basis functions are annihilated by ${(1 - e^{i \omega_k}
 B)}^{q_k}$.  Then we can write
\begin{align*}
 \beta^{\prime} x_t & = \sum_{j= q_k + 1}^d \phi_j (t) p^{(j)} (B) \, \beta^{\prime} \, x_{0}
  + \int_{-\pi}^{\pi} \frac{ e^{i \omega t} - \sum_{j= q_k + 1}^d \phi_j (t) \, p^{(j)} ( e^{-i \omega
 } )}{ \delta^{(k)} (e^{-i \omega}) } \, d \ZZ^{(k)}
 (\omega) \\
 & + \sum_{j=1}^{q_k} \phi_j (t) \,  \left(p^{(j)}
 (B)  \beta^{\prime} \,  x_0 - \int_{-\pi}^{\pi} \frac{ p^{(j)} (e^{-i \omega}) }{
 \delta^{(k)} (e^{-i \omega}) } \, d\ZZ^{(k)} (\omega) \right).
\end{align*}
 The first two terms are immediately recognized as the deterministic
 and stochastic portions respectively of a non-stationary process
 that has $\delta^{(k)} (B)$ for differencing operator.  The third
 term is left over, and consists of deterministic time series that
 are in the null space of ${(1 - e^{i \omega_k}
 B)}^{q_k}$.  To see this, observe that for the third term the expression in parentheses is
stochastic, but does not depend on time $t$, so that the resulting series is predictable.


 It is true that $\delta^{(k)} (B)$
 always divides $p^{(j)} (B)$, and hence the stochastic portion of
 the third term is well-defined.  We cannot prove that the
 coefficients of the $\phi_j (t)$ for $1 \leq j \leq q_k$ must be
 zero, as counter-examples are easy to construct; consider two series that
 have a common stochastic trend with null vector $\beta^{\prime} =
 [1, \, 1]$, but whose underlying linear deterministic trends have
different slopes.  
%(Such a bivariate series might not be considered
% co-integrated, because $\beta^{\prime} x_t$ equals a stationary
% process plus a linear drift term.)
  In our analysis henceforth, we
 will assume that this third term is identically zero.

\section{DFA for Co-Integrated Processes}


 This is the general treatment of co-integration.  Now consider the
 filter error $\epsilon_t = y_t - \widehat{y}_t$.  Let $\Delta(z) = \Gamma
 (z) - \widehat{\Gamma} (z)$, so that
\[
 \epsilon_t = \sum_{j=1}^d \Delta(B) \phi_j (t) p^{(j)} (B) \, x_{0} + \int_{-\pi}^{\pi}
 \frac{ e^{i \omega t} \, \Delta (e^{-i \omega})
 - \sum_{j=1}^d \Delta (B) \phi_j (t) \, p^{(j)} ( e^{-i \omega
 } )}{ \delta (e^{-i \omega}) } \; d \ZZ (\omega),
\]
 where $\Delta (B)$ acts only upon the basis functions $\phi_j (t)$.
  In order to write this expression, we really need the common
  differencing operators assumption.  Note that $\Delta (B) $ is a
  row vector of filters, and it gets multiplied by the initial value
  vectors and the orthogonal increments process $d\ZZ(\omega)$.
  Clearly the error process is stationary if all the basis functions
  are annihilated by $\Delta (B)$, because in that case we must be
  able to factor $\Delta (B) = \tau (B) \delta (B)$ (where $\tau (B)$ is
  a $1 \times m$ multivariate filter) and $\epsilon_t
  = \int_{-\pi}^{\pi} e^{i \lambda t } \, \tau (e^{-i \lambda}) \,
  d\ZZ (\lambda)$.  This is the case of full filter constraints,
  analogous to the stationary case considered above. 

We next consider some natural properties of target and
concurrent filters.  Let us first factor $\delta (B) = \delta^S (B)
\delta^N (B)$ according to signal and noise unit roots.  We will
henceforth suppose that $\delta^S (B) = {(1 - e^{i \omega k}
B)}^{q_k}$ for some unit root $\zeta_k = e^{-i \omega k}$ of
multiplicity $q_k$.  Thus $\delta^N (B) = \delta^{(k)} (B)$.  Both
$\Gamma (B)$ and $\widehat{\Gamma} (B)$ should preserve signal basis functions,
which are those $\phi_j (t)$ corresponding to the unit root
$\zeta_k$.  In order to preserve all these functions (i.e., act as
the identity filter on them all) when multiplicity $q_k$ is present,
we must have that
\[
 \frac{ \Gamma (z) - \Gamma (\zeta_k) }{ {( z- \zeta_k)}^{q_k} }
 \qquad  \frac{ \widehat{\Gamma} (z) - \widehat{\Gamma} (\zeta_k) }{ {( z- \zeta_k)}^{q_k} }
\]
 are both bounded in $z$.  Equivalently, the differences $\Gamma (z) - \Gamma (\zeta_k)$ and 
$  \widehat{\Gamma} (z) - \widehat{\Gamma} (\zeta_k)$ are each
 divisible by $\delta^S (z)$.  We call this the {\it  signal preservation}
 property of the filters.  For example, the signal extraction
 filters described in McElroy and Trimbur (2012) always satisfy this
 sort of condition.  In addition, because signal extraction filters
 must eradicate all basis functions associated with noise
 frequencies, it follows that $\delta^N (z)$ must be a factor of
 $\Gamma (z) $ and $\widehat{\Gamma} (z)$.  We call this the {\it noise annihilation}
 property of the filters.  Introduce the notation 
\[
 \Gamma^{N,k} (z) = \Gamma (\zeta_k)  \delta^N (z) / \delta^N
  (\zeta_k)  \qquad \widehat{\Gamma}^{N,k} =  \widehat{\Gamma} (\zeta_k)  \delta^N (z) / \delta^N
  (\zeta_k).
\]
 Then we can write
\begin{equation}
\label{eq:deltaErrdecomp}
 \Delta (z)  =
  \left( \frac{ \Gamma (z) - \Gamma^{N,k} (z) }{ \delta (z) } \right) \; \delta (z)
  - \left( \frac{ \widehat{\Gamma} (z) - \widehat{\Gamma}^{N,k} (z) }{ \delta (z) } \right) \; \delta (z)  
  + \left( \frac{ \Gamma (\zeta_k) - \widehat{\Gamma} (\zeta_k) }{ \delta^N
  (\zeta_k) } \right) \; \delta^N (z).
\end{equation}
 We claim that the first two expressions involve a bounded rational function times
 $\delta (z)$.  To see this true, observe that we only have to check
 boundedness of $[ \Gamma (z) - \Gamma^{N,k} (z) ]/\delta (z)$ at $z$ values that are either roots of
  $\delta^N (B)$ or $\delta^S (B)$ -- the same argument applies to
  the second term involving $\widehat{\Gamma}$.  For a signal unit root, we
  have $z = \zeta_k$, and boundedness follows from the signal
  preservation property.  For a noise unit root, observe that we may
  always factor $\delta^N (B)$ from $\Gamma (B)$ by the noise
  annihilation property.  As for the third term of (\ref{eq:deltaErrdecomp}), it is
  also always well-defined by the noise annihilation property.

  It is paramount that $\Delta (B)$ reduce the non-stationary
  process to stationarity, and this can only happen in two ways:
  first, a $\delta (B)$ can be factored out, which accomplishes the
  requirement by differencing.  Second, the filter may have a linear
  combination that is a co-integrating vector (associated with the
  single signal frequency, which is important!), which together with
  noise differencing accomplishes the requirement as well.  Note
  that application of a co-integrating vector alone only removes
  signal non-stationarity, and noise non-stationarity will remain.
  The above decomposition for $\Delta (B)$ accomplishes this (under
  the signal preservation and noise annihilation properties) if
 \begin{equation}
  \label{eq:co-intCond}
 \frac{ \Gamma (\zeta_k) - \widehat{\Gamma} (\zeta_k) }{ \delta^N
  (\zeta_k) } = \beta^{\prime}
 \end{equation}
  for some vector $\beta$ to be described.  If we impose that
  $\beta$ is the zero vector, then we obtain the first case above,
  where $\Delta (B)$ maintains stationarity by full differencing.
  If instead we relax this to only imposing that $\beta = \beta_k$
  be a co-integrating vector for the signal, then we obtain the
  second case above.  Because there may be many choices for
  $\beta_k$, depending on the co-integrating rank (the dimension of
  the left null space of the factor loading matrix in the latent
  dynamic factors formulation), this is a milder condition that may
  allow for more flexibility in filter estimation.  Note that since
  $\Gamma (\zeta_k) / \delta^N (\zeta_k)$ is a given quantity,
  imposing (\ref{eq:co-intCond}) amounts to setting $\widehat{\Gamma}
  (\zeta_k) / \delta^N (\zeta_k) = \beta^{\prime} + \Gamma (\zeta_k) /
  \delta^N (\zeta_k)$ for a known co-integrating $\beta$.

We next develop the consequences of (\ref{eq:co-intCond}), where
  $\beta$ is either zero or a signal co-integrating vector (the
  second case will reduce to the first when we set $\beta = 0$ in
  the following formulas).  Let $c_t = \delta^N (B) \beta^{\prime}
  x_t$, which by our prior expression for $\beta^{\prime} x_t$ is
  shown to be equal to $\int_{-\pi}^{\pi} e^{i \omega t} \; d
  \ZZ^{(k)} (\omega)$.  The signal extraction error is
\[
 \epsilon_t  = \int_{-\pi}^{\pi} e^{i \omega t} \,
  \left( \frac{ \Gamma (z) - \Gamma^{N,k} (z)   }{ \delta (z) } \right) \; d\ZZ (\omega)  - \int_{-\pi}^{\pi} e^{i \omega t} \,
  \left( \frac{ \widehat{\Gamma} (z) - \widehat{\Gamma}^{N,k} (z) }{ \delta (z) } \right) \; d\ZZ (\omega) 
 + \int_{-\pi}^{\pi} e^{i \omega t} \; d  \ZZ^{(k)} (\omega).
\]
 Its variance involves a spectral density matrix that combines
 information from the differenced series $\partial s_t$ as well as
 the noise-differenced co-integrated series $c_t$.  We now suppose
 that the joint spectral density matrix of these series is available
 to us, which is certainly possible in the case of latent dynamic
 factor models.  Letting $h_c$, $h_{c \partial x}$, and $h_{\partial
 x}$ denote the spectra and cross-spectra, we have the joint spectra
 for ${[c_t, \partial x_t^{\prime}]}^{\prime}$ is
\[
 h(\omega) = \left[ \begin{array}{ll} h_c (\omega) & h_{\partial x
 c} (\omega) \\ h_{c \partial x} (\omega) & h_{\partial x} (\omega)
 \end{array} \right].
\]
  Then the signal extraction variance is ${(2\pi)}^{-1}$ times the
  integral of
\[
 \left[ 1, - \left( \frac{ \Gamma (z) - \Gamma^{N,k} (z) }{ \delta (z) } \right) + 
   \left( \frac{ \widehat{\Gamma} (z) - \widehat{\Gamma}^{N,k} (z) }{ \delta (z) } \right)  \right] \; h (\omega) \;
  { \left[ 1, - \left( \frac{ \Gamma (\overline{z}) - \overline{\Gamma^{N,k}} (\overline{z})  }{ \delta (\overline{z}) } \right) +
 \left( \frac{ \widehat{ \Gamma} (\overline{z}) -
  \overline{\widehat{\Gamma}^{N,k} } (\overline{z}) }{ \delta (\overline{z}) } \right)
  \right] }^{\prime}.
 \]
 Substituting the periodogram for $h$ at this point will allow
 empirical estimation, where we impose the co-integrating relations
 on $\widehat{\Gamma}$ and then optimize.

 In the case that $\beta$ is the zero vector, we are merely imposing full filter constraints by (\ref{eq:co-intCond}), and the multivariate DFA (M-DFA) condition
  can be simplified.  Introduce the notation
\[
    \Gamma^{\delta,k} (z) =  \frac{ \Gamma (z) - \Gamma^{N,k} (z) }{ \delta (z) } \qquad
  \widehat{\Gamma}^{\delta, k} (z) =  \frac{ \widehat{\Gamma} (z) - \widehat{\Gamma}^{N,k} (z) }{ \delta (z) }.
\]
  Although the former quantity can be computed from a knowledge of the target filter and the goals of analysis, the latter quantity is obliquely related to the parameters of the proposed concurrent filter.  In terms of these quantities, the M-DFA MSE is
\[
 \langle   \left[   \Gamma^{\delta, k} (z) -  \widehat{\Gamma}^{\delta, k} (z)  \right]   \; h_{\partial x} \;
  { \left[  \Gamma^{\delta, k} (\overline{z}) -  \widehat{\Gamma}^{\delta, k} (\overline{z})    \right] }^{\prime} \rangle.
\]
 The filter conditions can be rephrased in terms of coefficient constraints, as in the stationary case.  

HERE

\section{Examples of Multivariate DFA}


\section{Examples of Co-Integrated Multivariate DFA}



